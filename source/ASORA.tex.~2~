%********************************************%
%*       Generated from PreTeXt source      *%
%*       on 2020-12-21T15:06:15-05:00       *%
%*   A recent stable commit (2020-08-09):   *%
%* 98f21740783f166a773df4dc83cab5293ab63a4a *%
%*                                          *%
%*         https://pretextbook.org          *%
%*                                          *%
%********************************************%
%% We elect to always write snapshot output into <job>.dep file
\RequirePackage{snapshot}
\documentclass[oneside,10pt,]{book}
%% Custom Preamble Entries, early (use latex.preamble.early)
%% Default LaTeX packages
%%   1.  always employed (or nearly so) for some purpose, or
%%   2.  a stylewriter may assume their presence
\usepackage{geometry}
%% Some aspects of the preamble are conditional,
%% the LaTeX engine is one such determinant
\usepackage{ifthen}
%% etoolbox has a variety of modern conveniences
\usepackage{etoolbox}
\usepackage{ifxetex,ifluatex}
%% Raster graphics inclusion
\usepackage{graphicx}
%% Color support, xcolor package
%% Always loaded, for: add/delete text, author tools
%% Here, since tcolorbox loads tikz, and tikz loads xcolor
\PassOptionsToPackage{usenames,dvipsnames,svgnames,table}{xcolor}
\usepackage{xcolor}
%% begin: defined colors, via xcolor package, for styling
%% end: defined colors, via xcolor package, for styling
%% Colored boxes, and much more, though mostly styling
%% skins library provides "enhanced" skin, employing tikzpicture
%% boxes may be configured as "breakable" or "unbreakable"
%% "raster" controls grids of boxes, aka side-by-side
\usepackage{tcolorbox}
\tcbuselibrary{skins}
\tcbuselibrary{breakable}
\tcbuselibrary{raster}
%% We load some "stock" tcolorbox styles that we use a lot
%% Placement here is provisional, there will be some color work also
%% First, black on white, no border, transparent, but no assumption about titles
\tcbset{ bwminimalstyle/.style={size=minimal, boxrule=-0.3pt, frame empty,
colback=white, colbacktitle=white, coltitle=black, opacityfill=0.0} }
%% Second, bold title, run-in to text/paragraph/heading
%% Space afterwards will be controlled by environment,
%% independent of constructions of the tcb title
%% Places \blocktitlefont onto many block titles
\tcbset{ runintitlestyle/.style={fonttitle=\blocktitlefont\upshape\bfseries, attach title to upper} }
%% Spacing prior to each exercise, anywhere
\tcbset{ exercisespacingstyle/.style={before skip={1.5ex plus 0.5ex}} }
%% Spacing prior to each block
\tcbset{ blockspacingstyle/.style={before skip={2.0ex plus 0.5ex}} }
%% xparse allows the construction of more robust commands,
%% this is a necessity for isolating styling and behavior
%% The tcolorbox library of the same name loads the base library
\tcbuselibrary{xparse}
%% Hyperref should be here, but likes to be loaded late
%%
%% Inline math delimiters, \(, \), need to be robust
%% 2016-01-31:  latexrelease.sty  supersedes  fixltx2e.sty
%% If  latexrelease.sty  exists, bugfix is in kernel
%% If not, bugfix is in  fixltx2e.sty
%% See:  https://tug.org/TUGboat/tb36-3/tb114ltnews22.pdf
%% and read "Fewer fragile commands" in distribution's  latexchanges.pdf
\IfFileExists{latexrelease.sty}{}{\usepackage{fixltx2e}}
%% shorter subnumbers in some side-by-side require manipulations
\usepackage{xstring}
%% Footnote counters and part/chapter counters are manipulated
%% April 2018:  chngcntr  commands now integrated into the kernel,
%% but circa 2018/2019 the package would still try to redefine them,
%% so we need to do the work of loading conditionally for old kernels.
%% From version 1.1a,  chngcntr  should detect defintions made by LaTeX kernel.
\ifdefined\counterwithin
\else
    \usepackage{chngcntr}
\fi
%% Text height identically 9 inches, text width varies on point size
%% See Bringhurst 2.1.1 on measure for recommendations
%% 75 characters per line (count spaces, punctuation) is target
%% which is the upper limit of Bringhurst's recommendations
\geometry{letterpaper,total={340pt,9.0in}}
%% Custom Page Layout Adjustments (use latex.geometry)
%% This LaTeX file may be compiled with pdflatex, xelatex, or lualatex executables
%% LuaTeX is not explicitly supported, but we do accept additions from knowledgeable users
%% The conditional below provides  pdflatex  specific configuration last
%% begin: engine-specific capabilities
\ifthenelse{\boolean{xetex} \or \boolean{luatex}}{%
%% begin: xelatex and lualatex-specific default configuration
\ifxetex\usepackage{xltxtra}\fi
%% realscripts is the only part of xltxtra relevant to lualatex 
\ifluatex\usepackage{realscripts}\fi
%% end:   xelatex and lualatex-specific default configuration
}{
%% begin: pdflatex-specific default configuration
%% We assume a PreTeXt XML source file may have Unicode characters
%% and so we ask LaTeX to parse a UTF-8 encoded file
%% This may work well for accented characters in Western language,
%% but not with Greek, Asian languages, etc.
%% When this is not good enough, switch to the  xelatex  engine
%% where Unicode is better supported (encouraged, even)
\usepackage[utf8]{inputenc}
%% end: pdflatex-specific default configuration
}
%% end:   engine-specific capabilities
%%
%% Fonts.  Conditional on LaTex engine employed.
%% Default Text Font: The Latin Modern fonts are
%% "enhanced versions of the [original TeX] Computer Modern fonts."
%% We use them as the default text font for PreTeXt output.
%% Automatic Font Control
%% Portions of a document, are, or may, be affected by defined commands
%% These are perhaps more flexible when using  xelatex  rather than  pdflatex
%% The following definitions are meant to be re-defined in a style, using \renewcommand
%% They are scoped when employed (in a TeX group), and so should not be defined with an argument
\newcommand{\divisionfont}{\relax}
\newcommand{\blocktitlefont}{\relax}
\newcommand{\contentsfont}{\relax}
\newcommand{\pagefont}{\relax}
\newcommand{\tabularfont}{\relax}
\newcommand{\xreffont}{\relax}
\newcommand{\titlepagefont}{\relax}
%%
\ifthenelse{\boolean{xetex} \or \boolean{luatex}}{%
%% begin: font setup and configuration for use with xelatex
%% Generally, xelatex is necessary for non-Western fonts
%% fontspec package provides extensive control of system fonts,
%% meaning *.otf (OpenType), and apparently *.ttf (TrueType)
%% that live *outside* your TeX/MF tree, and are controlled by your *system*
%% (it is possible that a TeX distribution will place fonts in a system location)
%%
%% The fontspec package is the best vehicle for using different fonts in  xelatex
%% So we load it always, no matter what a publisher or style might want
%%
\usepackage{fontspec}
%%
%% begin: xelatex main font ("font-xelatex-main" template)
%% Latin Modern Roman is the default font for xelatex and so is loaded with a TU encoding
%% *in the format* so we can't touch it, only perhaps adjust it later
%% in one of two ways (then known by NFSS names such as "lmr")
%% (1) via NFSS with font family names such as "lmr" and "lmss"
%% (2) via fontspec with commands like \setmainfont{Latin Modern Roman}
%% The latter requires the font to be known at the system-level by its font name,
%% but will give access to OTF font features through optional arguments
%% https://tex.stackexchange.com/questions/470008/
%% where-and-how-does-fontspec-sty-specify-the-default-font-latin-modern-roman
%% http://tex.stackexchange.com/questions/115321
%% /how-to-optimize-latin-modern-font-with-xelatex
%%
%% end:   xelatex main font ("font-xelatex-main" template)
%% begin: xelatex mono font ("font-xelatex-mono" template)
%% (conditional on non-trivial uses being present in source)
%% end:   xelatex mono font ("font-xelatex-mono" template)
%% begin: xelatex font adjustments ("font-xelatex-style" template)
%% end:   xelatex font adjustments ("font-xelatex-style" template)
%%
%% Extensive support for other languages
\usepackage{polyglossia}
%% Set main/default language based on pretext/@xml:lang value
%% document language code is "en-US", US English
%% usmax variant has extra hypenation
\setmainlanguage[variant=usmax]{english}
%% Enable secondary languages based on discovery of @xml:lang values
%% Enable fonts/scripts based on discovery of @xml:lang values
%% Western languages should be ably covered by Latin Modern Roman
%% end:   font setup and configuration for use with xelatex
}{%
%% begin: font setup and configuration for use with pdflatex
%% begin: pdflatex main font ("font-pdflatex-main" template)
\usepackage{lmodern}
\usepackage[T1]{fontenc}
%% end:   pdflatex main font ("font-pdflatex-main" template)
%% begin: pdflatex mono font ("font-pdflatex-mono" template)
%% (conditional on non-trivial uses being present in source)
%% end:   pdflatex mono font ("font-pdflatex-mono" template)
%% begin: pdflatex font adjustments ("font-pdflatex-style" template)
%% end:   pdflatex font adjustments ("font-pdflatex-style" template)
%% end:   font setup and configuration for use with pdflatex
}
%% Micromanage spacing, etc.  The named "microtype-options"
%% template may be employed to fine-tune package behavior
\usepackage{microtype}
%% Symbols, align environment, commutative diagrams, bracket-matrix
\usepackage{amsmath}
\usepackage{amscd}
\usepackage{amssymb}
%% allow page breaks within display mathematics anywhere
%% level 4 is maximally permissive
%% this is exactly the opposite of AMSmath package philosophy
%% there are per-display, and per-equation options to control this
%% split, aligned, gathered, and alignedat are not affected
\allowdisplaybreaks[4]
%% allow more columns to a matrix
%% can make this even bigger by overriding with  latex.preamble.late  processing option
\setcounter{MaxMatrixCols}{30}
%%
%%
%% Division Titles, and Page Headers/Footers
%% titlesec package, loading "titleps" package cooperatively
%% See code comments about the necessity and purpose of "explicit" option.
%% The "newparttoc" option causes a consistent entry for parts in the ToC 
%% file, but it is only effective if there is a \titleformat for \part.
%% "pagestyles" loads the  titleps  package cooperatively.
\usepackage[explicit, newparttoc, pagestyles]{titlesec}
%% The companion titletoc package for the ToC.
\usepackage{titletoc}
%% Fixes a bug with transition from chapters to appendices in a "book"
%% See generating XSL code for more details about necessity
\newtitlemark{\chaptertitlename}
%% begin: customizations of page styles via the modal "titleps-style" template
%% Designed to use commands from the LaTeX "titleps" package
%% Plain pages should have the same font for page numbers
\renewpagestyle{plain}{%
\setfoot{}{\pagefont\thepage}{}%
}%
%% Single pages as in default LaTeX
\renewpagestyle{headings}{%
\sethead{\pagefont\slshape\MakeUppercase{\ifthechapter{\chaptertitlename\space\thechapter.\space}{}\chaptertitle}}{}{\pagefont\thepage}%
}%
\pagestyle{headings}
%% end: customizations of page styles via the modal "titleps-style" template
%%
%% Create globally-available macros to be provided for style writers
%% These are redefined for each occurence of each division
\newcommand{\divisionnameptx}{\relax}%
\newcommand{\titleptx}{\relax}%
\newcommand{\subtitleptx}{\relax}%
\newcommand{\shortitleptx}{\relax}%
\newcommand{\authorsptx}{\relax}%
\newcommand{\epigraphptx}{\relax}%
%% Create environments for possible occurences of each division
%% Environment for a PTX "chapter" at the level of a LaTeX "chapter"
\NewDocumentEnvironment{chapterptx}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Chapter}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\chapter[{#3}]{#1}%
\label{#6}%
}{}%
%% Environment for a PTX "section" at the level of a LaTeX "section"
\NewDocumentEnvironment{sectionptx}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Section}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\section[{#3}]{#1}%
\label{#6}%
}{}%
%% Environment for a PTX "part" at the level of a LaTeX "part"
\NewDocumentEnvironment{partptx}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Part}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\part[{#3}]{#1}%
\label{#6}%
}{}%
%% Environment for a PTX "subsection" at the level of a LaTeX "subsection"
\NewDocumentEnvironment{subsectionptx}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Subsection}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\subsection[{#3}]{#1}%
\label{#6}%
}{}%
%% Environment for a PTX "references" at the level of a LaTeX "chapter"
\NewDocumentEnvironment{references-chapter}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{References}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\chapter[{#3}]{#1}%
\label{#6}%
}{}%
%% Environment for a PTX "references" at the level of a LaTeX "chapter"
\NewDocumentEnvironment{references-chapter-numberless}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{References}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\chapter*{#1}%
\addcontentsline{toc}{chapter}{#3}
\label{#6}%
}{}%
%% Environment for a PTX "index" at the level of a LaTeX "chapter"
\NewDocumentEnvironment{indexptx}{mmmmmm}
{%
\renewcommand{\divisionnameptx}{Index}%
\renewcommand{\titleptx}{#1}%
\renewcommand{\subtitleptx}{#2}%
\renewcommand{\shortitleptx}{#3}%
\renewcommand{\authorsptx}{#4}%
\renewcommand{\epigraphptx}{#5}%
\chapter*{#1}%
\addcontentsline{toc}{chapter}{#3}
\label{#6}%
}{}%
%%
%% Styles for six traditional LaTeX divisions
\titleformat{\part}[display]
{\divisionfont\Huge\bfseries\centering}{\divisionnameptx\space\thepart}{30pt}{\Huge#1}
[{\Large\centering\authorsptx}]
\titleformat{\chapter}[display]
{\divisionfont\huge\bfseries}{\divisionnameptx\space\thechapter}{20pt}{\Huge#1}
[{\Large\authorsptx}]
\titleformat{name=\chapter,numberless}[display]
{\divisionfont\huge\bfseries}{}{0pt}{#1}
[{\Large\authorsptx}]
\titlespacing*{\chapter}{0pt}{50pt}{40pt}
\titleformat{\section}[hang]
{\divisionfont\Large\bfseries}{\thesection}{1ex}{#1}
[{\large\authorsptx}]
\titleformat{name=\section,numberless}[block]
{\divisionfont\Large\bfseries}{}{0pt}{#1}
[{\large\authorsptx}]
\titlespacing*{\section}{0pt}{3.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}
\titleformat{\subsection}[hang]
{\divisionfont\large\bfseries}{\thesubsection}{1ex}{#1}
[{\normalsize\authorsptx}]
\titleformat{name=\subsection,numberless}[block]
{\divisionfont\large\bfseries}{}{0pt}{#1}
[{\normalsize\authorsptx}]
\titlespacing*{\subsection}{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\titleformat{\subsubsection}[hang]
{\divisionfont\normalsize\bfseries}{\thesubsubsection}{1em}{#1}
[{\small\authorsptx}]
\titleformat{name=\subsubsection,numberless}[block]
{\divisionfont\normalsize\bfseries}{}{0pt}{#1}
[{\normalsize\authorsptx}]
\titlespacing*{\subsubsection}{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\titleformat{\paragraph}[hang]
{\divisionfont\normalsize\bfseries}{\theparagraph}{1em}{#1}
[{\small\authorsptx}]
\titleformat{name=\paragraph,numberless}[block]
{\divisionfont\normalsize\bfseries}{}{0pt}{#1}
[{\normalsize\authorsptx}]
\titlespacing*{\paragraph}{0pt}{3.25ex plus 1ex minus .2ex}{1.5em}
%%
%% Styles for five traditional LaTeX divisions
\titlecontents{part}%
[0pt]{\contentsmargin{0em}\addvspace{1pc}\contentsfont\bfseries}%
{\Large\thecontentslabel\enspace}{\Large}%
{}%
[\addvspace{.5pc}]%
\titlecontents{chapter}%
[0pt]{\contentsmargin{0em}\addvspace{1pc}\contentsfont\bfseries}%
{\large\thecontentslabel\enspace}{\large}%
{\hfill\bfseries\thecontentspage}%
[\addvspace{.5pc}]%
\dottedcontents{section}[3.8em]{\contentsfont}{2.3em}{1pc}%
\dottedcontents{subsection}[6.1em]{\contentsfont}{3.2em}{1pc}%
\dottedcontents{subsubsection}[9.3em]{\contentsfont}{4.3em}{1pc}%
%%
%% Begin: Semantic Macros
%% To preserve meaning in a LaTeX file
%%
%% \mono macro for content of "c", "cd", "tag", etc elements
%% Also used automatically in other constructions
%% Simply an alias for \texttt
%% Always defined, even if there is no need, or if a specific tt font is not loaded
\newcommand{\mono}[1]{\texttt{#1}}
%%
%% Following semantic macros are only defined here if their
%% use is required only in this specific document
%%
%% Used for warnings, typically bold and italic
\newcommand{\alert}[1]{\textbf{\textit{#1}}}
%% Used for inline definitions of terms
\newcommand{\terminology}[1]{\textbf{#1}}
%% Style of a title on a list item, for ordered and unordered lists
\newcommand{\lititle}[1]{{\slshape#1}}
%% End: Semantic Macros
%% Division Numbering: Chapters, Sections, Subsections, etc
%% Division numbers may be turned off at some level ("depth")
%% A section *always* has depth 1, contrary to us counting from the document root
%% The latex default is 3.  If a larger number is present here, then
%% removing this command may make some cross-references ambiguous
%% The precursor variable $numbering-maxlevel is checked for consistency in the common XSL file
\setcounter{secnumdepth}{3}
%%
%% AMS "proof" environment is no longer used, but we leave previously
%% implemented \qedhere in place, should the LaTeX be recycled
\newcommand{\qedhere}{\relax}
%%
%% A faux tcolorbox whose only purpose is to provide common numbering
%% facilities for most blocks (possibly not projects, 2D displays)
%% Controlled by  numbering.theorems.level  processing parameter
\newtcolorbox[auto counter, number within=section]{block}{}
%%
%% This document is set to number PROJECT-LIKE on a separate numbering scheme
%% So, a faux tcolorbox whose only purpose is to provide this numbering
%% Controlled by  numbering.projects.level  processing parameter
\newtcolorbox[auto counter, number within=section]{project-distinct}{}
%% A faux tcolorbox whose only purpose is to provide common numbering
%% facilities for 2D displays which are subnumbered as part of a "sidebyside"
\makeatletter
\newtcolorbox[auto counter, number within=tcb@cnt@block, number freestyle={\noexpand\thetcb@cnt@block(\noexpand\alph{\tcbcounter})}]{subdisplay}{}
\makeatother
%%
%% tcolorbox, with styles, for THEOREM-LIKE
%%
%% theorem: fairly simple numbered block/structure
\tcbset{ theoremstyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, } }
\newtcolorbox[use counter from=block]{theorem}[3]{title={{Theorem~\thetcbcounter\notblank{#1#2}{\space}{}\notblank{#1}{\space#1}{}\notblank{#2}{\space(#2)}{}}}, phantomlabel={#3}, breakable, parbox=false, after={\par}, fontupper=\itshape, theoremstyle, }
%% lemma: fairly simple numbered block/structure
\tcbset{ lemmastyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, } }
\newtcolorbox[use counter from=block]{lemma}[3]{title={{Lemma~\thetcbcounter\notblank{#1#2}{\space}{}\notblank{#1}{\space#1}{}\notblank{#2}{\space(#2)}{}}}, phantomlabel={#3}, breakable, parbox=false, after={\par}, fontupper=\itshape, lemmastyle, }
%% corollary: fairly simple numbered block/structure
\tcbset{ corollarystyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, } }
\newtcolorbox[use counter from=block]{corollary}[3]{title={{Corollary~\thetcbcounter\notblank{#1#2}{\space}{}\notblank{#1}{\space#1}{}\notblank{#2}{\space(#2)}{}}}, phantomlabel={#3}, breakable, parbox=false, after={\par}, fontupper=\itshape, corollarystyle, }
%%
%% tcolorbox, with styles, for AXIOM-LIKE
%%
%% principle: fairly simple numbered block/structure
\tcbset{ principlestyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, } }
\newtcolorbox[use counter from=block]{principle}[3]{title={{Principle~\thetcbcounter\notblank{#1#2}{\space}{}\notblank{#1}{\space#1}{}\notblank{#2}{\space(#2)}{}}}, phantomlabel={#3}, breakable, parbox=false, after={\par}, fontupper=\itshape, principlestyle, }
%% conjecture: fairly simple numbered block/structure
\tcbset{ conjecturestyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, } }
\newtcolorbox[use counter from=block]{conjecture}[3]{title={{Conjecture~\thetcbcounter\notblank{#1#2}{\space}{}\notblank{#1}{\space#1}{}\notblank{#2}{\space(#2)}{}}}, phantomlabel={#3}, breakable, parbox=false, after={\par}, fontupper=\itshape, conjecturestyle, }
%% axiom: fairly simple numbered block/structure
\tcbset{ axiomstyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, } }
\newtcolorbox[use counter from=block]{axiom}[3]{title={{Axiom~\thetcbcounter\notblank{#1#2}{\space}{}\notblank{#1}{\space#1}{}\notblank{#2}{\space(#2)}{}}}, phantomlabel={#3}, breakable, parbox=false, after={\par}, fontupper=\itshape, axiomstyle, }
%%
%% tcolorbox, with styles, for DEFINITION-LIKE
%%
%% definition: fairly simple numbered block/structure
\tcbset{ definitionstyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, after upper={\space\space\hspace*{\stretch{1}}\(\lozenge\)}, } }
\newtcolorbox[use counter from=block]{definition}[2]{title={{Definition~\thetcbcounter\notblank{#1}{\space\space#1}{}}}, phantomlabel={#2}, breakable, parbox=false, after={\par}, definitionstyle, }
%%
%% tcolorbox, with styles, for EXAMPLE-LIKE
%%
%% problem: fairly simple numbered block/structure
\tcbset{ problemstyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, after upper={\space\space\hspace*{\stretch{1}}\(\square\)}, } }
\newtcolorbox[use counter from=block]{problem}[2]{title={{Problem~\thetcbcounter\notblank{#1}{\space\space#1}{}}}, phantomlabel={#2}, breakable, parbox=false, after={\par}, problemstyle, }
%% example: fairly simple numbered block/structure
\tcbset{ examplestyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, after upper={\space\space\hspace*{\stretch{1}}\(\square\)}, } }
\newtcolorbox[use counter from=block]{example}[2]{title={{Example~\thetcbcounter\notblank{#1}{\space\space#1}{}}}, phantomlabel={#2}, breakable, parbox=false, after={\par}, examplestyle, }
%%
%% tcolorbox, with styles, for ASIDE-LIKE
%%
%% historical: fairly simple un-numbered block/structure
\tcbset{ historicalstyle/.style={bwminimalstyle, runintitlestyle, blockspacingstyle, after title={\space}, } }
\newtcolorbox{historical}[2]{title={\notblank{#1}{#1}{}}, phantomlabel={#2}, breakable, parbox=false, historicalstyle}
%%
%% tcolorbox, with styles, for FIGURE-LIKE
%%
%% figureptx: 2-D display structure
\tcbset{ figureptxstyle/.style={bwminimalstyle, middle=1ex, blockspacingstyle, fontlower=\blocktitlefont} }
\newtcolorbox[use counter from=block]{figureptx}[3]{lower separated=false, before lower={{\textbf{Figure~\thetcbcounter}\space#1}}, phantomlabel={#2}, unbreakable, parbox=false, figureptxstyle, }
%%
%% xparse environments for introductions and conclusions of divisions
%%
%% introduction: in a structured division
\NewDocumentEnvironment{introduction}{m}
{\notblank{#1}{\noindent\textbf{#1}\space}{}}{\par\medskip}
%%
%% tcolorbox, with styles, for miscellaneous environments
%%
%% proof: title is a replacement
\tcbset{ proofstyle/.style={bwminimalstyle, fonttitle=\blocktitlefont\itshape, attach title to upper, after title={\space}, after upper={\space\space\hspace*{\stretch{1}}\(\blacksquare\)},
} }
\newtcolorbox{proof}[2]{title={\notblank{#1}{#1}{Proof.}}, phantom={\hypertarget{#2}{}}, breakable, parbox=false, after={\par}, proofstyle }
%% Localize LaTeX supplied names (possibly none)
\renewcommand*{\partname}{Part}
\renewcommand*{\chaptername}{Chapter}
%% Equation Numbering
%% Controlled by  numbering.equations.level  processing parameter
%% No adjustment here implies document-wide numbering
\numberwithin{equation}{section}
%% "tcolorbox" environment for a single image, occupying entire \linewidth
%% arguments are left-margin, width, right-margin, as multiples of
%% \linewidth, and are guaranteed to be positive and sum to 1.0
\tcbset{ imagestyle/.style={bwminimalstyle} }
\NewTColorBox{image}{mmm}{imagestyle,left skip=#1\linewidth,width=#2\linewidth}
%% Footnote Numbering
%% Specified by numbering.footnotes.level
%% Undo counter reset by chapter for a book
\counterwithout{footnote}{chapter}
\counterwithin*{footnote}{section}
%% QR Code Support
%% Videos and other interactives
\usepackage{qrcode}
\newlength{\qrsize}
\newlength{\previewwidth}
%% tcolorbox styles for interactive previews
%% changing size= and/or colback can aid in debugging
\tcbset{ previewstyle/.style={bwminimalstyle, halign=center} }
\tcbset{ qrstyle/.style={bwminimalstyle, hbox} }
\tcbset{ captionstyle/.style={bwminimalstyle, left=1em, width=\linewidth} }
%% Generic red play button (from SVG)
%% tikz package should be loaded by now
\definecolor{playred}{RGB}{230,33,23}
\newcommand{\genericpreview}{
        \begin{tikzpicture}[y=0.80pt, x=0.80pt, yscale=-1.000000, xscale=1.000000, inner sep=0pt, outer sep=0pt]
        \path[fill=playred] (94.9800,28.8400) .. controls (94.9800,28.8400) and
        (94.0400,22.2400) .. (91.1700,19.3400) .. controls (87.5300,15.5300) and
        (83.4500,15.5100) .. (81.5800,15.2900) .. controls (68.1800,14.3200) and
        (48.0600,14.4400) .. (48.0600,14.4400) .. controls (48.0600,14.4400) and
        (27.9400,14.3200) .. (14.5400,15.2900) .. controls (12.6700,15.5100) and
        (8.5900,15.5300) .. (4.9500,19.3400) .. controls (2.0800,22.2400) and
        (1.1400,28.8400) .. (1.1400,28.8400) .. controls (1.1400,28.8400) and
        (0.1800,36.5800) .. (0.0000,44.3300) -- (0.0000,51.5900) .. controls
        (0.1800,59.3400) and (1.1400,67.0800) .. (1.1400,67.0800) .. controls
        (1.1400,67.0800) and (2.0700,73.6800) .. (4.9500,76.5800) .. controls
        (8.5900,80.3900) and (13.3800,80.2700) .. (15.5100,80.6700) .. controls
        (23.0400,81.3900) and (47.2100,81.5600) .. (48.0500,81.5700) .. controls
        (48.0600,81.5700) and (68.1900,81.6000) .. (81.5900,80.6300) .. controls
        (83.4600,80.4100) and (87.5400,80.3900) .. (91.1800,76.5800) .. controls
        (94.0500,73.6800) and (94.9900,67.0800) .. (94.9900,67.0800) .. controls
        (94.9900,67.0800) and (95.9500,59.3300) .. (96.0100,51.5900) --
        (96.0100,44.3300) .. controls (95.9400,36.5800) and (94.9800,28.8400) ..
        (94.9800,28.8400) -- cycle(38.2800,61.4100) -- (38.2800,34.4100) --
        (64.0200,47.9100) -- (38.2800,61.4100) -- cycle;
        \end{tikzpicture}
        }
%% More flexible list management, esp. for references
%% But also for specifying labels (i.e. custom order) on nested lists
\usepackage{enumitem}
%% Lists of references in their own section, maximum depth 1
\newlist{referencelist}{description}{4}
\setlist[referencelist]{leftmargin=!,labelwidth=!,labelsep=0ex,itemsep=1.0ex,topsep=1.0ex,partopsep=0pt,parsep=0pt}
%% Support for index creation
%% imakeidx package does not require extra pass (as with makeidx)
%% Title of the "Index" section set via a keyword
%% Language support for the "see" and "see also" phrases
\usepackage{imakeidx}
\makeindex[title=Index, intoc=true]
\renewcommand{\seename}{See}
\renewcommand{\alsoname}{See also}
%% hyperref driver does not need to be specified, it will be detected
%% Footnote marks in tcolorbox have broken linking under
%% hyperref, so it is necessary to turn off all linking
%% It *must* be given as a package option, not with \hypersetup
\usepackage[hyperfootnotes=false]{hyperref}
%% configure hyperref's  \url  to match listings' inline verbatim
\renewcommand\UrlFont{\small\ttfamily}
%% Hyperlinking active in electronic PDFs, all links solid and blue
\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,filecolor=blue,urlcolor=blue}
\hypersetup{pdftitle={How We Got From There To Here: A Story of Real Analysis}}
%% If you manually remove hyperref, leave in this next command
\providecommand\phantomsection{}
%% Graphics Preamble Entries

%% If tikz has been loaded, replace ampersand with \amp macro
%% tcolorbox styles for sidebyside layout
\tcbset{ sbsstyle/.style={raster before skip=2.0ex, raster equal height=rows, raster force size=false} }
\tcbset{ sbspanelstyle/.style={bwminimalstyle, fonttitle=\blocktitlefont} }
%% Enviroments for side-by-side and components
%% Necessary to use \NewTColorBox for boxes of the panels
%% "newfloat" environment to squash page-breaks within a single sidebyside
%% "xparse" environment for entire sidebyside
\NewDocumentEnvironment{sidebyside}{mmmm}
  {\begin{tcbraster}
    [sbsstyle,raster columns=#1,
    raster left skip=#2\linewidth,raster right skip=#3\linewidth,raster column skip=#4\linewidth]}
  {\end{tcbraster}}
%% "tcolorbox" environment for a panel of sidebyside
\NewTColorBox{sbspanel}{mO{top}}{sbspanelstyle,width=#1\linewidth,valign=#2}
%% extpfeil package for certain extensible arrows,
%% as also provided by MathJax extension of the same name
%% NB: this package loads mtools, which loads calc, which redefines
%%     \setlength, so it can be removed if it seems to be in the 
%%     way and your math does not use:
%%     
%%     \xtwoheadrightarrow, \xtwoheadleftarrow, \xmapsto, \xlongequal, \xtofrom
%%     
%%     we have had to be extra careful with variable thickness
%%     lines in tables, and so also load this package late
\usepackage{extpfeil}
%% Custom Preamble Entries, late (use latex.preamble.late)
%% Begin: Author-provided packages
%% (From  docinfo/latex-preamble/package  elements)
%% End: Author-provided packages
%% Begin: Author-provided macros
%% (From  docinfo/macros  element)
%% Plus three from MBX for XML characters
\def\theindex{}
\def\subitem{

\hangindent 40pt \hspace{20pt}}
\def\subsubitem{

\hangindent 40pt \hspace{30pt}}
\def\indexspace{

plus 5pt minus 3pt\relax}
\def\halmos{\mbox{\rule{0.1in}{0.1in}}}
\newcommand{\xqedhere}[2]{
\rlap{\hbox to#1{\hfil\llap{#2}}}}
\newcommand{\xqed}[1]{
\leavevmode\unskip \hbox{}\hfill
\quad\hbox{#1}}
\def\IndexCorollaryGeneral#1#2{\index{#2!Corollary~\ref{#1}}}
\def\IndexCorollaryTheorems#1#2{\index{corollaries}{Corollary~\ref{#1}!{#2}}}
\def\IndexLemmaGeneral#1#2{\index{#2!Lemma~\ref{#1}}}
\def\IndexLemmaTheorems#1#2{\index{lemmas}{Lemma~\ref{#1}!{#2}}}
\def\ShowSolutionsP{0}
\newcommand{\solution}[1]{\ifnum\ShowSolutionsP=1 {\textbf{Solution: }#1}\fi}
\def\imp{\ \Rightarrow\ }
\newcommand{\dx}{1}{{\thinspace{\rm d}#1}}
\newcommand{\dfdx}[2]{\frac{\text{d}{#1}}{\text{d}{#2}}}
\newcommand{\abs}[1]{\left|#1\right|}
\def\limit#1#2#3{{\displaystyle\lim_{#1\rightarrow #2}#3}}
\newcommand{\eps}{\varepsilon}
\newcommand{\unif}{\stackrel{unif}{\longrightarrow}}
\newcommand{\ptwise}{\stackrel{ptwise}{\longrightarrow}}
\newcommand{\CC}{\mathbb {C}}
\newcommand{\DD}{\mathbb {D}}
\newcommand{\RR}{\mathbb {R}}
\newcommand{\QQ}{\mathbb {Q}}
\newcommand{\NN}{\mathbb {N}}
\newcommand{\ZZ}{\mathbb {Z}}
\newcommand{\lt}{<}
\newcommand{\gt}{>}
\newcommand{\amp}{&}
%% End: Author-provided macros
\begin{document}
\frontmatter
%% begin: half-title
\thispagestyle{empty}
{\titlepagefont\centering
\vspace*{0.28\textheight}
{\Huge How We Got From There To Here: A Story of Real Analysis}\\}
\clearpage
%% end:   half-title
%% begin: title page
%% Inspired by Peter Wilson's "titleDB" in "titlepages" CTAN package
\thispagestyle{empty}
{\titlepagefont\centering
\vspace*{0.14\textheight}
%% Target for xref to top-level element is ToC
\addtocontents{toc}{\protect\hypertarget{x:book:ASORA}{}}
{\Huge How We Got From There To Here: A Story of Real Analysis}\\[3\baselineskip]
{\Large Robert Rogers}\\[0.5\baselineskip]
{\Large SUNY, Fredonia\\
Fredonia, New York, USA}\\[3\baselineskip]
{\Large Eugene C. Boman}\\[0.5\baselineskip]
{\Large Penn State, Harrisburg campus\\
Harrisburg, PA, USA}\\}
\clearpage
%% end:   title page
%% begin: copyright-page
\thispagestyle{empty}
\hypertarget{x:colophon:FrontColophon}{}\vspace*{\stretch{2}}
\noindent\textcopyright{}2014\quad{}Robert Rogers and Eugene Boman\\[0.5\baselineskip]
How We Got from There to Here: A Story of Real Analysis by Eugene Boman and Robert Rogers is licensed under a \href{http://creativecommons.org/licenses/by-nc-sa/4.0/}{Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License}. \url{https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png}\par\medskip
\vspace*{\stretch{1}}
\null\clearpage
%% end:   copyright-page
%% begin: table of contents
%% Adjust Table of Contents
\setcounter{tocdepth}{0}
\renewcommand*\contentsname{Contents}
\tableofcontents
%% end:   table of contents
\mainmatter
%
%
\typeout{************************************************}
\typeout{Chapter 1 To the Instructor}
\typeout{************************************************}
%
\begin{chapterptx}{To the Instructor}{}{To the Instructor}{}{}{x:chapter:Instructor}
The irony of this section is that it exists to tell you that this book was not written for you; it was written for your students. After all, we don't need to teach you about Real Analysis. You already understand the subject. The purpose of this text is to help your students make sense of the formal definitions, theorems, and proofs that they will encounter in your course. We do this by immersing the student in the story of how what is usually called Calculus evolved into modern Real Analysis. Our hope and intention is that this will help the student to appreciate why their intuitive understanding of topics encountered in calculus needs to be replaced by the formalism of Real Analysis.%
\par
The traditional approach to this topic (what we might call the ``logical'' story of Real Analysis), starts with a rigorous development of the real number system and uses this to provide rigorous definitions of limits, continuity, derivatives and integrals, and convergence of series; typically in that order. This is a perfectly legitimate story of Real Analysis and, logically, it makes the most sense. Indeed, this is a view of the subject that every mathematician-in-training should eventually attain. However, it is our contention that your students will appreciate the subject more, and hopefully retain it better, if they see how the subject developed from the intuitive notions of Leibniz, Newton and others in the seventeenth and eighteenth centuries to the more modern approach developed in the nineteenth and twentieth centuries. After all, they are coming at it from a viewpoint very similar to that of the mathematicians of the seventeenth and eighteenth centuries. Our goal is to bring them into the nineteenth and early twentieth centuries, mathematically speaking.%
\par
We hasten to add that this is not a history of analysis book. It is an introductory textbook on Real Analysis which uses the historical context of the subject to frame the concepts and to show why mathematicians felt the need to develop rigorous, non-intuitive definitions to replace their intuitive notions.%
\par
You will notice that most of the problems are embedded in the chapters, rather than lumped together at the end of each chapter. This is done to provide a context for the problems which, for the most part, are presented on an as-needed basis.%
\par
Thus the proofs of nearly all of the theorems appear as problems in the text. Of course, it would be very unfair to ask most students at this level to prove, say, the Bolzano-Weierstrass Theorem without some sort of guidance. So in each case we provide an outline of the proof and the subsequent problem will be to use the outline to develop a formal proof. Proof outlines will become less detailed as the students progress. We have found that this approach helps students develop their proof writing skills.%
\par
We state in the text, and we encourage you to emphasize to your students, that often they will use the results of problems as tools in subsequent problems. Trained mathematicians do this naturally, but it is our experience that this is still somewhat foreign to students who are used to simply ``getting the problem done and forgetting about it.'' For quick reference, the page numbers of problems are listed in the table of contents.%
\par
The problems range from the fairly straightforward to the more challenging. Some of them require the use of a computer algebra system (for example, to plot partial sums of a power series). These tend to occur earlier in the book where we encourage the students to use technology to explore the wonders of series. A number of these problems can be done on a sufficiently advanced graphing calculator or even on Wolfram Alpha, so you should assure your students that they do not need to be super programmers to do this. Of course, this is up to you.%
\par
A testing strategy we have used successfully is to assign more time consuming problems as collected homework and to assign other problems as possible test questions. Students could then be given some subset of these (verbatim) as an in-class test. Not only does this make test creation more straightforward, but it allows the opportunity to ask questions that could not reasonably be asked otherwise in a timed setting. Our experience is that this does not make the tests ``too easy,'' and there are worse things than having students study by working together on possible test questions beforehand. If you are shocked by the idea of giving students all of the possible test questions ahead of time, think of how much (re)learning you did studying the list of possible questions you \emph{knew} would be asked on a qualifying exam.%
\par
In the end, use this book as you see fit. We believe your students will find it readable, as it is intended to be, and we are confident that it will help them to make sense out of the rigorous, non-intuitive definitions and theorems of Real Analysis and help them to develop their proof-writing skills.%
\par
If you have suggestions for improvement, comments or criticisms of our text please contact us at the email addresses below. We appreciate any feedback you can give us on this.%
\par
Thank you.%
\begin{equation*}
\begin{array}{lcl} \text{ Robert R. Rogers } \amp \amp  \text{ Eugene C. Boman } \\ \text{ robert.rogers@fredonia.edu }  \amp \amp  \text{ ecb5@psu.edu } \end{array}
\end{equation*}
%
\end{chapterptx}
%
%
\typeout{************************************************}
\typeout{Chapter 2 Prologue: Three Lessons Before We Begin}
\typeout{************************************************}
%
\begin{chapterptx}{Prologue: Three Lessons Before We Begin}{}{Prologue: Three Lessons Before We Begin}{}{}{x:chapter:Threelessons}
%
%
\typeout{************************************************}
\typeout{Section 2.1 Lesson One}
\typeout{************************************************}
%
\begin{sectionptx}{Lesson One}{}{Lesson One}{}{}{x:section:ThreeLessons-lesson-one}
Get a pad of paper and write down the answer to this question: What is .~.~.~No, really.  We're serious. \emph{Get a writing pad.} We'll wait.%
\par
Got it? Good.\footnote{We \emph{really} are serious about this. Get a pad of paper!\label{g:fn:idp1}} Now write down your answer to this question: What is a number? Don't think about it. Don't analyze it. Don't consider it. Just write down the best answer you can without thinking. You are the only person who ever needs to see what you've written.%
\par
Done? Good.%
\par
Now consider this: All of the objects listed below are ``numbers'' in a sense we will not make explicit here. How many of them does your definition include?%
\par
%
\begin{itemize}[label=\textbullet]
\item{}\(\displaystyle 1\)%
\item{}\(\displaystyle -1\)%
\item{}\(\displaystyle 0\)%
\item{}\(\displaystyle 3/5\)%
\item{}\(\displaystyle \sqrt{2}\)%
\item{}\(\displaystyle \sqrt{-1}\)%
\item{}\(\displaystyle i^i\)%
\item{}\(\displaystyle e^{5i}\)%
\item{}\(4+3i-2j+6k\) (this is called a quaternion)%
\item{}\(\dx{x}\) (this is the differential you learned all about in calculus)%
\item{}\(\begin{pmatrix}
1\amp 2\\-2\amp 1
\end{pmatrix}\) (yes, matrices can be considered numbers).%
\end{itemize}
%
\par
Surely you included \(1\). Almost surely you included \(3/5\). But what about \(0?\) \(-1?\) Does your definition include \(\sqrt{2}?\) Do you consider \(\dx{x}\) a number? Leibniz\index{Leibniz, Gottfried Wilhelm} did. Any of the others? (And, yes, they really are all ``numbers.'')%
\par
The lesson in this little demonstration is this: You don't really have a clear notion of what we mean when we use the word ``number.'' And this is fine. Not knowing is acceptable.\footnote{Sometimes it is even encouraged.\label{g:fn:idp2}} A principal goal of this course of study is to rectify this, at least a little bit. When the course is over you may or may not be able to give a better definition of the word ``number'' but you will have a deeper understanding of the real numbers at least. That is enough for now.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 2.2 Lesson Two}
\typeout{************************************************}
%
\begin{sectionptx}{Lesson Two}{}{Lesson Two}{}{}{x:section:ThreeLessons_lesson-two}
Read and understand the following development of the Quadratic Formula.%
\par
Suppose \(a\neq0\). If%
\begin{align}
ax^2+bx+c =0\label{x:mrow:eq_QForm1}\\
\intertext{then}
x^2+\frac{b}{a}x\amp =-\frac{c}{a}\label{x:mrow:eq_QForm2}\\
\intertext{Now let \(x=y-\frac{b}{2a}\) giving}
y^2\amp = -\frac{c}{a} +\frac{b^2}{4a^2}\label{x:mrow:eq_QForm3}\\
y\amp = \frac{\pm \sqrt{b^2-4ac}}{2a}\label{g:mrow:idp3}\\
x\amp = \frac{-b\pm\sqrt{b^2-4ac}}{2a}\label{x:mrow:eq_QForm4}
\end{align}
%
\par
Were you able to follow the argument? Probably the step from \hyperref[x:mrow:eq_QForm1]{equation~({\xreffont\ref{x:mrow:eq_QForm1}})} to \hyperref[x:mrow:eq_QForm2]{equation~({\xreffont\ref{x:mrow:eq_QForm2}})} presented no difficulties. But what about the next step? Do you see where \hyperref[x:mrow:eq_QForm3]{equation~({\xreffont\ref{x:mrow:eq_QForm3}})} came from? If so, good for you. Most students, in fact most mathematicians, cannot make that step in their heads. But are you sure? Is there, perhaps, some small detail you've overlooked?%
\begin{figureptx}{Tartaglia, ``The Stutterer''}{g:figure:idp4}{}%
\begin{image}{0.36}{0.28}{0.36}%
\includegraphics[width=\linewidth]{images/Tartaglia.png}
\end{image}%
\tcblower
\end{figureptx}%
Check to see.%
\par
That is, let \(x=y-\frac{b}{2a}\) in \hyperref[x:mrow:eq_QForm2]{equation~({\xreffont\ref{x:mrow:eq_QForm2}})} and see if you can get \hyperref[x:mrow:eq_QForm3]{equation~({\xreffont\ref{x:mrow:eq_QForm3}})}. Do it on that handy pad of paper we told you to get out earlier. Do it now. We'll wait.\footnote{If you \emph{still} haven't gotten out a pad of paper, give up now. You're going to fail this course. Seriously. Do you think we would spend so much time on this, that we would repeat it so many times, if it weren't important. \emph{\emph{GET OUT A PAD OF PAPER NOW!}} Last chance. You've been warned.\label{g:fn:idp5}}%
\par
Done? Good.%
\par
Perhaps you haven't been able to fill in the details on your own. That's ok. Many people can't. If not then get help: from a classmate, a friend, your instructor, whomever. Unfortunately most people won't get help in this situation. Instead they will perceive this as ``failure,'' hide it and berate themselves or the problem as ``stupid.'' In short they will let their personal insecurities and demons overwhelm them. \emph{Don't do this. Get help.} You are neither dumb nor incapable. There are a thousand reasons that on any given day you might not be able to solve this problem. But don't let a bad day interfere with the education you are here for. Get someone to help you over this hump. Later you will be able to help your helper in the same way. Really.%
\par
At this point we assume that you've successfully negotiated the transition from \hyperref[x:mrow:eq_QForm2]{equation~({\xreffont\ref{x:mrow:eq_QForm2}})} to \hyperref[x:mrow:eq_QForm4]{equation~({\xreffont\ref{x:mrow:eq_QForm4}})}.%
\par
See? It really wasn't that bad after all. Just a lot of elementary algebra. Now that you've done it (or seen it done), it is easy to see that there really wasn't much there.%
\begin{figureptx}{Girolomo Cardano}{g:figure:idp6}{}%
\index{Cardano, Girolomo}%
\begin{image}{0.36}{0.28}{0.36}%
\includegraphics[width=\linewidth]{images/Cardan.png}
\end{image}%
\tcblower
\end{figureptx}%
But this is the point! We left those computations out precisely because we knew that they were routine and that you could fill in the details. Moreover, filling in the details yourself gives you a little better insight into the computations. If we'd filled them in for you we would have robbed you of that insight. And we would have made this book longer than it needs to be. We don't want to do either of those things. If we fill in all of the details of every computation for you, you won't learn to have confidence in your ability to do them yourself. And this book will easily double in length.%
\par
So the lesson here is this: Keep that pad of paper handy whenever you are reading this (or any other) mathematics text. You will need it. Routine computations will often be skipped. But calling them ``routine'' and skipping them does not mean that they are unimportant. If they were truly unimportant we would leave them out entirely.%
\par
Moreover ``routine'' does not mean ``obvious.'' Every step we took in the development of the Quadratic Formula was ``routine.'' But even routine computations need to be understood and the best way to understand them is to do them. This is the way to learn mathematics; it is the \emph{only} way that really works. Don't deprive yourself of your mathematics education by skipping the most important parts.%
\begin{problem}{Alternative Quadratic Formula.}{g:problem:idp7}%
\index{Quadratic Formula!First Alternate Method}\index{Quadratic Formula!first proof}\index{first proof of the Quadratic Formula} As you saw when you filled in the details of our development of the Quadratic Formula\footnotemark{} the substitution \(x=y-\frac{b}{2a}\) was crucial because it turned%
\begin{equation*}
x^2+\frac{b}{a}x +\frac{c}{a}=0
\end{equation*}
into%
\begin{equation*}
y^2=k
\end{equation*}
where \(k\) depends only on \(a\), \(b\), and \(c\). In the sixteenth century a similar technique was used by Ludovico Ferrari (1522-1565) to reduce the general cubic equation%
\begin{equation}
ax^3+bx^2+cx+d=0\label{x:men:eq_GenCubic}
\end{equation}
into the so-called ``depressed cubic''%
\begin{equation*}
y^3 +py+q=0
\end{equation*}
where \(p\), and \(q\) depend only on \(a\), \(b\), \(c\), and \(d\).%
\par
The general depressed cubic\footnotemark{} had previously been solved by Tartaglia (the Stutterer, 1500-1557) so converting the general cubic into a depressed cubic provided a path for Ferrari to compute the ``Cubic Formula'' \textemdash{} like the Quadratic Formula but better.%
\par
Ferrari also knew how to compute the general solution of the ``depressed quartic'' so when he and his teacher Girolomo Cardano (1501-1576) figured out how to depress a general quartic they had a complete solution of the general quartic as well. Alas, their methods broke down entirely when they tried to solve the general quintic equation. Unfortunately the rest of this story belongs in a course on Abstract Algebra, not Real Analysis. But the lesson in this story applies to all of mathematics: Every problem solved is a new theorem which then becomes a tool for later use. Depressing a cubic would have been utterly useless had not Tartaglia had a solution of the depressed cubic in hand. The technique they used, with slight modifications, then allowed for a solution of the general quartic as well.%
\par
Keep this in mind as you proceed through this course and your mathematical education. Every problem you solve is really a theorem, a potential tool that you can use later. We have chosen the problems in this text deliberately with this in mind. Don't just solve the problems and move on. Just because you have solved a problem does not mean you should stop thinking about it. Keep thinking about the problems you've solved. Internalize them. Make the ideas your own so that when you need them later you will have them at hand to use.%
\begin{enumerate}[label=(\alph*)]
\item{}Find \(M\) so that the substitution \(x=y-M\) depresses \hyperref[x:men:eq_GenCubic]{equation~({\xreffont\ref{x:men:eq_GenCubic}})}, the general cubic equation. Then find \(p\) and \(q\) in terms of \(a\), \(b\), \(c\), and \(d\).%
\item{}Find \(K\) so that the substitution \(x=y-K\) depresses the general quartic equation. Make sure you demonstrate how you obtained that value or why it works (if you guessed it).%
\item{}Find \(N\) so that the substitution \(x=y-N\) depresses a polynomial of degree \(n\). Ditto on showing that this value works or showing how you obtained it.%
\end{enumerate}
%
\end{problem}
\footnotetext[4]{If you didn't fill in those details you're being stupid (or at least unduly stubborn). There is a good reason for putting these three lessons first. Stop wasting your time and intellect! Go do it now.\label{g:fn:idp8}}%
\footnotetext[5]{It is not entirely clear why eliminating the quadratic term should be depressing, but there it is.\label{g:fn:idp9}}%
\begin{problem}{Another Alternative Quadratic Formula.}{g:problem:idp10}%
\index{Quadratic Formula!Second Alternate Method}\index{Quadratic Formula!second proof}\index{second proof of the Quadratic Formula} Here is yet another way to solve a quadratic equation. Read the development below with pencil and paper handy. Confirm all of the computations that are not completely transparent to you. Then use your notes to present the solution with \emph{all} steps filled in.\footnotemark{}%
\par
Suppose that \(r_1\) and \(r_2\) are solutions of \(ax^2+bx+c=0\). Suppose further that \(r_1\ge r_2\). Then%
\begin{align*}
ax^2+bx+c \amp = a(x-r_1)(x-r_2)\\
\amp = a\left[x^2-(r_1+r_2)x+(r_1+r_2)^2-(r_1-r_2)^2-3r_1r_2\right]\text{.}
\end{align*}
%
\par
Therefore%
\begin{align}
r_1+r_2\amp = -\frac{b}{a}\label{x:mrow:eq_LagrangeQuadratic1}\\
\intertext{and}
r_1-r_2 \amp = \sqrt{\left(\frac{b}{a}\right)^2-\frac{4c}{a}}\text{.}\label{x:mrow:eq_LagrangeQuadratic2}
\end{align}
%
\par
Equations \hyperref[x:mrow:eq_LagrangeQuadratic1]{({\xreffont\ref{x:mrow:eq_LagrangeQuadratic1}})} and \hyperref[x:mrow:eq_LagrangeQuadratic2]{({\xreffont\ref{x:mrow:eq_LagrangeQuadratic2}})} can be solved simultaneously to yield%
\begin{align*}
r_1\amp =\frac{-b+\sqrt{b^2-4ac}}{2a}\\
r_2\amp =\frac{-b-\sqrt{b^2-4ac}}{2a}\text{.}
\end{align*}
%
\end{problem}
\footnotetext[6]{Be sure you are clear on the purpose of this problem before you begin. This is not about solving the Quadratic Equation. You already know how to do that. Our purpose here is to give you practice filling in the skipped details of mathematical exposition. We've chosen this particular problem because it should be a comfortable setting for you, but this particular solution is probably outside of your previous experience.\label{g:fn:idp11}}%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 2.3 Lesson Three}
\typeout{************************************************}
%
\begin{sectionptx}{Lesson Three}{}{Lesson Three}{}{}{x:section:ThreeLessons_lesson-three}
In the hustle and bustle of a typical college semester, with a lot of demands on your time and very little time to think, it becomes very easy to see each problem you solve as a small, isolated victory and then move on to the next challenge. This is understandable. Each problem you solve \emph{is} a small victory and you've every right to be proud of it. But it is not isolated and it is a mistake to think that it is.%
\par
In his book \emph{How to Solve It} the mathematician and teacher George Polya gave four steps for problem solving. The steps may be paraphrased as%
\begin{enumerate}
\item{}Understand the problem.%
\item{}Formulate a plan.%
\item{}Execute the plan.%
\item{}Reflect on what you've done.%
\end{enumerate}
%
\par
This process is iterative. That is, once a plan is formulated and executed we often find that our plan was not up to the task. So we have to ask what went wrong, form a new plan and try again. This is the fourth step: Reflect on what you've done.%
\par
Almost everyone remembers this fourth step when their plan \emph{doesn't} work. After all, you've got to try again so you have to ask what went wrong. But it is all too easy to neglect that crucial fourth step when the plan succeeds. In that case, flush with success we usually move on to the next problem and start over from scratch.%
\par
This is a mistake. Having solved a problem is no reason to stop thinking about it.%
\par
That fourth step is at least as important when we have succeeded as when we have failed. Each time you solve a problem stop and ask yourself a few questions:%
\begin{itemize}[label=\textbullet]
\item{}Are there any easy consequences that follow from the result?%
\item{}How does it fit into the broader scheme of other problems you have solved?%
\item{}How might it be used in the future?%
\end{itemize}
%
\begin{figureptx}{George Polya}{g:figure:idp12}{}%
\begin{image}{0.36}{0.28}{0.36}%
\includegraphics[width=\linewidth]{images/Polya.png}
\end{image}%
\tcblower
\end{figureptx}%
Also, notice the structure of the problem. Some assumptions had to be made. What were they? Were they all necessary? That is, did your solution use everything that was assumed? If not, you may have something considerably more general than it at first appears. What is that more general statement? Even if you used all of the assumptions, was that really necessary? Can you solve a similar problem with weaker assumptions?%
\par
Take a moment to pack all of these questions (and their answers) away in your mind so that when you see something similar in the future you will be reminded of it. \emph{Don't} solve any problem and then forget it and move on. The nature of mathematics is cumulative. Remember, you are not here to accumulate grade points. You are here to learn and understand the concepts and methods of mathematics, to gain ``mathematical maturity.'' Part of that maturation process is the accumulation of a body of facts (theorems), and techniques that can be used to prove new theorems (solve new problems).%
\par
This text has been written with the maturation process in mind. You will frequently find that the problems you solve today can be used to good effect in the ones you attempt tomorrow, but only if you remember them. So take a moment after you've solved each problem to think about how it fits into the patterns you already know. This is important enough to bear repeating: \emph{A problem, once solved, becomes a tool for solving subsequent problems!}%
\par
The purpose of the following sequence of problems is to help you become accustomed to this notion (if you aren't already). It is a progression of results about prime numbers. As you probably recall, a prime number is any integer greater than \(1\) whose only factors are itself and \(1\). For example, \(2\), \(3\), \(5\), \(7\), \(11\) are prime, while \(4\), \(6\), \(9\) are not. A major result about prime numbers is the following:%
\begin{theorem}{}{}{x:theorem:thm_FTA}%
\alert{The Fundamental Theorem of Arithmetic}%
\par
Any integer greater than \(1\) is either prime or it is a product of prime numbers. Furthermore, this prime decomposition is unique up to the order of the factors.%
\end{theorem}
We will not prove this, but we will use it as a starting point to examine the following problems. As you do these problems, notice how subsequent problems make use of the previous results.%
\par
Notice that the notation \(p\,|a\) simply means that the integer \(p\) divides the integer \(a\) with no remainder.%
\begin{problem}{Fermat's Little Theorem, step 1.}{g:problem:idp13}%
\index{Fermat's Little Theorem!first problem leading to}\index{if a prime divides a product of two numbers then it divides one of the factors} Let \(p\) be a prime number and \(a, b\) positive integers such that \(p\, | (a\cdot b)\). Show that \(p\,|a\) or \(p\,|b\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp14}{}\quad{}If \(p\,|a\) then we are done. If not then notice that \(p\) is a prime factor of \(a\cdot b\). What does the Fundamental Theorem of Arithmetic say about the prime factors of \(a\cdot b\) compared to the prime factors of \(a\) and \(b?\)%
\end{problem}
\begin{problem}{Fermat's Little Theorem, step 2.}{g:problem:idp15}%
\index{Fermat's Little Theorem!second problem leading to}\index{if a prime divides an arbitrary product then it divides one of the factors} Let \(p\) be a prime number and let \(a_1, a_2, \ldots,
a_n\) be positive integers such that \(p\,|\left(a_1\cdot
a_2\cdot a_3\cdot\ldots\cdot a_n\right)\).  Show that \(p\,|a_k\) for some \(k\in\{1, 2, 3, \ldots, n\}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp16}{}\quad{}Use induction on \(n\) and the result of the previous problem.%
\end{problem}
\begin{problem}{Fermat's Little Theorem, step 3.}{g:problem:idp17}%
\index{Fermat's Little Theorem!third problem leading to}\index{if \(p\) is prime then \(p\) divides \(p \choose{}k\)} Let \(p\) be a prime number and let \(k\) be an integer with \(1\le k\le p-1\). Prove that \(p\left|{p \choose{}k}\right.\), where \({p \choose{}k}\) is the binomial coefficient \(\frac{p!}{k!(p-k)!}\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp18}{}\quad{}We know \(p\,|p\,!\), so \(p\,|{p
\choose{}k}k!(p-k)!\).  How does the previous result apply?%
\end{problem}
We now have all the machinery in place to prove one of the really cool theorems from number theory.%
\begin{theorem}{}{}{x:theorem:thm_FermatsLittleTheorem}%
\alert{Fermat's Little Theorem}%
\par
\index{Fermat's Little Theorem} Let \(p\) be any prime number. Then \(p\,|(n^p-n)\) for all positive integers \(n\).%
\end{theorem}
\begin{problem}{Fermat's Little Theorem.}{g:problem:idp19}%
\index{Fermat's Little Theorem} Prove Fermat's Little Theorem.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp20}{}\quad{}Use induction on \(n\). To get from \(n\) to \(n+1\), use the binomial theorem on \((n+1)^p\).%
\end{problem}
Fermat's Little Theorem is the foundational basis for a number of results in number theory and encryption.%
\end{sectionptx}
\end{chapterptx}
%
%
\typeout{************************************************}
\typeout{Part I In Which We Raise A Number Of Questions}
\typeout{************************************************}
%
\begin{partptx}{In Which We Raise A Number Of Questions}{}{In Which We Raise A Number Of Questions}{}{}{x:part:AskingQuestions}
 %
%
\typeout{************************************************}
\typeout{Chapter 3 Numbers, Real (\(\RR\)) and Rational (\(\QQ\))}
\typeout{************************************************}
%
\begin{chapterptx}{Numbers, Real (\(\RR\)) and Rational (\(\QQ\))}{}{Numbers, Real (\(\RR\)) and Rational (\(\QQ\))}{}{}{x:chapter:NumbersRealRational}
\begin{introduction}{}%
The set of real numbers (denoted, \(\RR\)) is badly named. The real numbers are no more or less real \textemdash{} in the non-mathematical sense that they exist \textemdash{} than any other set of numbers, just like the set of rational numbers (\(\QQ\)), the set of integers (\(\ZZ\)), or the set of natural numbers (\(\NN\)). The name ``real numbers'' is (almost) an historical anomaly not unlike the name ``Pythagorean Theorem'' which was actually known and understood long before Pythagoras lived.%
\par
When calculus was being invented\footnote{Some would say ``re-invented.'' See \hyperlink{x:biblio:russo96__forgot_revol}{[{\xreffont 13}]}, or \hyperlink{x:biblio:netz07__archim_codex}{[{\xreffont 9}]}.  \label{g:fn:idp21}} in the \(17\)th century, numbers were thoroughly understood, or so it was believed. They were, after all, just numbers. Combine them. We call that addition. If you add them repeatedly we call it multiplication. Subtraction and division were similarly understood.%
\par
It was (and still is) useful to visualize these things in a more concrete way. If we take a stick of length 2 and another of length 3 and lay them end-to-end we get a length of 5. This is addition. If we lay them end-to-end but at right angles then our two sticks are the length and width of a rectangle whose area is 6. This is multiplication.%
\par
Of course measuring lengths with whole numbers has limitations, but these are not hard to fix. If we have a length (stick) of length 1 and another of length 2, then we can find another whose length when compared to 1 is the same (has the same proportion as) as 1 is to 2. That number of course, is \(1/2\).%
\begin{figureptx}{}{x:figure:Fractions}{}%
\begin{image}{0.125}{0.75}{0.125}%
\includegraphics[width=\linewidth]{images/Fractions.png}
\end{image}%
\tcblower
\end{figureptx}%
Notice how fraction notation reflects the operation of comparing 1 to 2.  This comparison is usually referred to as the \emph{ratio} of \(1\) to \(2\) so numbers of this sort are called \emph{rational numbers.} The set of rational numbers is denoted \(\QQ\) for quotients.  In grade school they were introduced to you as fractions.  Once fractions are understood, this visualization using line segments (sticks) leads quite naturally to their representation with the rational number line.%
\begin{figureptx}{The Rational Number Line}{x:figure:RationalNumberLine}{}%
\begin{image}{0}{1}{0}%
\includegraphics[width=\linewidth]{images/RationalNumberLine.png}
\end{image}%
\tcblower
\end{figureptx}%
This seems to work as a visualization because the rational numbers and the points on a line seem to share certain properties.  Chief among these is that between any two points on the rational line there is another point, just as between any two rational numbers there is another rational number.%
\begin{problem}{}{g:problem:idp22}%
\index{\(\QQ\)!rational numbers exist between rational numbers} Let \(a, b, c, d\in\NN\) and find a rational number between \(a/b\) and \(c/d\).%
\end{problem}
This is all very clean and satisfying until we examine it just a bit closer. Then it becomes quite mysterious. Consider again the rational numbers \(a/b\) and \(c/d\). If we think of these as lengths we can ask, "Is there a third length, say \(\alpha\), such that we can divide \(a/b\) into \(M\) pieces, each of length \(\alpha\) \emph{and also} divide \(c/d\) into \(N\) pieces each of length \(\alpha?\)" A few minutes thought should convince you that this is the same as the problem of finding a common denominator so \(\alpha=\frac{1}{bd}\) will work nicely. (Confirm this yourself.)%
\par
You may be wondering what we're making all of this fuss about. \emph{Obviously} this is \emph{always} true. In fact the previous paragraph gives an outline of a very nice little proof of this. Here are the theorem and its proof presented formally.%
\begin{theorem}{}{}{x:theorem:thm_CommonDenominatorsExist}%
\index{common denominators} Let \(a, b, c\), and \(d\) be integers. There is a number \(\alpha\in\QQ\) such that \(M\alpha=a/b\) and \(N\alpha=c/d\) where \(M\) and \(N\) are also integers.%
\end{theorem}
\begin{proof}{}{g:proof:idp23}
To prove this theorem we will display \(\alpha\), \(M\) and \(N\). It is your responsibility to confirm that these actually work. Here they are: \(\alpha=1/bd\), \(M=ad\), and \(N=cb\).%
\end{proof}
\begin{problem}{}{g:problem:idp24}%
\index{common denominators} Confirm that \(\alpha, M, \text{ and } N\) as given in the proof of \hyperref[x:theorem:thm_CommonDenominatorsExist]{Theorem~{\xreffont\ref{x:theorem:thm_CommonDenominatorsExist}}} satisfy the requirements of the theorem.%
\end{problem}
It should be clear that it is necessary for \(a\), \(b\), \(c\), and \(d\) to be integers for everything to work out. Otherwise \(M\) and \(N\) will not also be integers as required.%
\par
This suggests the following very deep and important question: Are there lengths which can \emph{not} be expressed as the ratio of two integer lengths? The answer, of course, is yes. Otherwise we wouldn't have asked the question. Notice that for such numbers our proof of \hyperref[x:theorem:thm_CommonDenominatorsExist]{Theorem~{\xreffont\ref{x:theorem:thm_CommonDenominatorsExist}}} is not valid (why not?).%
\par
One of the best known examples of such a number is the circumference of a circle with diameter 1. This is the number usually denoted by \(\pi\). But circles are extremely complex objects \textemdash{} they only seem simple because they are so familiar. Arising as it does from a circle, you would expect the number \(\pi\) to be very complex as well and this is true. In fact \(\pi\) is an exceptionally weird number for a variety of reasons. Let's start with something a little easier to think about.%
\par
Squares are simple. Two sets of parallel lines at right angles, all of the same length. What could be simpler? If we construct a square with sides having length 1 then its diagonal has length \(\sqrt{2}\).%
\begin{figureptx}{A construction of \(\sqrt{2}\)}{x:figure:Sqrt2}{}%
\begin{image}{0.25}{0.5}{0.25}%
\includegraphics[width=\linewidth]{images/Sqrt2.png}
\end{image}%
\tcblower
\end{figureptx}%
\index{\(\sqrt{2}\)!is irrational} This is a number which cannot be expressed as the \emph{ratio} of two integers.  That is, it is \emph{irrational.} This has been known since ancient times, but it is still quite disconcerting when first encountered.  It seems so counter-intuitive that the intellect rebels.  ``This can't be right,'' it says. ``That's just crazy!''%
\par
Nevertheless it is true and we can prove it is true as follows.%
\par
What happens if we suppose that the square root of two \emph{can} be expressed as a ratio of integers? We will show that this leads irrevocably to a conclusion that is manifestly not true.%
\par
Suppose \(\sqrt{2}=a/b\) where \(a\) and \(b\) are integers. Suppose further that the fraction \(a/b\) is in lowest terms. \emph{This assumption is crucial because if \(a/b\) is in lowest terms we know that at most only one of them is even.}%
\par
So%
\begin{align*}
\frac{a}{b} \amp = \sqrt{2}.\\
\intertext{Squaring both sides gives:}
a^2 \amp = 2b^2.\\
\intertext{Therefore \(a^2\) is even. But if \(a^2\) is even then \(a\) must be even also (why?). If \(a\) is even then \(a=2k\) for some integer \(k\). Therefore}
4k^2\amp =2b^2\textit{ or}\\
2k^2\amp = b^2\text{.}
\end{align*}
%
\par
Therefore \(b^2\) is also even and so \(b\) must be even too. But this is impossible. We've just concluded that \(a\) and \(b\) are both even and this conclusion follows directly from our initial assumption that at most one of them could be even.%
\par
This is nonsense. Where is our error? It is not in any single step of our reasoning. That was all solid. Check it again to be sure.%
\par
Therefore our error must be in the initial assumption that \(\sqrt{2}\) could be expressed as a fraction. That assumption must therefore be false. In other words, \(\sqrt{2}\) cannot be so expressed.%
\begin{problem}{Irrational Numbers.}{g:problem:idp25}%
Show that each of the following numbers is irrational:%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\(\sqrt{3}\)%
\item[(b)]\(\sqrt{5}\)%
\item[(c)]\(\sqrt[3]{2}\)%
\item[(d)]\(i\) \((=\sqrt{-1})\)%
\item[(e)]The square root of every positive integer which is not the square of an integer.%
\end{enumerate}
\end{problem}
\index{\(\sqrt{2}\)!meaning of} The fact that \(\sqrt{2}\) is not rational is cute and interesting, but unless, like the Pythagoreans of ancient Greece, you have a strongly held religious conviction that all numbers are rational, it does not seem terribly important. On the other hand, the very existence of \(\sqrt{2}\) raises some interesting questions. For example what can the symbol \(4^{\sqrt{2}}\) possibly mean? If the exponent were a rational number, say \(m/n\), then clearly \(4^{m/n}=\sqrt[n]{4^m}\). But since \(\sqrt{2}\neq m/n\) for \emph{any} integers \(m\) and \(n\) how do we interpret \(4^{\sqrt{2}}?\) Does it have any meaning at all. The more you think about this, the more puzzling the existence of irrational numbers becomes. Suppose for example we reconsider the construction of a line segment of length \(\sqrt{2}\). It is clear that the construction works and that we really can build such a line segment. It exists.%
\par
Repeat the construction but this time let's put the base side on the rational line.%
\begin{figureptx}{}{x:figure:Sqrt2OnRatLine}{}%
\begin{image}{0.25}{0.5}{0.25}%
\includegraphics[width=\linewidth]{images/Sqrt2OnRatLine.png}
\end{image}%
\tcblower
\end{figureptx}%
We know that the diagonal of this square is \(\sqrt{2}\) as indicated. And we know that \(\sqrt{2}\) is not a rational number.%
\par
Now leave the diagonal pinned at \((0,0)\) but allow it to rotate down so that it coincides with the \(x-\)axis.%
\begin{figureptx}{}{x:figure:Sqrt2Irrational}{}%
\begin{image}{0.25}{0.5}{0.25}%
\includegraphics[width=\linewidth]{images/Sqrt2Irrational.png}
\end{image}%
\tcblower
\end{figureptx}%
The end of our diagonal will trace out an arc of the circle with radius \(\sqrt{2}\). When the diagonal coincides with the \(x-\)axis, its endpoint will obviously be the point \((\sqrt{2}, 0)\) as shown.%
\par
But wait! We're using the \emph{rational} number line for our \(x-\)axis. That means the only points on the \(x-\)axis are those that correspond to rational numbers (fractions). But we know that \(\sqrt{2}\) is not rational! Conclusion: There is no point \((\sqrt{2},0)\). It simply doesn't exist.%
\par
Put differently, there is a hole in the rational number line right where \(\sqrt{2}\) should be.%
\par
This is weird!%
\par
Recall that between any two rational numbers there is always another. This fact is what led us to represent the rational numbers with a line in the first place.%
\begin{figureptx}{}{x:figure:RationalLineWithHoles}{}%
\begin{image}{0}{1}{0}%
\includegraphics[width=\linewidth]{images/RationalLineWithHoles.png}
\end{image}%
\tcblower
\end{figureptx}%
But it's even worse than that. It's straightforward to show that \(\sqrt{3}\), \(\sqrt{5}\), etc. are all irrational too. So are \(\pi\) and \(e\), though they aren't as easy to show. It seems that the rational line has a bunch of holes in it. Infinitely many.%
\par
And yet, the following theorem is true%
\begin{theorem}{}{}{x:theorem:thm_IrrationalBetweenIrrationals}%
\index{\(\RR\)!real numbers exist between real numbers}\index{real numbers exist between real numbers}%
\begin{enumerate}[label=(\alph*)]
\item{}Between any two distinct real numbers there is a rational number.%
\item{}Between any two distinct real numbers there is an irrational number.%
\end{enumerate}
%
\end{theorem}
Both parts of this theorem rely on a judicious use of what is now called the Archimedean Property of the Real Number System, which can be formally stated as follows.%
\begin{principle}{The Archimedean Property.}{}{g:principle:idp26}%
\index{Archimedean Property} Given any two positive real numbers, \(a\) and \(b\), there is a positive integer, \(n\) such that \(na>b\).%
\end{principle}
Physically this says that we can empty an ocean \(b\) with a teaspoon \(a\), provided we are willing to use the teaspoon a large number of times \(n\).%
\par
This is such an intuitively straightforward concept that it is easy to accept it without proof. Until the invention of calculus, and even for some time after that, it was simply assumed. However as the foundational problems posed by the concepts of calculus were understood and solved we were eventually led to a deeper understanding of the complexities of the real number system. The Archimedean Property is no longer taken as an unproved axiom, but rather it is now understood to be a consequence of other axioms. We will show this later, but for now we will accept it as obviously true just as Archimedes did.%
\par
With the invention of calculus, mathematicians of the seventeenth century began to use objects which didn't satisfy the Archimedean Property (in fact, so did Archimedes). As we shall see in the next chapter, when Leibniz wrote the first paper on his version of the calculus, he\index{Leibniz, Gottfried Wilhelm!and infinitesimals} followed this practice by explicitly laying out rules for manipulating infinitely small quantities (infinitesimals). These were taken to be actual numbers which are not zero and yet smaller than any real number. The notation he used was \(\dx{ x}\) (an infinitely small displacement in the \(x\) direction), and \(\dx{ y}\) (an infinitely small displacement in the \(y\) direction). These symbols should look familiar to you. They are the same \(\dx{ y}\) and \(\dx{ x}\) used to form the derivative symbol \(\dfdx{y}{x}\) that you learned about in calculus.%
\par
Mathematicians of the seventeenth and eighteenth centuries made amazing scientific and mathematical progress exploiting these infinitesimals, even though they were foundationally suspect. No matter how many times you add the infinitesimal \(\dx{ x}\) to itself the result will not be bigger than, say \(10^{-1000}\), which is very bizarre.%
\par
When foundational issues came to the forefront, infinitesimals fell somewhat out of favor. You probably didn't use them very much in calculus. Most of the time you probably used the prime notation, \(f^\prime(x)\) introduced by Lagrange \index{Lagrange, Joseph-Louis} in the eighteenth century. Some of the themes in this book are: Why differentials fell out of favor, what were they replaced with and how the modern notations you learned in calculus evolved over time.%
\par
To conclude this aside on the Archimedean Property, the idea of infinitesimals was revisited in the twentieth century by the logician Abraham Robinson in~\hyperlink{x:biblio:robinson74__non_stand_analy}{[{\xreffont 12}]}. Robinson was able to put the idea of infinitesimals on a solid logical foundation. But in the 18th century, the existence of infinitesimal numbers was shaky to say the very least. However this did not prevent mathematicians from successfully exploiting these infinitely small quantities.%
\par
We will come back to this saga in later chapters, but for now we return to \hyperref[x:theorem:thm_IrrationalBetweenIrrationals]{Theorem~{\xreffont\ref{x:theorem:thm_IrrationalBetweenIrrationals}}}.%
\begin{proof}{Sketch of Proof.}{g:proof:idp27}
We will outline the proof of part (a) of \hyperref[x:theorem:thm_IrrationalBetweenIrrationals]{Theorem~{\xreffont\ref{x:theorem:thm_IrrationalBetweenIrrationals}}} and indicate how it can be used to prove part b.%
\par
Let \(\alpha\) and \(\beta\) be real numbers with \(\alpha > \beta\). There are two cases.%
\begin{itemize}[label=\textbullet]
\item{}\lititle{Case 1:.}\par%
\(\alpha-\beta > 1\). In this case there is at least one integer between \(\alpha\) and \(\beta\). Since integers are rational we are done.%
\item{}\lititle{Case 2:.}\par%
\(\alpha-\beta \le 1\).  In this case, by the Archimedean Property there is a positive integer, say \(n\), such that \(n(\alpha-\beta) = n\alpha-n\beta
> 1\).  Now there will be an integer between \(n\alpha\) and \(n\beta\).  You should now be able to find a rational number between \(\alpha\) and \(\beta\).%
\end{itemize}
%
\end{proof}
For part b, divide \(\alpha\) and \(\beta\) by any positive irrational number and apply part a. There are a couple of details to keep in mind. These are considered in the following problem.%
\begin{problem}{}{g:problem:idp28}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Prove that the product of a nonzero rational number and an irrational number is irrational.%
\item[(b)]Use the result of part (a) to prove \hyperref[x:theorem:thm_IrrationalBetweenIrrationals]{Theorem~{\xreffont\ref{x:theorem:thm_IrrationalBetweenIrrationals}}}.%
\end{enumerate}
\end{problem}
As a practical matter, the existence of irrational numbers isn't really very important. In light of \hyperref[x:theorem:thm_IrrationalBetweenIrrationals]{Theorem~{\xreffont\ref{x:theorem:thm_IrrationalBetweenIrrationals}}}, any irrational number can be approximated arbitrarily closely by a rational number. So if we're designing a bridge and \(\sqrt{2}\) is needed we just use \(1.414\) instead. The error introduced is less than \(0.001 =1/1000\) so it probably doesn't matter.%
\par
But from a theoretical point of view this is devastating. When calculus was invented, the rational numbers were suddenly not up to the task of justifying the concepts and operations we needed to work with.%
\par
Newton\index{Newton, Isaac!foundation of calculus} explicitly founded his version of calculus on the assumption that we can think of variable quantities as being generated by a continuous motion. If our number system has holes in it such continuous motion is impossible because we have no way to jump over the gaps. So Newton simply postulated that there were no holes. He filled in the hole where \(\sqrt{2}\) should be. He simply said, yes there is a number there called \(\sqrt{2}\) and he did the same with all of the other holes.%
\par
To be sure there is no record of Newton explicitly saying, ``Here's how I'm going to fill in the holes in the rational number line.'' Along with everyone else at the time, he simply assumed there were no holes and moved on. It took about \(200\) years of puzzling and arguing over the contradictions, anomalies and paradoxes to work out the consequences of that apparently simple assumption. The task may not yet be fully accomplished, but by the 20th century the properties of the real number system (\(\RR\)) as an extension of the rational number system (\(\QQ\)) were well understood. Here are both systems visualized as lines:%
\begin{figureptx}{\(\RR\) and \(\QQ\)}{x:figure:RandQ}{}%
\begin{image}{0}{1}{0}%
\includegraphics[width=\linewidth]{images/RandQ.png}
\end{image}%
\tcblower
\end{figureptx}%
Impressive, no?%
\par
The reason they look alike, except for the labels \(\RR\) and \(\QQ\) of course, is that our ability to draw sketches of the objects we're studying utterly fails when we try to sketch \(\RR\), as different from \(\QQ\). All of the holes in \(\QQ\) really are there, but the non-holes are packed together so closely that we can't separate them in a drawing. This inability to sketch the objects we study will be a frequent source of frustration.%
\par
Of course, this will not stop us from drawing sketches. When we do, our imaginations will save us because it \emph{is} possible to \emph{imagine} \(\QQ\) as distinct from \(\RR\). But put away the idea that a sketch is an accurate representation of anything. At best our sketches will only be aids to the imagination.%
\par
So, at this point we will simply assume the existence of the real numbers. We will assume also that they have all of the properties that we are used to. This is perfectly acceptable as long as we make our assumptions explicit. However we need to be aware that, so far, the existence and properties of the real numbers is an \emph{assumption} that has not been logically derived. Any time we make an assumption we need to be prepared to either abandon it completely if we find that it leads to nonsensical results, or to re-examine the assumption in the light of these results to see if we can find another assumption that subsumes the first and explains the (apparently) nonsensical results.%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Section 3.1 Additional Problems}
\typeout{************************************************}
%
\begin{sectionptx}{Additional Problems}{}{Additional Problems}{}{}{x:section:NumbersRealRational-AddProbs}
\begin{problem}{}{g:problem:idp29}%
Determine if each of the following is always rational or always irrational. Justify your answers.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]The sum of two rational numbers.%
\item[(b)]The sum of two irrational numbers.%
\item[(c)]The sum of a rational and an irrational number.%
\end{enumerate}
\end{problem}
\begin{problem}{}{g:problem:idp30}%
\index{\(\QQ\)!creating irrationals from rationals}\index{irrational numbers}\index{\(\QQ\)!is it possible to have two rational numbers, \(a\) and \(b\), such that \(a^b\) is irrational} Is it possible to have two rational numbers, \(a\) and \(b\), such that \(a^b\) is irrational? If so, display an example of such \(a\) and \(b\). If not, prove that it is not possible.%
\end{problem}
\begin{problem}{}{g:problem:idp31}%
Decide if it is possible to have two irrational numbers, \(a\) and \(b\), such that \(a^b\) is rational. Prove it in either case.%
\end{problem}
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Chapter 4 Calculus in the 17th and 18th Centuries}
\typeout{************************************************}
%
\begin{chapterptx}{Calculus in the 17th and 18th Centuries}{}{Calculus in the 17th and 18th Centuries}{}{}{x:chapter:CalcIn17th18thCentury}
%
%
\typeout{************************************************}
\typeout{Section 4.1 Newton and Leibniz Get Started}
\typeout{************************************************}
%
\begin{sectionptx}{Newton and Leibniz Get Started}{}{Newton and Leibniz Get Started}{}{}{x:section:CalcIn17th18thCentury-NewtLeibStart}
%
%
\typeout{************************************************}
\typeout{Subsection 4.1.1 Leibniz's Calculus Rules}
\typeout{************************************************}
%
\begin{subsectionptx}{Leibniz's Calculus Rules}{}{Leibniz's Calculus Rules}{}{}{x:subsection:sec_leibn-calc-rules}
\begin{figureptx}{Gottfried Wilhelm Leibniz}{g:figure:idp32}{}%
\index{Leibniz, Gottfried Wilhelm!portrait of}%
\begin{image}{0.325}{0.35}{0.325}%
\includegraphics[width=\linewidth]{images/Leibniz.png}
\end{image}%
\tcblower
\end{figureptx}%
The rules for calculus were first laid out in Gottfried Wilhelm Leibniz's 1684 paper\index{Leibniz, Gottfried Wilhelm!first calculus publication} \textit{Nova methodus pro maximis et minimis, itemque tangentibus, quae nec fractas nec irrationales, quantitates moratur, et singulare pro illi calculi genus} (A New Method for Maxima and Minima as Well as Tangents, Which is Impeded Neither by Fractional Nor by Irrational Quantities, and a Remarkable Type of Calculus for This). Leibniz started with subtraction.  That is, if \(x_1\) and \(x_2\) are very close together then their difference, \(\Delta
x=x_2-x_1\), is very small.  He expanded this idea to say that if \(x_1\) and \(x_2\) are \emph{infinitely} close together (but still distinct) then their difference, \(\dx{ x}\), is infinitesimally small (but not zero).%
\par
This idea is logically very suspect and Leibniz knew it.  But he also knew that when he used his \textit{calculus differentialis}\footnote{This translates, loosely, as the calculus of differences.\label{g:fn:idp33}} he was getting correct answers to some very hard problems.  So he persevered.%
\par
Leibniz called both \(\Delta x\) and \(\dx{ x}\) ``differentials'' (Latin for difference) because he thought of them as, essentially, the same thing.  Over time it has become customary to refer to the infinitesimal \(\dx{ x}\) as a differential, reserving ``difference'' for the finite case, \(\Delta x\).  This is why calculus is often called ``differential calculus.''%
\par
In his paper Leibniz gave rules for dealing with these infinitely small differentials.  Specifically, given a variable quantity \(x\), \(dx\) represented an infinitesimal change in \(x\).  Differentials are related via the slope of the tangent line to a curve.  That is, if \(y=f(x)\), then \(\dx{ y}\) and \(\dx{ x}\) are related by%
\begin{equation*}
\dx{ y}=\text{ (slope of the tangent line) } \cdot \dx{ x}\text{.}
\end{equation*}
%
\par
Leibniz then divided by \(\dx{ x}\) giving%
\begin{equation*}
\dfdx{y}{x}= \text{ (slope of the tangent line). }
\end{equation*}
%
\par
The elegant and expressive notation Leibniz invented was so useful that it has been retained through the years despite some profound changes in the underlying concepts.  For example, Leibniz and his contemporaries would have viewed the symbol \(\dfdx{y}{x}\) as an actual quotient of infinitesimals, whereas today we define it via the limit concept first suggested by Newton.  \index{Newton, Isaac}%
\par
As a result the rules governing these differentials are very modern in appearance: \index{Leibniz, Gottfried Wilhelm!differentiation rules}%
\begin{align*}
\dx{(\text{ constant } )}\amp =0\\
\dx{(z-y+w+x)}\amp =\dx{ z}-\dx{ y}+\dx{ w}+\dx{ x}\\
\dx{(xv)}\amp =x\dx{ v}+v\dx{ x}\\
\dx{\left(\frac{v}{y}\right)}\amp =\frac{y\dx{ v}-v\dx{ y}}{yy}\\
\intertext{and, when \(a\) is an integer:}
\dx{(x^a)}\amp =ax^{a-1}\dx{ x}\text{.}
\end{align*}
%
\par
Leibniz states these rules without proof: ``. . . the demonstration of all this will be easy to one who is experienced in such matters . . ..'' As an example, mathematicians in Leibniz's day would be expected to understand intuitively that if \(c\) is a constant, then \(d(c)=c-c=0\).  Likewise, \(d(x+y)=dx+dy\) is really an extension of \((x_2+y_2)-(x_1+y_1)=(x_2-x_1)+(y_2-y_1)\).%
\end{subsectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection 4.1.2 Leibniz's Approach to the Product Rule}
\typeout{************************************************}
%
\begin{subsectionptx}{Leibniz's Approach to the Product Rule}{}{Leibniz's Approach to the Product Rule}{}{}{x:subsection:LeibnizProductRule}
\index{Leibniz, Gottfried Wilhelm} The explanation of the product rule using differentials is a bit more involved, but Leibniz expected that mathematicans would be fluent enough to derive it.  The product \(p=xv\) can be thought of as the area of the following rectangle%
\begin{figureptx}{}{x:figure:fig1}{}%
\begin{image}{0.25}{0.5}{0.25}%
\includegraphics[width=\linewidth]{images/fig1.png}
\end{image}%
\tcblower
\end{figureptx}%
With this in mind, \(\dx{ p}=\dx{(xv)}\) can be thought of as the change in area when \(x\) is changed by \(\dx{ x}\) and \(v\) is changed by \(\dx{ v}\).  This can be seen as the L shaped region in the following drawing.%
\begin{figureptx}{}{x:figure:fig2}{}%
\begin{image}{0.05}{0.9}{0.05}%
\includegraphics[width=\linewidth]{images/fig2.png}
\end{image}%
\tcblower
\end{figureptx}%
By dividing the L shaped region into 3 rectangles we obtain%
\begin{equation}
\dx{(xv)}=x\dx{ v}+v\dx{ x}+\dx{ x}\,\dx{ v}\text{.}\label{x:men:eq_LeibnizProductRule}
\end{equation}
%
\par
Even though \(\dx{ x}\) and \(\dx{ v}\) are infinitely small, Leibniz reasoned that \(\dx{ x}\,\dx{ v}\) is \emph{even more} infinitely small (quadratically infinitely small?)  compared to \(x\dx{ v}\) and \(v\dx{ x}\) and can thus be ignored leaving%
\begin{equation*}
\dx{ (xv)}=x\dx{ v}+v\dx{ x}\text{.}
\end{equation*}
%
\par
You should feel some discomfort at the idea of simply tossing the product \(\dx{ x}\,\dx{ v}\) aside because it is ``comparatively small.'' This means you have been well trained, and have thoroughly internalized Newton's \index{Newton, Isaac} dictum~\hyperlink{x:biblio:newton45__sir_isaac_two_treat_quadr}{[{\xreffont 10}]}: ``The smallest errors may not, in mathematical matters, be scorned.'' It is logically untenable to toss aside an expression just because it is small.  Even less so should we be willing to ignore an expression on the grounds that it is ``infinitely smaller'' than another quantity which is itself ``infinitely small.''%
\par
Newton and Leibniz both knew this as well as we do.  But they also knew that their methods \emph{worked}.  They gave verifiably correct answers to problems which had, heretofore, been completely intractable.  It is the mark of their genius that both men persevered in spite of the very evident difficulties their methods entailed.%
\end{subsectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection 4.1.3 Newton's Approach to the Product Rule}
\typeout{************************************************}
%
\begin{subsectionptx}{Newton's Approach to the Product Rule}{}{Newton's Approach to the Product Rule}{}{}{x:subsection:NewtonsApproach}
In the Principia, Newton ``proved'' the Product Rule as follows: Let \(x\) and \(v\) be ``flowing quantites'' and consider the rectangle, \(R\), whose sides are \(x\) and \(v\).  \(R\) is also a flowing quantity and we wish to find its fluxion (derivative) at any time.%
\begin{historical}{Newton's `Method of Fluxions'.}{g:historical:idp34}%
Newton's approach to calculus \textemdash{} his `Method of Fluxions' \textemdash{} depended fundamentally on motion. That is, he viewed his variables (fluents) as changing (flowing or fluxing) in time.  The rate of change of a fluent he called a fluxion.  As a foundation both Leibniz's and Newton's approaches have fallen out of favor, although both are still universally used as a conceptual approach, a ``way of thinking,'' about the ideas of calculus.%
\end{historical}
First increment \(x\) and \(v\) by \(\frac{\Delta x}{2}\) and \(\frac{\Delta v}{2}\) respectively. Then the corresponding increment of \(R\) is%
\begin{equation}
\left(x+\frac{\Delta x}{2}\right)\left(v+\frac{\Delta v}{2}\right) = xv + x\frac{\Delta v}{2} + v\frac{\Delta x}{2} +\frac{\Delta x\Delta v}{4}\text{.}\label{x:men:eq_prodruleinc}
\end{equation}
%
\par
Now decrement \(x\) and \(v\) by the same amounts:%
\begin{equation}
\left(x-\frac{\Delta x}{2}\right)\left(v-\frac{\Delta v}{2}\right) = xv - x\frac{\Delta v}{2} - v\frac{\Delta x}{2} + \frac{\Delta x\Delta v}{4}\text{.}\label{x:men:eq_prodruledec}
\end{equation}
%
\par
Subtracting the right side of \hyperref[x:men:eq_prodruledec]{equation~({\xreffont\ref{x:men:eq_prodruledec}})} from the right side of \hyperref[x:men:eq_prodruleinc]{equation~({\xreffont\ref{x:men:eq_prodruleinc}})} gives%
\begin{equation*}
\Delta R = x\Delta v + v\Delta x
\end{equation*}
which is the total change of \(R = xv\) over the intervals \(\Delta x\) and \(\Delta v\) and also recognizably the Product Rule.%
\begin{figureptx}{Isaac Newton}{x:figure:Newton}{}%
\index{Newton, Isaac!portrait of}%
\begin{image}{0.325}{0.35}{0.325}%
\includegraphics[width=\linewidth]{images/Newton.png}
\end{image}%
\tcblower
\end{figureptx}%
This argument is no better than Leibniz's as it relies heavily on the number \(1/2\) to make it work.  If we take any other increments in \(x\) and \(v\) whose total lengths are \(\Delta x\) and \(\Delta v\) it will simply not work.  Try it and see.%
\par
In Newton's defense, he wasn't really trying to justify his mathematical methods in the Principia.  His attention there was on physics, not math, so he was really just trying to give a convincing demonstration of his methods.  You may decide for yourself how convincing his demonstration is.%
\par
\index{Lagrange, Joseph-Louis} Notice that there is no mention of limits of difference quotients or derivatives.  In fact, the term derivative was not coined until 1797, by Lagrange.  In a sense, these topics were not necessary at the time, as Leibniz and Newton both assumed that the curves they dealt with had tangent lines and, in fact, Leibniz explicitly used the tangent line to relate two differential quantities.  This was consistent with the thinking of the time and for the duration of this chapter we will also assume that all quantities are differentiable.  As we will see later this assumption leads to difficulties.%
\par
Both Newton and Leibniz were satisfied that their calculus provided answers that agreed with what was known at the time.  For example \(\dx{ \left(x^2\right)}=\dx{\left(xx\right)}=x\dx{ x}+x\dx{ x}=2x\dx{ x}\) and \(\dx{\left(x^3\right)}=\dx{\left(x^2x\right)}=x^2\dx{ x}+x\dx{\left(x^2\right)}\) \(=x^2+x\left(2x\dx{ x}\right)=3x^2\dx{ x}\),\(\) results that were essentially derived by others in different ways.%
\begin{problem}{}{g:problem:idp35}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Use Leibniz's product rule \(\dx{ \left(xv\right)}=x\dx{ v}+v\dx{ x}\) to show that if \(n\) is a positive integer then \(\dx{ \left(x^n\right)}=nx^{n-1}\dx{ x}\)%
\item[(b)]Use Leibniz's product rule to derive the quotient rule%
\begin{equation*}
\dx{ \left(\frac{v}{y}\right)}=\frac{y\,\dx{ v}-v\,\dx{ y}}{yy}\text{.}
\end{equation*}
%
\item[(c)]Use the quotient rule to show that if \(n\)is a positive integer, then%
\begin{equation*}
\dx{ \left(x^{-n}\right)}=-nx^{-n-1}\dx{ x}.
\end{equation*}
%
\end{enumerate}
\end{problem}
\begin{problem}{}{g:problem:idp36}%
\index{differentiation!power rule with fractional exponents} Let \(p\) and \(q\) be integers with \(q\neq 0\). Show \(\dx{ \left(x^{\frac{p}{q}}\right)}=\frac{p}{q}x^{\frac{p}{q}-1}\dx{ x}\)%
\end{problem}
Leibniz also provided applications of his calculus to prove its worth. As an example he derived Snell's Law of Refraction from his calculus rules as follows.%
\par
Given that light travels through air at a speed of \(v_a\) and travels through water at a speed of \(v_w\) the problem is to find the fastest path from point \(A\) to point \(B\).%
\begin{figureptx}{}{x:figure:snellfig}{}%
\begin{image}{0.125}{0.75}{0.125}%
\includegraphics[width=\linewidth]{images/snellfig.png}
\end{image}%
\tcblower
\end{figureptx}%
According to Fermat's Principle of Least Time, this fastest path is the one that light will travel.%
\par
Using the fact that \(\text{ Time } =\text{ Distance }
/\text{ Velocity }\) and the labeling in the picture below we can obtain a formula for the time \(T\) it takes for light to travel from \(A\) to \(B\).%
\begin{figureptx}{}{x:figure:snellfig2}{}%
\begin{image}{0.125}{0.75}{0.125}%
\includegraphics[width=\linewidth]{images/snellfig2.png}
\end{image}%
\tcblower
\end{figureptx}%
%
\begin{equation*}
T=\frac{\sqrt{x^2+a^2}}{v_a}+\frac{\sqrt{(c-x)^2+b^2}}{v_w}
\end{equation*}
%
\par
Using the rules of Leibniz's calculus, we obtain%
\begin{align*}
\dx{ T}\amp = \left(\frac{1}{v_a}\frac{1}{2}\left(x^2+a^2\right)^{-\frac{1}{2}} (2x)+\frac{1}{v_w}\frac{1}{2}((c-x)^2+b^2)^{-\frac{1}{2}}(2(c-x)(-1))\right) \dx{ x}\\
\amp =\left(\frac{1}{v_a}\frac{x}{\sqrt{x^2+a^2}}-\frac{1}{v_w}\frac{c-x}{\sqrt{(c-x)^2+b^2}}\right)\dx{ x}\text{.}
\end{align*}
%
\par
Using the fact that at the minimum value for \(T\), \(\dx{ T}=0\), we have that the fastest path from \(A\)to \(B\) must satisfy \(\frac{1}{v_a}\frac{x}{\sqrt{x^2+a^2}}=\frac{1}{v_w}\frac{c-x}{\sqrt{(c-x)^2+b^2}}\). Inserting the following angles%
\begin{figureptx}{}{x:figure:snellfig3}{}%
\begin{image}{0.125}{0.75}{0.125}%
\includegraphics[width=\linewidth]{images/snellfig3.png}
\end{image}%
\tcblower
\end{figureptx}%
we get that the path that light travels must satisfy \(\frac{\sin\theta_a}{v_a}=\frac{\sin\theta_w}{v_w}\) which is Snell's Law.%
\par
\index{Bernoulli, Johann}\index{Brachistochrone problem, the} To compare 18th century and modern techniques we will consider Johann Bernoulli's solution of the Brachistochrone problem. In 1696, Bernoulli posed, and solved, the Brachistochrone problem; that is, to find the shape of a frictionless wire joining points A and B so that the time it takes for a bead to slide down under the force of gravity is as small as possible.%
\begin{figureptx}{}{x:figure:brachfig1}{}%
\begin{image}{0.125}{0.75}{0.125}%
\includegraphics[width=\linewidth]{images/brachfig1.png}
\end{image}%
\tcblower
\end{figureptx}%
Bernoulli posed this ``path of fastest descent'' problem to challenge the mathematicians of Europe and used his solution to demonstrate the power of Leibniz's calculus as well as his own ingenuity.  \index{Bernoulli, Johann!Bernoulli's challenge}%
\begin{quote}%
I, Johann Bernoulli, address the most brilliant mathematicians in the world. Nothing is more attractive to intelligent people than an honest, challenging problem, whose possible solution will bestow fame and remain as a lasting monument. Following the example set by Pascal, Fermat, etc., I hope to gain the gratitude of the whole scientific community by placing before the finest mathematicians of our time a problem which will test their methods and the strength of their intellect. If someone communicates to me the solution of the proposed problem, I shall publicly declare him worthy of praise. \hyperlink{x:biblio:Bernoulli_bio_mactutor}{[{\xreffont 11}]}%
\end{quote}
\begin{figureptx}{Johann Bernoulli}{g:figure:idp37}{}%
\index{Bernoulli, Johann!portrait of}%
\begin{image}{0.325}{0.35}{0.325}%
\includegraphics[width=\linewidth]{images/BernoulliJohann.png}
\end{image}%
\tcblower
\end{figureptx}%
In addition to Johann's, solutions were obtained from Newton, \index{Newton, Isaac} Leibniz, Johann's brother Jacob Bernoulli, \index{Bernoulli Jacob} and the Marquis de l'Hopital \hyperlink{x:biblio:struik69__sourc_book_mathem}{[{\xreffont 15}]}.  At the time there was an ongoing and very vitriolic controversy raging over whether Newton or Leibniz had been the first to invent calculus.  An advocate of the methods of Leibniz, Bernoulli did not believe Newton would be able to solve the problem using his methods. Bernoulli attempted to embarrass Newton by sending him the problem.  However Newton did solve it.%
\par
At this point in his life Newton had all but quit science and mathematics and was fully focused on his administrative duties as Master of the Mint.  In part due to rampant counterfeiting, England's money had become severely devalued and the nation was on the verge of economic collapse.  The solution was to recall all of the existing coins, melt them down, and strike new ones.  As Master of the Mint this job fell to Newton \hyperlink{x:biblio:levenson09__newton_count}{[{\xreffont 8}]}.  As you might imagine this was a rather Herculean task.  Nevertheless, according to his niece:%
\begin{quote}%
When the problem in 1696 was sent by Bernoulli\textendash{}Sir I.N. was in the midst of the hurry of the great recoinage and did not come home till four from the Tower very much tired, but did not sleep till he had solved it, which was by four in the morning.%
\end{quote}
He is later reported to have complained, ``I do not love . . . to be . . . teezed by forreigners about Mathematical things'' \hyperlink{x:biblio:dunham90__journ_throug_genius}{[{\xreffont 2}]}.%
\par
\index{Bernoulli, Johann!Tanquam ex ungue leonem} Newton submitted his solution anonymously, presumably to avoid more controversy.  Nevertheless the methods used were so distinctively Newton's that Bernoulli is said to have exclaimed ``\textit{Tanquam ex ungue leonem}.''\footnote{I know the lion by his claw.\label{g:fn:idp38}}%
\par
\index{Brachistochrone problem, the!Bernoulli's solution} Bernoulli's ingenious solution starts, interestingly enough, with Snell's Law of Refraction. He begins by considering the stratified medium in the following figure, where an object travels with velocities \(v_1,\,v_2,\,v_3,\,\ldots\) in the various layers.%
\begin{figureptx}{}{x:figure:brachfig2}{}%
\begin{image}{0.125}{0.75}{0.125}%
\includegraphics[width=\linewidth]{images/brachfig2.png}
\end{image}%
\tcblower
\end{figureptx}%
By repeatedly applying Snell's Law he concluded that the fastest path must satisfy%
\begin{equation*}
\frac{\sin \theta_1}{v_1}=\frac{\sin \theta_2}{v_2}=\frac{\sin\theta_3}{v_3}=\cdots\text{.}
\end{equation*}
%
\par
In other words, the ratio of the sine of the angle that the curve makes with the vertical and the speed remains constant along this fastest path.%
\par
If we think of a continuously changing medium as stratified into infinitesimal layers and extend Snell's law to an object whose speed is constantly changing,%
\begin{figureptx}{}{x:figure:snellfig4}{}%
\begin{image}{0.125}{0.75}{0.125}%
\includegraphics[width=\linewidth]{images/snellfig4.png}
\end{image}%
\tcblower
\end{figureptx}%
then along the fastest path, the ratio of the sine of the angle that the curve's tangent makes with the vertical, \(\alpha\), and the speed, \(v\), must remain constant.%
\begin{equation*}
\frac{\text{ sin } \alpha}{v}=c\text{.}
\end{equation*}
%
\par
If we include axes and let \(P\) denote the position of the bead at a particular time then we have the following picture.%
\begin{figureptx}{}{x:figure:snellfig5}{}%
\begin{image}{0.125}{0.75}{0.125}%
\includegraphics[width=\linewidth]{images/snellfig5.png}
\end{image}%
\tcblower
\end{figureptx}%
In the above figure, \(s\) denotes the length that the bead has traveled down to point \(P\)(that is, the arc length of the curve from the origin to that point) and \(a\) denotes the tangential component of the acceleration due to gravity \(g\).  Since the bead travels only under the influence of gravity then \(\dfdx{v}{t}=a\).%
\par
To get a sense of how physical problems were approached using Leibniz's calculus we will use the above equation to show that \(v=\sqrt{2gy}\).%
\par
By similar triangles we have \(\frac{a}{g}=\frac{\dx{ y}}{\dx{ s}}\). As a student of Leibniz, Bernoulli would have regarded \(\frac{\dx{ y}}{\dx{ s}}\) as a fraction so%
\begin{align*}
a\dx{ s} \amp = g\dx{ y}\\
\intertext{and since acceleration is the rate of change of velocity we have}
\frac{\dx{ v}}{\dx{ t}}\dx{ s} \amp = g\dx{ y}.\\
\intertext{Again, 18th century European mathematicians regarded \(\dx{ v}, \dx{ t}\), and \(\dx{ s}\) as infinitesimally small numbers which nevertheless obey all of the usual rules of algebra. Thus we can rearrange the above to get}
\frac{\dx{ s}}{\dx{ t}}\dx{ v} \amp = g\dx{ y}.\\
\intertext{Since \(\frac{\dx{ s}}{\dx{ t}}\) is the rate of change of position with respect to time it is, in fact, the velocity of the bead. That is}
v\dx{ v} \amp = g\dx{ y}.\\
\intertext{Bernoulli would have interpreted this as a statement that two rectangles of height \(v\) and \(g\), with respective widths \(\dx{ v}\) and \(\dx{ y}\) have equal area. Summing (integrating) all such rectangles we get:}
\int{}v\dx{ v} \amp = \int{}g\dx{ y}\\
\frac{v^2}{2} \amp = gy
\end{align*}
or%
\begin{equation}
v=\sqrt{2gy}\text{.}\label{x:men:eq_brach_vel}
\end{equation}
%
\par
You are undoubtedly uncomfortable with the cavalier manipulation of infinitesimal quantities you've just witnessed, so we'll pause for a moment now to compare a modern development of \hyperref[x:men:eq_brach_vel]{equation~({\xreffont\ref{x:men:eq_brach_vel}})} to Bernoulli's. As before we begin with the equation:%
\begin{align*}
\frac{a}{g}\amp = \dfdx{y}{s}\\
a \amp = g\dfdx{y}{s}.\\
\intertext{Moreover, since acceleration is the derivative of velocity this is the same as:}
\dfdx{v}{t} \amp = g\dfdx{y}{s}.\\
\intertext{Now observe that by the Chain Rule \(\dfdx{v}{t} = \dfdx{v}{s}\dfdx{s}{t}\). The physical interpretation of this formula is that velocity will depend on \(s\), how far down the wire the bead has moved, but that the distance traveled will depend on how much time has elapsed. Therefore}
\dfdx{v}{s}\dfdx{s}{t} \amp = g\dfdx{y}{s}\\
\intertext{or}
\dfdx{s}{t}\dfdx{v}{s} \amp = g\dfdx{y}{s}\\
\intertext{and since \(\dfdx{s}{t} = v\) we have}
v\dfdx{v}{s} \amp = g\dfdx{y}{s}\\
\intertext{Integrating both sides with respect to \(s\) gives:}
\int{}v\dfdx{v}{s}d s \amp = g\int{}\dfdx{y}{s}d s\\
\int{}vd v \amp = g\int{}d y\\
\intertext{and integrating gives}
\frac{v^2}{2} \amp = gy
\end{align*}
as before.%
\par
In effect, in the modern formulation we have traded the simplicity and elegance of differentials for a comparatively cumbersome repeated use of the Chain Rule. No doubt you noticed when taking Calculus that in the differential notation of Leibniz,\index{Leibniz, Gottfried Wilhelm} the Chain Rule looks like ``canceling'' an expression in the top and bottom of a fraction: \(\dfdx{y}{u}\dfdx{u}{x} = \dfdx{y}{x}\). This is because for 18th century mathematicians, this is exactly what it was.%
\par
To put it another way, 18th century mathematicians wouldn't have recognized a need for what we call the Chain Rule because this operation was a triviality for them. Just reduce the fraction. This begs the question: Why did we abandon such a clear, simple interpretation of our symbols in favor of the, comparatively, more cumbersome modern interpretation? This is one of the questions we will try to answer in this course.%
\par
Returning to the Brachistochrone problem we observe that%
\begin{align}
\frac{\sin\alpha}{v} \amp = c\notag\\
\intertext{and since \(\sin\alpha = \frac{d x}{d s}\)   we see that}
\frac{\frac{d x}{d s}}{\sqrt{2gy}} \amp = c\notag\\
\frac{d x}{\sqrt{2gy(ds)^2}} \amp = c\notag\\
\frac{d x}{\sqrt{2gy\left[(d x)^2+(d y)^2\right]}} \amp = c\text{.}\label{x:mrow:eq_Brachistochrone}
\end{align}
%
\par
Bernoulli was then able to solve this differential equation.%
\begin{problem}{}{g:problem:idp39}%
\index{Brachistochrone problem, the} Show that the equations \(x=\frac{t-\sin t}{4gc^2},\,y=\frac{1-\cos t}{4gc^2}\) satisfy equation \hyperref[x:mrow:eq_Brachistochrone]{({\xreffont\ref{x:mrow:eq_Brachistochrone}})}. Bernoulli recognized this solution to be an inverted cycloid, the curve traced by a fixed point on a circle as the circle rolls along a horizontal surface.%
\end{problem}
This illustrates the state of calculus in the late 1600's and early 1700's; the foundations of the subject were a bit shaky but there was no denying its power.%
\end{subsectionptx}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 4.2 Power Series as Infinite Polynomials}
\typeout{************************************************}
%
\begin{sectionptx}{Power Series as Infinite Polynomials}{}{Power Series as Infinite Polynomials}{}{}{x:section:ExponentAdditionProperty}
\index{polynomials!infinite} Applied to polynomials, the rules of differential and integral calculus are straightforward.  Indeed, differentiating and integrating polynomials represent some of the easiest tasks in a calculus course.  For example, computing \(\int(7-x+x^2)\dx{ x}\) is relatively easy compared to computing \(\int\sqrt[3]{1+x^3}\d
x\).  Unfortunately, not all functions can be expressed as a polynomial.  For example, \(f(x)=\sin x\) cannot be since a polynomial has only finitely many roots and the sine function has infinitely many roots, namely \(\{n\pi|\,n\in\ZZ\}\).  A standard technique in the 18th century was to write such functions as an ``infinite polynomial,'' what we typically refer to as a power series.  Unfortunately an ``infinite polynomial'' is a much more subtle object than a mere polynomial, which by definition is finite. For now we will not concern ourselves with these subtleties.  We will follow the example of our forebears and manipulate all ``polynomial-like'' objects (finite or infinite) as if they are polynomials.%
\begin{definition}{}{x:definition:def_PowerSeries}%
\index{power series!definition of} A \terminology{power series centered at \(\boldsymbol{a}\)} is a series of the form%
\begin{equation*}
\sum_{n=0}^\infty a_n(x-a)^n=a_0+a_1(x-a)+a_2(x-a)^2+\cdots\text{.}
\end{equation*}
%
\par
Often we will focus on the behavior of power series \(\sum_{n=0}^\infty a_nx^n\), centered around \(0\), as the series centered around other values of \(a\) are obtained by shifting a series centered at \(0\).%
\end{definition}
Before we continue, we will make the following notational comment. The most advantageous way to represent a series is using summation notation since there can be no doubt about the pattern to the terms. After all, this notation contains a formula for the general term. This being said, there are instances where writing this formula is not practical. In these cases, it is acceptable to write the sum by supplying the first few terms and using ellipses (the three dots). If this is done, then enough terms must be included to make the pattern clear to the reader.%
\par
Returning to our definition of a power series, consider, for example, the \index{series!Geometric series!naive derivation} geometric series \(\sum_{n=0}^\infty x^n=1+x+x^2+\cdots\). If we multiply this series by \((1-x)\), we obtain%
\begin{equation*}
(1-x)(1+x+x^2+\cdots)=(1+x+x^2+\cdots)-(x+x^2+x^3+\cdots)=1\text{.}
\end{equation*}
%
\par
This leads us to the power series representation%
\begin{equation*}
\frac{1}{1-x}=1+x+x^2+\cdots=\sum_{n=0}^\infty x^n\text{.}
\end{equation*}
%
\par
If we substitute \(x=\frac{1}{10}\) into the above, we obtain%
\begin{equation*}
1+\frac{1}{10}+\left(\frac{1}{10}\right)^2+\left(\frac{1}{10}\right)^3+ \cdots=\frac{1}{1-\frac{1}{10}}=\frac{10}{9}\text{.}
\end{equation*}
%
\par
This agrees with the fact that \(.333\ldots=\frac{1}{3}\), and so \(.111\ldots=\frac{1}{9}\), and \(1.111\ldots=\frac{10}{9}\).%
\par
There are limitations to these formal manipulations however. Substituting \(x=1\) or \(x=2\) yields the questionable results%
\begin{equation*}
\frac{1}{0}=1+1+1+\cdots\,\text{  and  }  \,\frac{1}{-1}=1+2+2^2+\cdots\text{.}
\end{equation*}
%
\par
We are missing something important here, though it may not be clear exactly what. A series representation of a function works \emph{sometimes,} but there are some problems. For now, we will continue to follow the example of our 18th century predecessors and ignore them. That is, for the rest of this section we will focus on the formal manipulations to obtain and use power series representations of various functions. Keep in mind that this is all highly suspect until we can resolve problems like those just given.%
\par
Power series became an important tool in analysis in the 1700's.  By representing various functions as power series they could be dealt with as if they were (infinite) polynomials.  The following is an example.%
\begin{example}{}{g:example:idp40}%
Solve the following Initial Value problem \footnotemark{} : Find \(y(x)\) given that \(\frac{\dx{ y}}{\dx{ x}}=y,\,y(0)=1\).%
\par
Assuming the solution can be expressed as a power series we have%
\begin{equation*}
y=\sum_{n=0}^\infty a_nx^n=a_0+a_1x+a_2x^2+\cdots\text{.}
\end{equation*}
%
\par
Differentiating gives us%
\begin{equation*}
\frac{\dx{ y}}{\dx{ x}}=a_1+2a_2x+3a_3x^2+4a_4x^3+\ldots\text{.}
\end{equation*}
%
\par
Since \(\frac{\dx{ y}}{\dx{ x}}=y\) we see that%
\begin{equation*}
a_1=a_0\,,\,2a_2=a_1\,,\,3a_3=a_2\,,\,\ldots,\,na_n=a_{n-1}\,,\ldots\text{.}
\end{equation*}
%
\par
This leads to the relationship%
\begin{equation*}
a_n=\frac{1}{n}a_{n-1}=\frac{1}{n(n-1)}a_{n-2}=\cdots=\frac{1}{n!}a_0\text{.}
\end{equation*}
%
\par
Thus the series solution of the differential equation is%
\begin{equation*}
y=\sum_{n=0}^\infty\frac{a_0}{n!}x^n=a_0\sum_{n=0}^\infty\frac{1}{n!}x^n\text{.}
\end{equation*}
%
\par
Using the initial condition \(y(0)=1\), we get \(1=a_0(1+0+\frac{1}{2!}0^2+\cdots)=a_0\). Thus the solution to the initial problem is \(y=\sum_{n=0}^\infty\frac{1}{n!}x^n\). Let's call this function \(E(x)\). Then by definition%
\begin{equation*}
E(x)=\sum_{n=0}^\infty\frac{1}{n!}x^n=1+\frac{x^1}{1!}+\frac{x^2}{2!}+\frac{x^3}{3!}+\,\ldots\text{.}
\end{equation*}
%
\end{example}
\footnotetext[1]{A few seconds of thought should convince you that the solution of this problem is \(y(x) = e^x\).  We will ignore this for now in favor of emphasising the technique.\label{g:fn:idp41}}%
Let's examine some properties of this function. The first property is clear from the definition.%
\par
\index{\(e^x\)!\(E(0)=1\)} \terminology{Property 1}. \(E(0)=1\)%
\par
\index{\(e^x\)!\(E(x+y)=E(x)E(y)\)} \terminology{Property 2}. \(E(x+y)=E(x)E(y)\).%
\par
To see this we multiply the two series together, so we have%
\begin{align*}
E(x)E(y) \amp =\left(\sum_{n=0}^\infty\frac{1}{n!}x^n\right)\left(\sum_{n=0}^\infty\frac{1}{n!}y^n\right)\\
\amp =\left(\frac{x^0}{0!}+\frac{x^1}{1!}+\frac{x^2}{2!}+\frac{x^3}{3!}+\,\ldots\right)\left(\frac{y^0}{0!}+\frac{y^1}{1!}+\frac{y^2}{2!}+\frac{y^3}{3!}+\,\ldots\right)\\
\amp =\frac{x^0}{0!}\frac{y^0}{0!}+\frac{x^0}{0!}\frac{y^1}{1!}+\frac{x^1}{1!}\frac{y^0}{0!}+\frac{x^0}{0!}\frac{y^2}{2!}+\frac{x^1}{1!}\frac{y^1}{1!}+\frac{x^2}{2!}\frac{y^0}{0!}\\
\amp \ \ \ \ \ \  +\frac{x^0}{0!}\frac{y^3}{3!}+\frac{x^1}{1!}\frac{y^2}{2!}+\frac{x^2}{2!}\frac{y^1}{1!}+\frac{x^3}{3!}\frac{y^0}{0!}+\,\ldots\\
\amp =\frac{x^0}{0!}\frac{y^0}{0!}+\left(\frac{x^0}{0!}\frac{y^1}{1!}+ \frac{x^1}{1!}\frac{y^0}{0!}\right)\\
\amp \ \ \ \ \ \  +\left(\frac{x^0}{0!}\frac{y^2}{2!}+\frac{x^1}{1!}\frac{y^1}{1!}+\frac{x^2}{2!}\frac{y^0}{0!}\right)\\
\amp \ \ \ \ \ \ +\left(\frac{x^0}{0!}\frac{y^3}{3!}+\frac{x^1}{1!}\frac{y^2}{2!}+\frac{x^2}{2!}\frac{y^1}{1!}+\frac{x^3}{3!}\frac{y^0}{0!}\right)+\,\ldots\\
\amp =\frac{1}{0!}+\frac{1}{1!}\left(\frac{1!}{0!1!}x^0y^1+\frac{1!}{1!0!}x^1y^0\right)\\
\amp \ \ \ \ \ \ +\frac{1}{2!}\left(\frac{2!}{0!2!}x^0y^2+\frac{2!}{1!1!}x^1y^1+\frac{2!}{2!0!}x^2y^0\right)\\
\amp \ \ \ \ \ \ +\frac{1}{3!}\left(\frac{3!}{0!3!}x^0y^3+\frac{3!}{1!2!}x^1y^2+\frac{3!}{2!1!}x^2y^1+\frac{3!}{3!0!}x^3y^0\right)+\ldots
\end{align*}
%
\begin{align}
E(x)E(y) \amp =\frac{1}{0!}+\frac{1}{1!}\left(\binom{1}{0}x^0y^1+\binom{1}{1}x^1y^0\right)\notag\\
\amp \ \ \ \ \ \ +\frac{1}{2!}\left(\binom{2}{0}x^0y^2+\binom{2}{1}x^1y^1+\binom{2}{2}x^2y^0\right)\notag\\
\amp \ \ \ \ \ \ +\frac{1}{3!}\left(\binom{3}{0}x^0y^3+\binom{3}{1}x^1y^2+\binom{3}{2}x^2y^1+\binom{3}{3}x^3y^0\right)+\ldots\notag\\
\amp =\frac{1}{0!}+\frac{1}{1!}\left(x+y\right)^1+\frac{1}{2!}\left(x+y\right)^2+\frac{1}{3!}\left(x+y\right)^3+\ldots\notag\\
\amp =E(x+y)\text{.}\label{x:mrow:eq_ExponentAdditionProperty}
\end{align}
%
\par
\index{\(e^x\)!\(E(m)=\left(E(1)\right)^mE(0)=1\)} \terminology{Property 3}. If \(m\) is a positive integer then \(E(mx)=\left(E(x\right))^m\). In particular, \(E(m)=\left(E(1)\right)^m\).%
\begin{problem}{}{g:problem:idp42}%
\index{\(e^x\)!\(E(mx)=\left(E(x\right))^m\)} Prove Property 3.%
\end{problem}
\terminology{Property 4}. \(E(-x)=\frac{1}{E(x)}=\left(E(x)\right)^{-1}\).%
\begin{problem}{}{g:problem:idp43}%
\index{\(e^x\)!\(E(-x)=\left(E(x)\right)^{-1}\)} Prove Property 4.%
\end{problem}
\terminology{Property 5}. If \(n\) is an integer with \(n\neq 0\), then \(E(\frac{1}{n})=\sqrt[n]{E(1)}=\left(E(1)\right)^{1/n}\).%
\begin{problem}{}{g:problem:idp44}%
\index{\(e^x\)!\(E(\frac{1}{n})=\left(E(1)\right)^{1/n}\)} Prove Property 5.%
\end{problem}
\terminology{Property 6}. If \(m\) and \(n\) are integers with \(n\neq 0\), then \(E\left(\frac{m}{n}\right)=\left(E(1)\right)^{m/n}\).%
\begin{problem}{}{g:problem:idp45}%
\index{\(e^x\)!\(E\left(\frac{m}{n}\right)=\left(E(1)\right)^{m/n}\)} Prove Property 6.%
\end{problem}
\begin{definition}{}{x:definition:def_e}%
\index{\(e^x\)!definition of \(e\)} Let \(E(1)\) be denoted by the number \(e\). Using the series \(e=E(1)=\sum_{n=0}^\infty\frac{1}{n!}\), we can approximate \(e\) to any degree of accuracy. In particular \(e\approx 2.71828\).%
\end{definition}
In light of Property~6, we see that for any rational number \(r\), \(E(r)=e^r\). Not only does this give us the series representation \(e^r=\sum_{n=0}^\infty\frac{1}{n!}r^n\) for any rational number \(r\), but it gives us a way to define \(e^x\) for irrational values of \(x\) as well. That is, we can define%
\begin{equation*}
e^x=E(x)=\sum_{n=0}^\infty\frac{1}{n!}x^n
\end{equation*}
for any real number \(x\).%
\par
As an illustration, we now have \(e^{\sqrt{2}}=\sum_{n=0}^\infty\frac{1}{n!}\left(\sqrt{2}\right)^n\). The expression \(e^{\sqrt{2}}\) is meaningless if we try to interpret it as one irrational number raised to another. What does it mean to raise anything to the \(\sqrt{2}\) power? However the series \(\sum_{n=0}^\infty\frac{1}{n!}\left(\sqrt{2}\right)^n\) does seem to have meaning and it can be used to extend the exponential function to irrational exponents. In fact, defining the exponential function via this series answers the question we raised in  \hyperref[x:chapter:NumbersRealRational]{Chapter~{\xreffont\ref{x:chapter:NumbersRealRational}}}: What does \(4^{\sqrt{2}}\) mean?%
\par
It means \(\displaystyle 4^{\sqrt{2}} = e^{\sqrt{2}\log 4} = \sum_{n=0}^\infty\frac{(\sqrt{2}\log 4)^n}{n!}\).%
\par
This may seem to be the long way around just to define something as simple as exponentiation. But this is a fundamentally misguided attitude. Exponentiation only \emph{seems} simple because we've always thought of it as repeated multiplication (in \(\ZZ\)) or root-taking (in \(\QQ\)). When we expand the operation to the real numbers this simply can't be the way we interpret something like \(4^{\sqrt{2}}\). How do you take the product of \(\sqrt{2}\) copies of \(4?\) The concept is meaningless. What we need is an interpretation of \(4^{\sqrt{2}}\) which is consistent with, say \(4^{3/2} = \left(\sqrt{4}\right)^3=8\). This is exactly what the series representation of \(e^x\) provides.%
\par
We also have a means of computing integrals as series. For example, the famous ``bell shaped'' curve given by the function \(f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\) is of vital importance in statistics and must be integrated to calculate probabilities. The power series we developed gives us a method of integrating this function. For example, we have%
\begin{align*}
\int_{x=0}^b\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}d x \amp  =\frac{1}{\sqrt{2\pi}}\int_{x=0}^b\left(\sum_{n=0}^\infty\frac{1}{n!}\left(\frac{-x^2}{2}\right)^n\right)d x\\
\amp =\frac{1}{\sqrt{2\pi}}\,\sum_{n=0}^\infty\left(\frac{\left(-1\right)^n}{n!2^n}\int_{x=0}^bx^{2n}d x\right)\\
\amp =\frac{1}{\sqrt{2\pi}}\,\sum_{n=0}^\infty\left(\frac{\left(-1\right)^nb^{2n+1}}{n!2^n\left(2n+1\right)}\right)\text{.}
\end{align*}
%
\par
This series can be used to approximate the integral to any degree of accuracy. The ability to provide such calculations made power series of paramount importance in the 1700's.%
\begin{problem}{}{g:problem:idp46}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Show that if \(y=\sum_{n=0}^\infty a_nx^n\) satisfies the differential equation \(\frac{\dx^2y}{\dx{ x}^2}=-y\), then%
\begin{equation*}
a_{n+2}=\frac{-1}{\left(n+2\right)\left(n+1\right)}\,a_n
\end{equation*}
and conclude that%
\begin{equation*}
y=a_0+a_1x-\frac{1}{2!}\,a_0x^2-\frac{1}{3!}\,a_1x^3+\frac{1}{4!}\,a_0x^4+\frac{1}{5!}\,a_1x^5-\frac{1}{6!}\,a_0x^6-\frac{1}{7!}\,a_1x^7+\cdots\text{.}
\end{equation*}
%
\item[(b)]Since \(y=\sin x\) satisfies \(\frac{\dx^2y}{\dx{
x}^2}=-y\), we see that%
\begin{equation*}
\sin x=a_0+a_1x-\frac{1}{2!}\,a_0x^2-\frac{1}{3!}\,a_1x^3+\frac{1}{4!}\,a_0x^4+\frac{1}{5!}\,a_1x^5-\frac{1}{6!}\,a_0x^6-\frac{1}{7!}\,a_1x^7+\cdots
\end{equation*}
for some constants \(a_0\) and \(a_1\).  Show that in this case \(a_0=0\) and \(a_1=1\) and obtain%
\begin{equation*}
\sin x=x-\frac{1}{3!}\,x^3+\frac{1}{5!}x^5-\frac{1}{7!}x^7+\cdots=\sum_{n=0}^\infty\frac{\left(-1\right)^n}{\left(2n+1\right)!}x^{2n+1}\text{.}
\end{equation*}
%
\end{enumerate}
\end{problem}
\begin{problem}{}{g:problem:idp47}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Use the series%
\begin{equation*}
\sin x=x-\frac{1}{3!}\,x^3+\frac{1}{5!}x^5-\frac{1}{7!}x^7+\cdots=\sum_{n=0}^\infty\frac{\left(-1\right)^n}{\left(2n+1\right)!}x^{2n+1}
\end{equation*}
to obtain the series%
\begin{equation*}
\cos x=1-\frac{1}{2!}\,x^2+\frac{1}{4!}x^4-\frac{1}{6!}x^6+\cdots=\sum_{n=0}^\infty\frac{\left(-1\right)^n}{\left(2n\right)!}x^{2n}\text{.}
\end{equation*}
%
\item[(b)]Let \(s(x,N)=\sum_{n=0}^N\frac{\left(-1\right)^n}{\left(2n+1\right)!}x^{2n+1}\) and \(c(x,N)=\sum_{n=0}^N\frac{\left(-1\right)^n}{\left(2n\right)!}x^{2n}\) and use a computer algebra system to plot these for\(\,-4\pi\leq x\leq 4\pi,\,\,N=1,2,5,\,10,\,15\). Describe what is happening to the series as N becomes larger.%
\end{enumerate}
\end{problem}
\begin{problem}{}{x:problem:prob_alternating_harmonic_series}%
Use the geometric series, \(\frac{1}{1-x}=1+x+x^2+x^3+\cdots=\sum_{n=0}^\infty x^n\), to obtain a series for \(\frac{1}{1+x^2}\) and use this to obtain the series%
\begin{equation*}
\arctan x=x-\frac{1}{3}x^3+\frac{1}{5}x^5-\cdots=\sum_{n=0}^\infty(-1)^n \frac{1}{2n+1}x^{2n+1}\text{.}
\end{equation*}
%
\par
Use the series above to obtain the series \(\frac{\pi}{4}=\sum_{n=0}^\infty(-1)^n\frac{1}{2n+1}\).%
\end{problem}
The series for arctangent was known by James Gregory (1638-1675) and it is sometimes referred to as ``Gregory's series.'' Leibniz\index{Leibniz, Gottfried Wilhelm} independently discovered \(\frac{\pi}{4}=1-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}+\cdots\) by examining the area of a circle.  Though it gives us a means for approximating \(\pi\) to any desired accuracy, the series converges too slowly to be of any practical use.  For example, if we compute the sum of the first \(1000\) terms we get%
\begin{equation*}
4\left(\sum_{n=0}^{1000}(-1)^n\frac{1}{2n+1}\right)\approx 3.142591654
\end{equation*}
which only approximates \(\pi\) to two decimal places.%
\par
Newton \index{Newton, Isaac} knew of these results and the general scheme of using series to compute areas under curves. These results motivated Newton to provide a series approximation for \(\pi\) as well, which, hopefully, would converge faster. We will use modern terminology to streamline Newton's ideas. First notice that \(\frac{\pi}{4}=\int_{x=0}^1\sqrt{1-x^2}\dx{ x}\) as this integral gives the area of one quarter of the unit circle. The trick now is to find series that represents \(\sqrt{1-x^2}\).%
\par
To this end we start with the binomial theorem%
\begin{equation*}
\left(a+b\right)^N=\sum_{n=0}^N\binom{N}{n}a^{N-n}b^n\text{,}
\end{equation*}
where%
\begin{align*}
\binom{N}{n}\amp =\frac{N!}{n!\left(N-n\right)!}\\
\amp =\frac{N\left(N-1\right)\left(N-2\right)\cdots\left(N-n+1\,\right)}{n!}\\
\amp =\frac{\prod_{j=0}^{n-1}\left(N-j\right)}{n!}\text{.}
\end{align*}
%
\par
Unfortunately, we now have a small problem with our notation which will be a source of confusion later if we don't fix it. So we will pause to address this matter. We will come back to the binomial expansion afterward.%
\par
This last expression is becoming awkward in much the same way that an expression like%
\begin{equation*}
1+\frac{1}{2}+\left(\frac{1}{2}\right)^2+\left(\frac{1}{2}\right)^3+\ldots+\left(\frac{1}{2}\right)^k
\end{equation*}
is awkward. Just as this sum becomes is less cumbersome when written as \(\sum_{n=0}^k\left(\frac{1}{2}\right)^n\) the \emph{product}%
\begin{equation*}
N\left(N-1\right)\left(N-2\right)\cdots\left(N-n+1\,\right)
\end{equation*}
is less awkward when we write it as \(\prod_{j=0}^{n-1}\left(N-j\right)\).%
\par
A capital pi (\(\Pi\)) is used to denote a product in the same way that a capital sigma (\(\Sigma\)) is used to denote a sum. The most familiar example would be writing%
\begin{equation*}
n!=\prod_{j=1}^{n}j\text{.}
\end{equation*}
%
\par
Just as it is convenient to define \(0!=1\), we will find it convenient to define \(\prod_{j=1}^{0}=1\). Similarly, the fact that \(\binom{N}{0}=1\) leads to convention \(\prod_{j=0}^{-1}\left(N-j\right)=1\). Strange as this may look, it is convenient and \emph{is} consistent with the convention \(\sum_{j=0}^{-1}s_j=0\).%
\par
Returning to the binomial expansion and recalling our convention%
\begin{equation*}
\prod_{j=0}^{-1}\left(N-j\right)=1\text{,}
\end{equation*}
we can write,%
\begin{equation*}
\left(1+x\right)^N=1+\sum_{n=1}^N\left(\frac{\prod_{j=0}^{n-1}\left(N-j\right)}{n!}\right)x^n = \sum_{n=0}^N\left(\frac{\prod_{j=0}^{n-1}\left(N-j\right)}{n!}\right)x^n\text{.}
\end{equation*}
%
\par
These two representations probably look the same at first. Take a moment and be sure you see where they differ.%
\par
There is an advantage to using this convention (especially when programing a product into a computer), but this is not a deep mathematical insight. It is just a notational convenience and we don't want you to fret over it, so we will use both formulations (at least initially).%
\par
Notice that we can extend the above definition of \(\binom{N}{n}\) to values \(n>N\). In this case, \(\prod_{j=0}^{n-1}\left(N-j\right)\) will equal 0 as one of the factors in the product will be \(0\) (the one where \(n=N\)). This gives us that \(\binom{N}{n}=0\) when \(n>N\) and so%
\begin{equation*}
\left(1+x\right)^N=1+\sum_{n=1}^\infty\left(\frac{\prod_{j=0}^{n-1}\left(N-j\right)}{n!}\text{ } \right)x^n= \sum_{n=0}^\infty\left(\frac{\prod_{j=0}^{n-1}\left(N-j\right)}{n!}\text{ } \right)x^n
\end{equation*}
holds true for any nonnegative integer \(N\). Essentially Newton asked if it could be possible that the above equation could hold values of \(N\) which are not nonnegative integers. For example, if the equation held true for \(N=\frac{1}{2}\) , we would obtain%
\begin{equation*}
\left(1+x\right)^{\frac{1}{2}}=1+\sum_{n=1}^\infty\left(\frac{ \prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}\right)x^n=\sum_{n=0}^\infty\left(\frac{ \prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}\right)x^n
\end{equation*}
or%
\begin{equation}
\left(1+x\right)^{\frac{1}{2}}=1+\frac{1}{2}x+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)}{2!}x^2+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)\left(\frac{1}{2}-2\right)}{3!}x^3+\cdots\text{.}\label{x:men:eq_BinomialSeries}
\end{equation}
%
\par
Notice that since \(1/2\) is not an integer the series no longer terminates. Although Newton did not prove that this series was correct (nor did we), he tested it by multiplying the series by itself. When he saw that by squaring the series he started to obtain \(1+x+0\,x^2+0\,x^3+\cdots\), he was convinced that the series was exactly equal to \(\sqrt{1+x}\).%
\begin{problem}{}{g:problem:idp48}%
\index{Binomial Series, the!squaring the}%
\par
Consider the series representation%
\begin{align*}
\left(1+x\right)^{\frac{1}{2}}\amp =1+\sum_{n=1}^\infty\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}x^n\\
\amp  =\sum_{n=0}^\infty\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}x^n\text{.}
\end{align*}
%
\par
Multiply this series by itself and compute the coefficients for \(x^0,\,x^1,\,x^2,\,x^3,\,x^4\) in the resulting series.%
\end{problem}
\begin{problem}{}{x:problem:prob_SqrtSeriesProb}%
\index{series!graph the square root series} Let%
\begin{equation*}
S(x,M)=\sum_{n=0}^M\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j \right)}{n!}x^n\text{.}
\end{equation*}
%
\par
Use a computer algebra system to plot \(S(x,M)\) for \(M=5,\,10,\,15,\,95,\,100\) and compare these to the graph for \(\sqrt{1+x}\). What seems to be happening? For what values of \(x\) does the series appear to converge to \(\sqrt{1+x}?\)%
\end{problem}
Convinced that he had the correct series, Newton used it to find a series representation of \(\int_{x=0}^1\sqrt{1-x^2} \dx{ x}\).%
\begin{problem}{}{g:problem:idp49}%
\index{\(\pi\)!first series expansion} Use the series \(\displaystyle \left(1+x\right)^{\frac{1}{2}}=\sum_{n=0}^\infty\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}x^n\) to obtain the series%
\begin{align*}
\frac{\pi}{4}\amp =\int_{x=0}^1\sqrt{1-x^2} \dx{ x}\\
\amp =\sum_{n=0}^\infty\left(\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}\text{ } \right)\left(\frac{\left(-1\right)^n}{2n+1}\right)\\
\amp =1-\frac{1}{6}-\frac{1}{40}-\frac{1}{112}-\frac{5}{1152}-\cdots\text{.}
\end{align*}
%
\par
Use a computer algebra system to sum the first 100 terms of this series and compare the answer to \(\frac{\pi}{4}\).%
\end{problem}
Again, Newton had a series which could be verified (somewhat) computationally. This convinced him even further that he had the correct series.%
\begin{problem}{}{g:problem:idp50}%
\index{\(\pi\)!second series expansion}%
\begin{enumerate}[label=(\alph*)]
\item{}Show that%
\begin{equation*}
\int_{x=0}^{1/2}\sqrt{x-x^2}\dx{ x}=\sum_{n=0}^\infty\frac{(-1)^n\,\,\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{\sqrt{2\,}n!\left(2n+3\right)2^n}
\end{equation*}
and use this to show that%
\begin{equation*}
\pi=16\left(\sum_{n=0}^\infty\frac{(-1)^n\,\,\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{\sqrt{2\,}n!\left(2n+3\right)2^n}\right)\text{.}
\end{equation*}
%
\item{}We now have two series for calculating \(\pi:\)  the one from part (a) and the one derived earlier, namely%
\begin{equation*}
\pi=4\left(\sum_{n=0}^\infty\frac{(-1)^n\,\,}{2n+1}\right)\text{.}
\end{equation*}
We will explore which one converges to \(\pi\) faster. With this in mind, define \(S1(N)=16\left(\sum_{n=0}^N\frac{(-1)^n\,\,\prod_{j=0}^{n-1}\left( \frac{1}{2}-j\right)}{\sqrt{2\,}n!\left(2n+3\right)2^n}\right)\) and \(S2(N)=4\left(\sum_{n=0}^N\frac{(-1)^n\,\,}{2n+1}\right)\). Use a computer algebra system to compute \(S1(N)\)and \(S2(N)\) for \(N=5,10,15,20\). Which one appears to converge to \(\pi\) faster?%
\end{enumerate}
%
\end{problem}
In general the series representation%
\begin{align*}
\left(1+x\right)^\alpha \amp  =\sum_{n=0}^\infty\left(\frac{\prod_{j=0}^{n-1}\left(\alpha-j\right)}{n!}\text{ } \right)x^n\\
\amp =1+\alpha x+\frac{\alpha\left(\alpha-1\right)}{2!}x^2+\frac{\alpha\left(\alpha-1\right)\left(\alpha-2\right)}{3!}x^3+\cdots
\end{align*}
is called the \terminology{binomial series} (or Newton's binomial series). This series is correct when \(\alpha\) is a non-negative integer (after all, that is how we got the series). We can also see that it is correct when \(\alpha=-1\) as we obtain%
\begin{align*}
\left(1+x\right)^{-1}\amp =\sum_{n=0}^\infty\left(\frac{\prod_{j=0}^{n-1}\left(-1-j\right)}{n!}\text{ } \right)x^n\\
\amp =1+(-1)x+\frac{-1\left(-1-1\right)}{2!}x^2+\frac{-1\left(-1-1\right)\left(-1-2\right)}{3!}x^3+\cdots\\
\amp =1-x+x^2-x^3+\cdots
\end{align*}
which can be obtained from the geometric series \(\frac{1}{1-x}=1+x+x^2+\cdots\) .%
\par
In fact, the binomial series is the correct series representation for all values of the exponent \(\alpha\) (though we haven't proved this yet).%
\begin{problem}{}{g:problem:idp51}%
Let \(k\) be a positive integer. Find the power series, centered at zero, for \(f(x) = \left(1-x\right)^{-k}\) by%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Differentiating the \index{series!Geometric series!differentiating} geometric series \(\left(k-1\right)\) times.%
\item[(b)]Applying the binomial series.%
\item[(c)]Compare these two results.%
\end{enumerate}
\end{problem}
\begin{figureptx}{Leonhard Euler}{g:figure:idp52}{}%
\index{Euler, Leonhard!portrait of}%
\begin{image}{0.325}{0.35}{0.325}%
\includegraphics[width=\linewidth]{images/Euler.png}
\end{image}%
\tcblower
\end{figureptx}%
Leonhard Euler was a master at exploiting power series. In 1735, the 28 year-old Euler won acclaim for what is now called the Basel problem: to find a closed form for \(\sum_{n=1}^\infty\frac{1}{n^2}\). Other mathematicans knew that the series converged, but Euler was the first to find its exact value. The following problem essentially provides Euler's solution.%
\begin{problem}{The Basel Problem.}{g:problem:idp53}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Show that the power series for \(\frac{\sin x}{x}\) is given by \(1-\frac{1}{3!}x^2+\frac{1}{5!}x^4-\cdots\)%
\item[(b)]Use (a) to infer that the roots of \(1-\frac{1}{3!}x^2+\frac{1}{5!}x^4-\cdots\) are given by%
\begin{equation*}
x=\pm\pi,\,\pm 2\pi,\,\pm 3\pi,\,\ldots
\end{equation*}
%
\item[(c)]Suppose \(p(x)=a_0+a_1x+\cdots+a_nx^n\)is a polynomial with roots \(r_1,\,r_2,\,\ldots,r_n\). Show that if \(a_0\neq\) \(0\), then all the roots are non-zero and%
\begin{equation*}
p(x)=a_0\left(1-\frac{x}{r_1}\right)\left(1-\frac{x}{r_2}\right)\cdots\left(1-\frac{x}{r_n}\right)\text{.}
\end{equation*}
%
\item[(d)]Assuming that the result in c holds for an infinite polynomial power series, deduce that%
\begin{align*}
1-\frac{1}{3!}x^2+\frac{1}{5!}x^4-\cdots\amp =\left(1-\left(\frac{x}{\pi}\right)^2\right)\left(1-\left(\frac{x}{2\pi}\right)^2\right)\left(1-\left(\frac{x}{3\pi}\right)^2\right)\cdots
\end{align*}
%
\item[(e)]Expand this product to deduce%
\begin{equation*}
\sum_{n=1}^\infty\frac{1}{n^2}=\frac{\pi^2}{6}.{}
\end{equation*}
%
\end{enumerate}
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 4.3 Additional Problems}
\typeout{************************************************}
%
\begin{sectionptx}{Additional Problems}{}{Additional Problems}{}{}{x:section:CalcIn17th18thCentury-AddProb}
\begin{problem}{}{g:problem:idp54}%
\index{series!Geometric series!alternating}\index{series!Geometric series!derivation of the series representation of \(\ln(1+x)\) from} Use the geometric series to obtain the series%
\begin{align*}
\ln \left(1+x\right)\amp =x-\frac{1}{2}x^2+\frac{1}{3}x^3-\cdots\\
\amp =\sum_{n=0}^\infty\frac{(-1)^n}{n+1}x^{n+1}.{}
\end{align*}
%
\end{problem}
\begin{problem}{}{g:problem:idp55}%
\index{power series!drills} \emph{Without} using Taylor's Theorem, represent the following functions as power series expanded about 0 (i.e., in the form \(\sum_{n=0}^\infty a_nx^n\)).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\(\ln\left(1-x^2\right)\)%
\item[(b)]\(\frac{x}{1+x^2}\)%
\item[(c)]\(\arctan \left(x^3\right)\)%
\item[(d)]\(\ln\left(2+x\right)\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp56}{}\quad{}\(2+x=2\left(1+\frac{x}{2}\right)\)%
\end{enumerate}
\end{problem}
\begin{problem}{}{g:problem:idp57}%
\index{power series!for \(a^x\) expanded about 0} Let \(a\) be a positive real number. Find a power series for \(a^x\) expanded about 0.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp58}{}\quad{}\(a^x=e^{\ln\,\left(a^x\right)}\)%
\end{problem}
\begin{problem}{}{g:problem:idp59}%
\index{power series!of \(\sin(x)\), expanded about \(a\)}\index{\(\sin x\)!as a power series} Represent the function \(\)sin \(x\) as a power series expanded about \(a\) (i.e., in the form \(\sum_{n=0}^\infty a_n\left(x-a\right)^n\)).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp60}{}\quad{}\(\sin x=\sin \left(a+x-a\right)\).%
\end{problem}
\begin{problem}{}{g:problem:idp61}%
\index{Maclaurin series drills} \emph{Without} using Taylor's Theorem, represent the following functions as a power series expanded about \(a\) for the given value of \(a\) (i.e., in the form \(\sum_{n=0}^\infty a_n\left(x-a\right)^n\)).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\(\ln x\), \(a=1\)%
\item[(b)]\(e^x\), \(a=3\)%
\item[(c)]\(x^3+2x^2+3\) , \(a=1\)%
\item[(d)]\(\frac{1}{x}\) , \(a=5\)%
\end{enumerate}
\end{problem}
\begin{problem}{}{g:problem:idp62}%
\index{series!term by term integration of} Evaluate the following integrals as series.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\(\displaystyle\int_{x=0}^1e^{x^2}\dx{ x}\)%
\item[(b)]\(\displaystyle\int_{x=0}^1\frac{1}{1+x^4}\dx{ x}\)%
\item[(c)]\(\displaystyle\int_{x=0}^1\sqrt[3]{1-x^3}\dx{ x}\)%
\end{enumerate}
\end{problem}
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Chapter 5 Questions Concerning Power Series}
\typeout{************************************************}
%
\begin{chapterptx}{Questions Concerning Power Series}{}{Questions Concerning Power Series}{}{}{x:chapter:PowerSeriesQuestions}
%
%
\typeout{************************************************}
\typeout{Section 5.1 Taylor's Formula}
\typeout{************************************************}
%
\begin{sectionptx}{Taylor's Formula}{}{Taylor's Formula}{}{}{x:section:PowerSeriesQuestions-TaylorsFormula}
As we saw in the previous chapter, representing functions as power series was a fruitful strategy for mathematicans in the eighteenth century (as it still is). Differentiating and integrating power series term by term was relatively easy, \emph{seemed} to work, and led to many applications. Furthermore, power series representations for all of the elementary functions could be obtained if one was clever enough.%
\par
However, cleverness is an unreliable tool. Is there some systematic way to find a power series for a given function? To be sure, there were nagging questions: If we can find a power series, how do we know that the series we've created represents the function we started with? Even worse, is it possible for a function to have more than one power series representation centered at a given value \(a?\) This uniqueness issue is addressed by the following theorem.%
\begin{theorem}{}{}{x:theorem:TaylorSeriesThm}%
\index{Taylor's Formula} If \(f(x)=\sum_{n=0}^\infty a_n(x-a)^n\), then \(a_n=\frac{f^{(n)}(a)}{n!}\), where \(f^{(n)}(a)\) represents the \(n^{th}\) derivative of \(f\) evaluated at \(a\).%
\end{theorem}
A few comments about \hyperref[x:theorem:TaylorSeriesThm]{Theorem~{\xreffont\ref{x:theorem:TaylorSeriesThm}}} are in order. Notice that we did \emph{not} start with a function and derive its series representation. Instead we \emph{defined} \(f(x)\) to be the series we wrote down. This assumes that the expression \(\sum_{n=0}^\infty a_n(x-a)^n\) actually has meaning (that it converges). At this point we have every reason to expect that it does, however expectation is not proof so we note that this is an assumption, not an established truth. Similarly, the idea that we can differentiate an infinite polynomial term-by-term as we would a finite polynomial is also assumed. As before, we follow in the footsteps of our 18th century forebears in making these assumptions. For now.%
\begin{problem}{}{g:problem:idp63}%
\index{Taylor's Formula} Prove \hyperref[x:theorem:TaylorSeriesThm]{Theorem~{\xreffont\ref{x:theorem:TaylorSeriesThm}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp64}{}\quad{}\(f(a)=a_0+a_1(a-a)+a_2(a-a)^2+\cdots=a_0\). Differentiate to obtain the other terms.%
\end{problem}
From \hyperref[x:theorem:TaylorSeriesThm]{Theorem~{\xreffont\ref{x:theorem:TaylorSeriesThm}}} we see that if we do start with the function \(f(x)\) then no matter how we obtain its power series, the result will always be the same. The series%
\begin{equation*}
\sum_{n=0}^\infty\frac{f^{(n)}(a)}{n!}(x-a)^n=f(a)+f^\prime(a)(x-a)+\frac{f^{\prime\prime}(a)}{2!}(x-a)^2+\frac{f^{\prime\prime\prime}(a)}{3!}(x-a)^3+\cdots
\end{equation*}
%
\begin{figureptx}{Brook Taylor}{g:figure:idp65}{}%
\index{Taylor, Brook!portrait of}%
\begin{image}{0.325}{0.35}{0.325}%
\includegraphics[width=\linewidth]{images/Taylor.png}
\end{image}%
\tcblower
\end{figureptx}%
is called the \emph{\alert{Taylor series} for \(f\) expanded about (centered at) \(a\)}. Although this systematic ``machine'' for obtaining power series for a function seems to have been known to a number of mathematicians in the early 1700's, Brook Taylor \index{Taylor, Brook} was the first to publish this result in his \textit{Methodus Incrementorum} (1715). The special case when \(a=0\) was included by Colin Maclaurin \index{Maclaurin, Colin} in his \emph{Treatise of Fluxions} (1742). Thus when \(a=0\), the series \(\sum_{n=0}^\infty\frac{f^{(n)}(0)}{n!}x^n\) is often called the \emph{Maclaurin Series for \(f\)}.%
\par
The ``prime notation'' for the derivative was not used by Taylor, \index{Taylor, Brook} Maclaurin or their contemporaries. It was introduced by Joseph Louis Lagrange \index{Lagrange, Joseph-Louis} in his 1779 work \textit{Thorie des Fonctions Analytiques}. In that work, Lagrange sought to get rid of Leibniz's infinitesimals\index{Leibniz, Gottfried Wilhelm} and base calculus on the power series idea. His idea was that by representing every function as a power series, calculus could be done ``algebraically'' by manipulating power series and examining various aspects of the series representation instead of appealing to the ``controversial'' notion of infinitesimals. He implicitly assumed that every continuous function could be replaced with its power series representation.%
\par
That is, he wanted to think of the Taylor series as a ``great big polynomial,'' because polynomials are easy to work with. It was a very simple, yet exceedingly clever and far-reaching idea. Since \(e^x = 1 +x +x^2/2 +\ldots\), for example, why not just define the exponential to be the series and work with the series. After all, the series is just a very long polynomial.%
\par
This idea did not come out of nowhere. Leonhard Euler \index{Euler, Leonhard} had put exactly that idea to work to solve many problems throughout the 18th century. Some of his solutions are still quite breath-taking when you first see them~\hyperlink{x:biblio:sandifer07__early_mathem_leonar_euler}{[{\xreffont 14}]}.%
\par
Taking his cue from the Taylor series%
\begin{equation*}
f(x) = \sum_{n=0}^\infty\frac{f^{(n)}(a)}{n!}(x-a)^n
\end{equation*}
%
\begin{figureptx}{Joseph-Louis Lagrange}{g:figure:idp66}{}%
\index{Lagrange, Joseph-Louis!portrait of}%
\begin{image}{0.325}{0.35}{0.325}%
\includegraphics[width=\linewidth]{images/Lagrange.png}
\end{image}%
\tcblower
\end{figureptx}%
Lagrange observed that the coefficient of \((x-a)^n\) provides the derivative of \(f\) at \(a\) (divided by \(n!\)). Modifying the formula above to suit his purpose, Lagrange supposed that every differentiable function could be represented as%
\begin{equation*}
f(x) = \sum_{n=0}^\infty g_n(a)(x-a)^n\text{.}
\end{equation*}
%
\par
If we regard the parameter \(a\) as a variable then \(g_1\) is the derivative of \(f\), \(g_2=2f^{\prime\prime}\) and generally%
\begin{equation*}
g_n=n!f^{(n)}\text{.}
\end{equation*}
%
\par
Lagrange dubbed his function \(g_1\) the \textit{``fonction drive''} from which we get the modern name ``derivative.''%
\par
All in all, this was a very clever and insightful idea whose only real flaw is that its fundamental assumption is not true. It turns out that not every differentiable function can be represented as a Taylor series. This was demonstrated very dramatically by Augustin Cauchy's \index{Cauchy, Augustin} famous counter-example%
\begin{equation}
f(x) = \begin{cases} e^{-\frac{1}{x^2}}\amp  x\ne0\\ 0 \amp x=0 \end{cases}\text{.}\label{x:men:eq_CauchyCounterEx}
\end{equation}
%
\par
This function is actually infinitely differentiable everywhere but its Maclaurin series (that is, a Taylor series with \(a=0\)) does not converge to \(f\) because all of its derivatives at the origin are equal to zero: \(f^{(n)}(0) = 0, \forall n\in\NN\).%
\par
Computing these derivatives using the definition you learned in calculus is not conceptually difficult but the formulas involved do become complicated rather quickly. Some care must be taken to avoid error.%
\par
To begin with, let's compute a few derivatives when \(x \neq 0\).%
\begin{align*}
f^{(0)}(x) \amp = e^{x^{-2}}\\
f^{(1)}(x) \amp = 2x^{-3}e^{-x^{-2}}\\
f^{(2)}(x) \amp = \left(4x^{-6}-6x^{-4}\right)e^{-x^{-2}}\text{.}
\end{align*}
%
\par
As you can see the calculations are already getting a little complicated and we've only taken the second derivative. To streamline things a bit we take \(y= x^{-1}\), and define \(p_2(x) = 4x^6-6x^4\) so that%
\begin{equation*}
f^{(2)}(x) = p_2(x^{-1})e^{-x^{-2}} = p_2(y)e^{-y^2}\text{.}
\end{equation*}
%
\begin{problem}{Cauchy's Counterexample, Part 1.}{g:problem:idp67}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Adopting the notation \(y=x^{-1}\) and \(f^{(n)}(x) =p_n(y)e^{-y^2}\), find \(p_{n+1}(y)\) in terms of \(p_{n}(y)\). [Note: Don't forget that you are differentiating with respect to \(x\), not \(y\).]%
\item[(b)]Use induction on \(n\) to show that \(p_n(y)\) is a polynomial for all \(n\in\NN\).%
\end{enumerate}
\end{problem}
Unfortunately everything we've done so far only gives us the derivatives we need when \(x\) is \emph{not} zero, and we need the derivatives when \(x\) \emph{is} zero. To find these we need to get back to very basic ideas.%
\par
Let's assume for the moment that we know that \(f^{(n)}(0)=0\) and recall that%
\begin{align*}
f^{(n+1)}(0) \amp = \limit{x}{0}{\frac{f^{(n)}(x)-f^{(n)}(0)}{x-0}}\\
f^{(n+1)}(0) \amp = \limit{x}{0}{x^{-1}p_n(x^{-1})e^{-x^{-2}}}\\
f^{(n+1)}(0) \amp = \limit{y}{\pm\infty}{\frac{yp_n(y)}{e^{y^2}}}\text{.}
\end{align*}
%
\par
We can close the deal with the following problem.%
\begin{problem}{Cauchy's Counterexample, Part 2.}{g:problem:idp68}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Let \(m\) be a nonnegative integer. Show that \(\limit{y}{\pm\infty}{\frac{y^m}{e^{y^2}}}=0\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp69}{}\quad{}Induction and a dash of L'Hpital's rule should do the trick.%
\item[(b)]Prove that \(\limit{y}{\pm\infty}{\frac{q(y)}{e^{y^2}}}=0\) for any polynomial \(q\).%
\item[(c)]Let \(f(x)\) be as in \hyperref[x:men:eq_CauchyCounterEx]{equation~({\xreffont\ref{x:men:eq_CauchyCounterEx}})} and show that for every nonnegative integer \(n\), \(f^{(n)}(0)=0\).%
\end{enumerate}
\end{problem}
This example showed that while it was fruitful to exploit Taylor series representations of various functions, basing the foundations of calculus on power series was not a sound idea.%
\par
While Lagrange's \index{Lagrange, Joseph-Louis} approach wasn't totally successful, it was a major step away from infinitesimals and toward the modern approach. We still use aspects of it today. For instance we still use his prime notation (\(f^\prime\)) to denote the derivative.%
\par
Turning Lagrange's idea on its head it is clear that if we know how to compute derivatives, we can use this machine to obtain a power series when we are not ``clever enough'' to obtain the series in other (typically shorter) ways. For example, consider Newton's binomial series when \(\alpha=\frac{1}{2}\). Originally, we obtained this series by extending the binomial theorem to non-integer exponents. Taylor's formula provides a more systematic way to obtain this series:%
\begin{align*}
f(x)\amp =(1+x)^{\frac{1}{2}};\amp f(0)\amp =1\\
f^\prime(x)\amp =\frac{1}{2}(1+x)^{\frac{1}{2}-1};\amp  f^\prime(0)\amp =\frac{1}{2}\\
f^{\prime\prime}(x)\amp =\frac{1}{2}\left(\frac{1}{2}-1\right)(1+x)^{\frac{1}{2}-2}\amp f^{\prime\prime}(0)\amp =\frac{1}{2}\left(\frac{1}{2}-1\right)
\end{align*}
and in general since%
\begin{align*}
f^{(n)}(x)\amp =\frac{1}{2}\left(\frac{1}{2}-1\right)\cdots\left(\frac{1}{2}-(n-1)\right)(1+x)^{\frac{1}{2}-n}\\
\intertext{we have}
f^{(n)}(0)\amp =\frac{1}{2}\left(\frac{1}{2}-1\right)\cdots\left(\frac{1}{2}-(n-1)\right)\text{.}
\end{align*}
%
\par
Using Taylor's formula we obtain the series%
\begin{equation*}
\sum_{n=0}^\infty\frac{f^{(n)}(0)}{n!}x^n = 1+\sum_{n=1}^\infty\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)\cdots\left( \frac{1}{2}-(n-1)\right)}{n!}x^n= 1+\sum_{n=1}^\infty\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}x^n
\end{equation*}
which agrees with \hyperref[x:men:eq_BinomialSeries]{equation~({\xreffont\ref{x:men:eq_BinomialSeries}})} in the previous chapter.%
\begin{problem}{}{g:problem:idp70}%
\index{Taylor's Formula!use to obtain the general binomial series} Use Taylor's formula to obtain the general binomial series%
\begin{equation*}
(1+x)^\alpha=1+\sum_{n=1}^\infty\frac{\prod_{j=0}^{n-1}\left(\alpha-j\right)}{n!}x^n.{}
\end{equation*}
%
\end{problem}
\begin{problem}{}{g:problem:idp71}%
\index{\(e^x\)!Taylor's Series for}\index{Taylor's Series!expansion of \(e^x, \sin x\), and \(\cos x\)}\index{\(\sin x\)!Taylor's Series for}\index{\(\cos x\)!Taylor's Series for} Use Taylor's formula to obtain the Taylor series for the functions \(e^x\), sin \(x\), and cos \(x\) expanded about \(a\).%
\end{problem}
As you can see, Taylor's ``machine'' will produce the power series for a function (if it has one), but is tedious to perform. We will find, generally, that this tediousness can be an obstacle to understanding. In many cases it will be better to be clever if we can. This is usually shorter. However, it is comforting to have Taylor's formula available as a last resort.%
\par
The existence of a Taylor series is addressed (to some degree) by the following.%
\begin{theorem}{}{}{x:theorem:TaylorsTheorem}%
\index{Taylor's Theorem} If \(f^\prime,\,f^{\prime\prime},\,\ldots,\,f^{(n+1)}\) are all continuous on an interval containing \(a\) and \(x\), then%
\begin{align*}
f(x)=f(a)+\frac{f^{\prime}(a)}{1!}(x-a)+\frac{f^{\prime \prime}(a)}{2!}(x-a)^2\amp +\cdots+\frac{f^{(n)}(a)}{n!}(x-a)^n\\
\amp + \frac{1}{n!}\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{t}\text{.}
\end{align*}
%
\end{theorem}
Before we address the proof, notice that the \(n\)-th degree polynomial%
\begin{equation*}
f(a)+\frac{f^{\,\prime}(a)}{1!}(x-a)+\frac{f^{\,\prime\prime}(a)}{2!}(x-a)^2+\cdots+\frac{f^{(n)}(a)}{n!}(x-a)^n
\end{equation*}
resembles the Taylor series and, in fact, is called the \emph{\(n\)-th degree Taylor polynomial of \(f\) about \(a\).} \hyperref[x:theorem:TaylorsTheorem]{Theorem~{\xreffont\ref{x:theorem:TaylorsTheorem}}} says that a function can be written as the sum of this polynomial and a specific integral which we will analyze in the next chapter. We will get the proof started and leave the formal induction proof as an exercise.%
\par
Notice that the case when \(n=0\) is really a restatement of the Fundamental Theorem of Calculus. Specifically, the FTC says \(\int_{t=a}^xf^\prime(t)\dx{ t}=f(x)-f(a)\) which we can rewrite as%
\begin{equation*}
f(x)=f(a)+\frac{1}{0!}\int_{t=a}^xf^\prime(t)(x-t)^0\dx{ t}
\end{equation*}
to provide the anchor step for our induction.%
\par
To derive the case where \(n=1\), we use integration by parts. If we let%
\begin{align*}
u\amp =f^\prime(t)\amp  d v\amp =(x-t)^0d t\\
d u\amp =f^{\prime\prime}(t)d t\amp  v\amp =-\frac{1}{1}(x-t)^1
\end{align*}
we obtain%
\begin{align*}
f(x)\amp =f(a)+\frac{1}{0!}\left(-\frac{1}{1}f^\prime(t)(x-t)^1|_{t=a}^{^x}+\frac{1}{1} \int_{t=a}^xf^{\prime\prime}(t)(x-t)^1\dx{ t}\right)\\
\amp =f(a)+\frac{1}{0!}\left(-\frac{1}{1}f^\prime(x)(x-x)^1+ \frac{1}{1}f^\prime(a)(x-a)^1+\frac{1}{1}\int_{t=a}^xf^{\prime\prime}(t)(x-t)^1\dx{ t}\right)\\
\amp =f(a)+\frac{1}{1!}f^\prime(a)\left(x-a\right)^1 + \frac{1}{1!}\int_{t=a}^xf^{\prime\prime}(t)(x-t)^1\dx{ t}\text{.}
\end{align*}
%
\begin{problem}{}{g:problem:idp72}%
\index{Taylor's Theorem} Provide a formal induction proof for \hyperref[x:theorem:TaylorsTheorem]{Theorem~{\xreffont\ref{x:theorem:TaylorsTheorem}}}.%
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 5.2 Series Anomalies}
\typeout{************************************************}
%
\begin{sectionptx}{Series Anomalies}{}{Series Anomalies}{}{}{x:section:PowerSeriesQuestions-SeriesAnomalies}
Up to this point, we have been somewhat frivolous in our approach to series. This approach mirrors eighteenth century mathematicians who ingeniously exploited calculus and series to provide mathematical and physical results which were virtually unobtainable before. Mathematicans were eager to push these techniques as far as they could to obtain their results and they often showed good intuition regarding what was mathematically acceptable and what was not. However, as the envelope was pushed, questions about the validity of the methods surfaced.%
\par
As an illustration consider the series expansion%
\begin{equation*}
\frac{1}{1+x}=1-x+x^2-x^3+\cdots\text{.}
\end{equation*}
%
\par
If we substitute \(x=1\) into this equation, we obtain%
\begin{equation*}
\frac{1}{2}=1-1+1-1+\cdots\text{.}
\end{equation*}
%
\par
If we group the terms as follows \((1-1)+(1-1)+\cdots\), the series would equal \(0\). A regrouping of \(1+(-1+1)+(-1+1)+\cdots\) provides an answer of \(1\). This violation of the associative law of addition did not escape the mathematicians of the 1700's. In his 1760 paper \emph{On Divergent Series} Euler \index{Euler, Leonhard} said:%
\begin{quote}%
Notable enough, however are the controversies over the series \(1-1+1-1+etc\), \index{Leibniz, Gottfried Wilhelm} whose sum was given by Leibniz as \(\frac{1}{2}\), although others disagree .~.~. Understanding of this question is to be sought in the word ``sum;'' this idea, if thus conceived - namely, the sum of a series is said to be that quantity to which it is brought closer as more terms of a series are taken - has relevance only for the convergent series, and we should in general give up this idea of sum for divergent series. On the other hand, as series in analysis arise from the expansion of fractions or irrational quantities or even of transcendentals, it will, in turn, be permissible in calculation to substitute in place of such series that quantity out of whose development it is produced.%
\end{quote}
Even with this formal approach to series, an interesting question arises. The series for the antiderivative of \(\frac{1}{1+x}\) \emph{does} converge for \(x=1\) while this one does not. Specifically, taking the antiderivative of the above series, we obtain%
\begin{equation*}
\ln(1+x)=x-\frac{1}{2}x^2+\frac{1}{3}x^3-\cdots\text{.}
\end{equation*}
%
\par
If we substitute \(x=1\) into this series, we obtain \(\ln 2=1-\frac{1}{2}+\frac{1}{3}-\cdots\). It is not hard to see that such an alternating series converges. The following picture shows why. In this diagram, \(S_n\) denotes the partial sum \(1-\frac{1}{2}+\frac{1}{3}-\cdots+\frac{(-1)^{n+1}}{n}\).%
\begin{image}{0.05}{0.9}{0.05}%
\includegraphics[width=\linewidth]{images/AltHarmonic.png}
\end{image}%
From the diagram we can see \(S_2\leq S_4\leq S_6\leq\cdots\leq\cdots\leq S_5\leq S_3\leq S_1\) and \(S_{2k+1}-S_{2k}=\frac{1}{2k+1}\). It seems that the sequence of partial sums will converge to whatever is in the ``middle.'' Our diagram indicates that it is ln \(2\) in the middle but actually this is not obvious. Nonetheless it is interesting that one series converges for \(x=1\) but the other does not.%
\begin{problem}{}{g:problem:idp73}%
\index{Taylor's Series!used to approximate \(\ln 2\)}\index{determine the number of terms needed for the Taylor series to approximate \(\ln 2\) to within \(.0001\)} Use the fact that%
\begin{equation*}
1-\frac{1}{2}+\frac{1}{3}-\cdots+\frac{(-1)^{2k+1}}{2k}\leq\ln 2\leq 1-\frac{1}{2}+\frac{1}{3}-\cdots+\frac{(-1)^{2k+2}}{2k+1}
\end{equation*}
to determine how many terms of the series \(\sum_{n=1}^\infty\frac{(-1)^{n+1}}{n}\) should be added together to approximate \(\ln 2\) to within \(.0001\) without actually computing what \(\ln 2\) is.%
\end{problem}
There is an even more perplexing situation brought about by these examples. An infinite sum such as \(1-1+1-1+\cdots\) appears to not satisfy the associative law for addition. While a convergent series such as \(1-\frac{1}{2}+\frac{1}{3}-\cdots\) does satisfy the associative law, it does not satisfy the commutative law. In fact, it does not satisfy it rather spectacularly.%
\par
A generalization of the following result was stated and proved by Bernhard Riemann \index{Riemann, Bernhard} in 1854.%
\begin{theorem}{}{}{x:theorem:thm_rearrangements}%
\index{series!rearrangements of the Alternating Harmonic series} Let \(a\) be any real number. There exists a rearrangement of the series \(1-\frac{1}{2}+\frac{1}{3}-\cdots\) which converges to \(a\).%
\end{theorem}
This theorem shows that a series is most decidedly \emph{not} a great big sum. It follows that a power series is \emph{not} a great big polynomial.%
\par
To set the stage, consider the \emph{harmonic series} \index{Harmonic Series}%
\begin{equation*}
\sum_{n=1}^\infty\frac{1}{n}=1+\frac{1}{2}+\frac{1}{3}+\cdots\text{.}
\end{equation*}
%
\par
Even though the individual terms in this series converge to \(0\), the series still diverges (to infinity) as evidenced by the inequality%
\begin{align*}
\left(1+\frac{1}{2}\right)\amp +\left(\frac{1}{3}+\frac{1}{4}\right)+\left(\frac{1}{5}+\frac{1}{6}+ \frac{1}{7}+\frac{1}{8}\right)+\left(\frac{1}{9}+\cdots+\frac{1}{16}\right)+\cdots\\
\amp >\frac{1}{2}+\left(\frac{1}{4}+\frac{1}{4}\right)+\left(\frac{1}{8}+ \frac{1}{8}+\frac{1}{8}+\frac{1}{8}\right)+\left(\frac{1}{16}+\cdots+\frac{1}{16}\right)+\cdots\\
\amp =\frac{1}{2}+\frac{1}{2}+\frac{1}{2}+\frac{1}{2}+\cdots\\
\amp =   \infty\text{.}
\end{align*}
%
\par
Armed with this fact, we can see why \hyperref[x:theorem:thm_rearrangements]{Theorem~{\xreffont\ref{x:theorem:thm_rearrangements}}} is true. First note that%
\begin{equation*}
-\frac{1}{2}-\frac{1}{4}-\frac{1}{6}-\cdots=-\frac{1}{2}(1+\frac{1}{2}+ \frac{1}{3}+\cdots)=-\infty
\end{equation*}
and%
\begin{equation*}
1+\frac{1}{3}+\frac{1}{5}+\cdots\geq\frac{1}{2}+\frac{1}{4}+\frac{1}{6}+\ldots= \infty\text{.}
\end{equation*}
%
\par
This says that if we add enough terms of \(-\frac{1}{2}-\frac{1}{4}-\frac{1}{6}-\cdots\) we can make such a sum as small as we wish and if we add enough terms of \(1+\frac{1}{3}+\frac{1}{5}+\cdots\) we can make such a sum as large as we wish. This provides us with the general outline of the proof. The trick is to add just enough positive terms until the sum is just greater than \(a\). Then we start to add on negative terms until the sum is just less than \(a\). Picking up where we left off with the positive terms, we add on just enough positive terms until we are just above \(a\) again. We then add on negative terms until we are below \(a\). In essence, we are bouncing back and forth around \(a\). If we do this carefully, then we can get this rearrangement to converge to \(a\). The notation in the proof below gets a bit hairy, but keep this general idea in mind as you read through it.%
\par
Let \(O_1\) be the first odd integer such that \(1+\frac{1}{3}+\frac{1}{5}+\cdots+\frac{1}{O_1}>a\). Now choose \(E_1\) to be the first even integer such that%
\begin{equation*}
-\frac{1}{2}-\frac{1}{4}-\frac{1}{6}-\cdots-\frac{1}{E_1} \lt a-\left(1+\frac{1}{3}+\frac{1}{5}+\cdots+\frac{1}{O_1}\right)\text{.}
\end{equation*}
%
\par
Thus%
\begin{equation*}
1+\frac{1}{3}+\frac{1}{5}+\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4} - \frac{1}{6}-\cdots-\frac{1}{E_1}\lt a\text{.}
\end{equation*}
%
\par
Notice that we still have \(\frac{1}{O_1+2}+\frac{1}{O_1+4}+\cdots=\infty\). With this in mind, choose \(O_2\) to be the first odd integer with%
\begin{equation*}
\frac{1}{O_1+2}+\frac{1}{O_1+4}+\cdots\frac{1}{O_2}>a-\left(1+\frac{1}{3}+ \frac{1}{5}+\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}-\frac{1}{6}-\cdots- \frac{1}{E_1}\right)\text{.}
\end{equation*}
%
\par
Thus we have%
\begin{equation*}
a\lt 1+\frac{1}{3}+\frac{1}{5}+\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}- \frac{1}{6}-\cdots-\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+\cdots+ \frac{1}{O_2}\text{.}
\end{equation*}
%
\par
Furthermore, since%
\begin{equation*}
1+\frac{1}{3}+\frac{1}{5}+\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}- \frac{1}{6}-\cdots-\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+\cdots+ \frac{1}{O_2-2}\lt a
\end{equation*}
we have%
\begin{align*}
\amp \left|1+\frac{1}{3}+\frac{1}{5}+\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}- \frac{1}{6}-\cdots\right.\\
\amp \left.-\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+\cdots+ \frac{1}{O_2}-a\right|\lt \frac{1}{O_2}\text{.}
\end{align*}
%
\par
In a similar fashion choose \(E_2\) to be the first even integer such that%
\begin{align*}
1+\frac{1}{3}+\frac{1}{5}+\cdots\amp +\frac{1}{O_1}-\frac{1}{2}- \frac{1}{4}-\frac{1}{6}-\amp \cdots\\
\amp -\frac{1}{E_1}+ \frac{1}{O_1+2}+\frac{1}{O_1+4}+\cdots\\
\amp +\frac{1}{O_2}-\frac{1}{E_1+2}-\frac{1}{E_1+4}-\cdots\\
\amp -\frac{1}{E_2}\lt a\text{.}
\end{align*}
%
\par
Since%
\begin{align*}
1+\frac{1}{3}+\frac{1}{5}\amp +\cdots+\frac{1}{O_1}-\frac{1}{2}- \frac{1}{4}-\frac{1}{6}-\cdots-\frac{1}{E_1}\\
\amp +\frac{1}{O_1+2}+\frac{1}{O_1+4}+\cdots+\frac{1}{O_2}- \frac{1}{E_1+2}-\frac{1}{E_1+4}-\cdots-\frac{1}{E_2-2}>a
\end{align*}
then%
\begin{align*}
\left|1+\frac{1}{3}\right.+\frac{1}{5}\amp +\cdots+\frac{1}{O_1}-\frac{1}{2}- \frac{1}{4}-\frac{1}{6}-\cdots-\frac{1}{E_1}\\
\amp +\frac{1}{O_1+2}+\frac{1}{O_1+4}+\cdots+\frac{1}{O_2}- \frac{1}{E_1+2}-\frac{1}{E_1+4}-\cdots-\left.\frac{1}{E_2}-a\right|\\
\amp \lt \frac{1}{E_2}\text{.}
\end{align*}
%
\par
Again choose \(O_3\) to be the first odd integer such that%
\begin{align*}
a\lt 1+\frac{1}{3}\amp +\frac{1}{5}+\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}- \frac{1}{6}-\cdots\\
\amp -\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+\cdots+ \frac{1}{O_2}+ \cdots\\
\amp -\frac{1}{E_1+2}-\frac{1}{E_1+4}-\cdots-\frac{1}{E_2}+\frac{1}{O_2+2}+ \frac{1}{O_2+4}+\cdots+\frac{1}{O_3}
\end{align*}
and notice that%
\begin{align*}
\left|1+\frac{1}{3}\right.\amp +\frac{1}{5}+\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}\\
\amp -\frac{1}{6}-\cdots-\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+\cdots+ \frac{1}{O_2}+\cdots\\
\amp -\frac{1}{E_1+2}-\frac{1}{E_1+4}-\cdots-\frac{1}{E_2}+\frac{1}{O_2+2}+ \frac{1}{O_2+4}+\cdots+\left.\frac{1}{O_3}-a\right|\\
\amp \lt \frac{1}{O_3}\text{.}
\end{align*}
%
\par
Continue defining \(O_k\)and \(E_k\) in this fashion. Since \(\lim_{k\rightarrow\infty}\frac{1}{O_k}=\,\lim_{k\rightarrow\infty} \frac{1}{E_k}=0\), it is evident that the partial sums%
\begin{align*}
1+\frac{1}{3}\amp +\frac{1}{5}+\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}\\
\amp -\frac{1}{6}-\cdots-\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+\cdots\\
\amp + \frac{1}{O_2}+\cdots -\frac{1}{E_{k-2}+2}-\frac{1}{E_{k-2}+4}-\cdots\\
\amp -\frac{1}{E_{k-1}}+  \frac{1}{O_{k-1}+2}+\frac{1}{O_{k-1}+4}+\cdots+\frac{1}{O_k}
\end{align*}
and%
\begin{align*}
1+\frac{1}{3}\amp +\frac{1}{5}+\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}- \frac{1}{6}\\
\amp -\cdots-\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+\cdots+ \frac{1}{O_2}+\cdots\\
\amp -\frac{1}{E_{k-2}+2}-\frac{1}{E_{k-2}+4}-\cdots-\frac{1}{E_{k-1}}
\end{align*}
must converge to \(a\). Furthermore, it is evident that every partial sum of the rearrangement%
\begin{align*}
1+\frac{1}{3}\amp +\frac{1}{5}+\cdots+\frac{1}{O_1}-\frac{1}{2}-\frac{1}{4}- \frac{1}{6}\\
\amp -\cdots-\frac{1}{E_1}+\frac{1}{O_1+2}+\frac{1}{O_1+4}+\cdots+ \frac{1}{O_2}+\cdots
\end{align*}
is trapped between two such extreme partial sums. This forces the entire rearranged series to converge to \(a\).%
\par
The next two problems are similar to the above, but notationally are easier since we don't need to worry about converging to an actual number. We only need to make the rearrangement grow (or shrink in the case of \hyperref[x:problem:prob_RearrangeDivToNegInf]{problem~{\xreffont\ref{x:problem:prob_RearrangeDivToNegInf}}}) without bound.%
\begin{problem}{}{g:problem:idp74}%
\index{series!Alternating Harmonic Series!rearrangements of}\index{series!Alternating Harmonic Series!there is a rearrangement of the alternating harmonic series which diverges to \(\infty\)} Show that there is a rearrangement of \(1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots\) which diverges to \(\infty\).%
\end{problem}
\begin{problem}{}{x:problem:prob_RearrangeDivToNegInf}%
\index{Alternating Harmonic Series!rearrangements of}\index{Alternating Harmonic Series!there is a rearrangement of the alternating harmonic series which diverges to \(-\infty\)} Show that there is a rearrangement of \(1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots\) which diverges to \(-\infty\).%
\end{problem}
It is fun to know that we can rearrange some series to make them add up to anything you like but there is a more fundamental idea at play here. That the negative terms of the alternating Harmonic Series \emph{diverge} to negative infinity and the positive terms \emph{diverge} to positive infinity make the \emph{convergence} of the alternating series very special.%
\par
Consider, first we add \(1\). This is one of the positive terms so our sum is starting to increase without bound. Next we add \(-1/2\) which is one of the negative terms so our sum has turned around and is now starting to decrease without bound. Then another positive term is added: increasing without bound. Then another negative term: decreasing. And so on. The convergence of the alternating Harmonic Series is the result of a delicate balance between a tendency to run off to positive infinity and back to negative infinity. When viewed in this light it is not really too surprising that rearranging the terms can destroy this delicate balance.%
\par
Naturally, the alternating Harmonic Series is not the only such series. Any such series is said to converge ``conditionally'' \textemdash{} the condition being the specific arrangement of the terms.%
\par
To stir the pot a bit more, some series do satisfy the commutative property. More specifically, one can show that any rearrangement of the series \(1-\frac{1}{2^2}+\frac{1}{3^2}-\cdots\) must converge to the same value as the original series (which happens to be \(\int_{x=0}^1\frac{\text{ ln } (1+x)}{x}dx\approx.8224670334\)). Why does one series behave so nicely whereas the other does not?%
\par
Issues such as these and, more generally, the validity of using the infinitely small and infinitely large certainly existed in the 1700's, but they were overshadowed by the utility of the calculus. Indeed, foundational questions raised by the above examples, while certainly interesting and of importance, did not significantly deter the exploitation of calculus in studying physical phenomena. However, the envelope eventually was pushed to the point that not even the most practically oriented mathematician could avoid the foundational issues.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 5.3 Additional Problems}
\typeout{************************************************}
%
\begin{sectionptx}{Additional Problems}{}{Additional Problems}{}{}{x:section:PowerSeriesQuestions-AddProb}
\begin{problem}{}{g:problem:idp75}%
Use Taylor's formula to find the Taylor series of the given function expanded about the given point \(a\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\(f(x)=\ln\left(1+x\right)\), \(a=0\)%
\item[(b)]\(f(x)=e^x\), \(a=-1\)%
\item[(c)]\(f(x)=x^3+x^2+x+1\), \(a=0\)%
\item[(d)]\(f(x)=x^3+x^2+x+1\), \(a=1\)%
\end{enumerate}
\end{problem}
\end{sectionptx}
\end{chapterptx}
\end{partptx}
%
%
\typeout{************************************************}
\typeout{Part II Interregnum}
\typeout{************************************************}
%
\begin{partptx}{Interregnum}{}{Interregnum}{}{}{x:part:Interregnum}
 %
%
\typeout{************************************************}
\typeout{Chapter 6 Joseph Fourier: The Man Who Broke Calculus}
\typeout{************************************************}
%
\begin{chapterptx}{Joseph Fourier: The Man Who Broke Calculus}{}{Joseph Fourier: The Man Who Broke Calculus}{}{}{x:chapter:Interegnum}
%
%
\typeout{************************************************}
\typeout{Section 6.1 Joseph Fourier and His Series}
\typeout{************************************************}
%
\begin{sectionptx}{Joseph Fourier and His Series}{}{Joseph Fourier and His Series}{}{}{g:section:idp76}
Applying mathematics to physical problems such as heat flow in a solid body drew much attention in the latter part of the 1700's and the early part of the 1800's. One of the people to attack the heat flow problem was%
\begin{figureptx}{Jean Baptiste Joseph Fourier}{g:figure:idp77}{}%
\index{Fourier, Jean Baptiste Joseph!portrait of}%
\begin{image}{0.325}{0.35}{0.325}%
\includegraphics[width=\linewidth]{images/Fourier.png}
\end{image}%
\tcblower
\end{figureptx}%
\index{Fourier, Jean Baptiste Joseph} Jean Baptiste Joseph Fourier.  Fourier submitted a manuscript on the subject, \textit{Sur la propagation de la chaleur} (\emph{On the Propagation of Heat}), to the \textit{Institut National des Sciences et des Arts} in 1807.  These ideas were subsequently published in \textit{La theorie analytique de la chaleur} (\emph{The Analytic Theory of Heat (1822)}).%
\par
To examine Fourier's ideas, consider the example of a thin wire of length one, which is perfectly insulated and whose endpoints are held at a fixed temperature of zero. Given an initial temperature distribution in the wire, the problem is to monitor the temperature of the wire at any point \(x\) and at any time \(t\). Specifically, if we let \(u(x,t)\) denote the temperature of the wire at point \(x\in[0,1]\) at time \(t\geq 0\), then it can be shown that \(u\) must satisfy the one-dimensional heat equation \(\rho^2\frac{\partial^2u}{\partial x^2}=\frac{\partial u}{\partial t}\), where \(\rho^2\) is a positive constant known as the thermal diffusivity. If the initial temperature distribution is given by the function \(f(x)\), then the \(u\) we are seeking must satisfy all of the following%
\begin{equation*}
\rho^2\frac{\partial^2u}{\partial x^2}=\frac{\partial u}{\partial t}
\end{equation*}
%
\begin{align*}
u(0,t)\amp =u(1,t)=0,\ \ \forall\,t\geq 0\\
u(x,0)\amp =f(x),\ \ \forall\,x\in[\,0,1]\text{.}
\end{align*}
%
\par
To solve this, Fourier employed what is now referred to as Fourier's method of separation of variables. Specifically, Fourier looked for solutions of the form \(u(x,t)=X(x)T(t)\); that is, solutions where the \(x\)-part can be separated from the \(t\)-part. Assuming that \(u\) has this form, we get \(\frac{\partial^2u}{\partial x^2}=X^{\prime\prime}T\) and \(\frac{\partial u}{\partial t}=X\,T^{\prime}\). Substituting these into the differential equation \(\rho^2\frac{\partial^2u}{\partial x^2}=\frac{\partial u}{\partial t}\), we obtain%
\begin{equation*}
\rho^2X^{\prime\prime}T=X T^\prime\text{  or  } \frac{X^{\prime\prime}}{X}=\frac{T^\prime}{\rho^2T}\text{.}
\end{equation*}
%
\par
Since the left-hand side involves no \(t\)'s and the right-hand side involves no \(x\)'s, both sides must equal a constant \(k\). Thus we have%
\begin{equation*}
X^{\prime\prime}=k X\text{ and } T^\prime=\rho^2k T\text{.}
\end{equation*}
%
\begin{problem}{}{x:problem:prob_HarmonicMotion}%
\index{Heat Equation, the}\index{Heat Equation, the!parameter \(k\) must be less than zero} Show that \(T=Ce^{\rho^2kt}\) satisfies the equation \(T^\prime=\rho^2k T\), where \(C\), and \(\rho\) are arbitrary constants. Use the physics of the problem to show that if \(u\) is not constantly zero, then \(k\lt 0\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp78}{}\quad{}Consider \(\lim_{t\rightarrow\infty}u(x,t)\).%
\end{problem}
Using the result from \hyperref[x:problem:prob_HarmonicMotion]{problem~{\xreffont\ref{x:problem:prob_HarmonicMotion}}} that \(k\lt 0\), we will let \(k=-p^2\).%
\begin{problem}{}{g:problem:idp79}%
\index{Heat Equation, the}\index{Heat Equation, the!solving for \(\xi(x)\)} Show that \(X=A\sin\left(px\right)+B\cos\left(px\right)\) satisfies the equation \(X\,''=-p^2X\), where \(A\) and \(B\) are arbitrary constants. Use the boundary conditions \(u(0,t)=u(1,t)=0\), \(\forall\,t\geq 0\) to show that \(B=0\) and \(A\sin p=0\). Conclude that if \(u\) is not constantly zero, then \(p=n\pi\), where \(n\) is any integer.%
\end{problem}
\begin{problem}{}{g:problem:idp80}%
\index{Heat Equation, the!fundamental solutions of}%
\par
Show that if \(u_1\) and \(u_2\) satisfy the equations \(\rho^2\frac{\partial^2u}{\partial x^2}=\frac{\partial
u}{\partial t}\) and \(u(0,t)=u(1,t)=0, \forall\,t\geq
0\) then \(u=A_1u_1+A_2u_2\) satisfy these as well, where \(A_1\) and \(A_2\) are arbitrary constants.%
\end{problem}
\index{Fourier, Jean Baptiste Joseph} Putting all of these results together, Fourier surmised that the general solution to%
\begin{equation*}
\rho^2\frac{\partial^2u}{\partial x^2}=\frac{\partial u}{\partial t} u(0,t)=u(1,t)=0, \,\forall\,t\geq 0
\end{equation*}
could be expressed as the series%
\begin{equation*}
u(x,t)=\sum_{n=1}^\infty A_ne^{-(\rho n\pi)^2t}\sin\left(n\pi x\right)\text{.}
\end{equation*}
%
\par
All that is left is to have \(u\) satisfy the initial condition \(u(x,0)=f(x)\), \(\forall\,x\in[\,0,1]\). That is, we need to find coefficients \(A_n\), such that%
\begin{equation*}
f(x)=u(x,0)=\sum_{n=1}^\infty A_n\sin\left(n\pi x\right)\text{.}
\end{equation*}
%
\par
The idea of representing a function as a series of sine waves was proposed by Daniel Bernoulli in 1753 while examining the problem of modeling a vibrating string. Unfortunately for Bernoulli, he didn't know how to compute the coefficients in such a series representation. What distinguished Fourier was that he developed a technique to compute these coefficients. The key is the result of the following problem.%
\begin{problem}{}{x:problem:prob_SinOrthogonality}%
\index{\(\sin x\)!orthogonality of}\index{orthogonality!of \(\sin nx\)} Let \(n\) and \(m\) be positive integers. Show%
\begin{equation*}
\int_{x=0}^1\sin\left(n\pi x\right)\sin\left(m\pi x\right)\dx{ x}= \left\{\begin{matrix}0\amp \text{ if } n\neq m\\ \frac{1}{2}\amp \text{ if } n=m \end{matrix} \right.. {}
\end{equation*}
%
\end{problem}
Armed with the result from \hyperref[x:problem:prob_SinOrthogonality]{Problem~{\xreffont\ref{x:problem:prob_SinOrthogonality}}}, \index{Fourier, Jean Baptiste Joseph} Fourier could compute the coefficients \(A_n\) in the series representation \(f(x)=\sum_{n=1}^\infty A_n \sin\left(n\pi x\right)\) in the following manner. Since we are trying to find \(A_n\) for a particular (albeit general) \(n\), we will temporarily change the index in the summation from \(n\) to \(j\). With this in mind, consider%
\begin{align*}
\int_{x=0}^1f(x)\sin\left(n\pi x\right)\dx{ x} \amp =\int_{x=0}^1\left(\sum_{j=1}^\infty A_j\text{ sin } \left(j\pi x\right)\right)\sin\left(n\pi x\right)\dx{ x}\\
\amp =\sum_{j=1}^\infty A_j\int_{x=0}^1\sin\left(j\pi x\right)\sin\left(n\pi x\right)\dx{ x}\\
\amp =A_n\cdot\frac{1}{2}
\end{align*}
%
\par
This leads to the formula \(A_n=2\int_{x=0}^1f(x)\sin\left(n\pi x\right)d x\).%
\par
The above series \(f(x)=\sum_{n=1}^\infty A_n\sin\left(n\pi x\right)\) with \(A_n=2\int_{x=0}^1f(x)\sin\left(n\pi x\right)\dx{ x}\) is called the \emph{Fourier (sine) series of \(\boldsymbol{f}\)}.%
\begin{example}{}{g:example:idp81}%
Let's apply this to the following function, \(f(x)=\frac{1}{2}-\abs{x-\frac{1}{2}}\), whose graph of this function is seen below.%
\begin{image}{0.125}{0.75}{0.125}%
\includegraphics[width=\linewidth]{images/FourierEx1.png}
\end{image}%
\end{example}
\begin{problem}{}{g:problem:idp82}%
\index{Fourier Series!sine series of an odd function} Let \(n\) be a positive integer. Show that if%
\begin{equation*}
f(x)=\frac{1}{2}-\abs{x-\frac{1}{2}}
\end{equation*}
then%
\begin{equation*}
\int_{x=0}^1f(x)\sin\left(n\pi x\right)d x = \frac{2}{\left(n\pi\right)^2}\sin\left(\frac{n\pi}{2}\right)
\end{equation*}
and show that the Fourier sine series of \(f\) is given by%
\begin{equation*}
f(x)=\sum_{n=1}^\infty\frac{4}{\left(n\pi\right)^2}\sin\left(\frac{n\pi}{2} \right)\sin\left(n\pi x\right)=\frac{4}{\pi^2}\sum_{k=0}^\infty\frac{\left(-1\right)^k}{\left(2k+1\right)^2}\sin\left(\left(2k+1\right)\pi x\right).{}
\end{equation*}
%
\end{problem}
To check if this series really does, in fact, represent \(f\) on \([0,1]\), let%
\begin{equation*}
S_N(x)=\frac{4}{\pi^2}\sum_{k=0}^N\frac{\left(-1\right)^k}{\left(2k+1\right)^2} \sin\left(\left(2k+1\right)\pi x\right)
\end{equation*}
be the \(N^{th}\) partial sum of the series and use the graphing tool below to view the graph of \(S_N(x)\) for several values of \(N\).%
\begin{figureptx}{Use the Desmos window to explore.}{g:figure:idp83}{}%
\centering
\setlength{\qrsize}{9em}
\setlength{\previewwidth}{\linewidth}
\addtolength{\previewwidth}{-\qrsize}
\begin{tcbraster}[raster columns=2, raster column skip=1pt, raster halign=center, raster force size=false, raster left skip=0pt, raster right skip=0pt]%
\begin{tcolorbox}[previewstyle, width=\previewwidth]%
\IfFileExists{images/interactive-1-preview.png}%
{\includegraphics[width=0.80\linewidth,height=\qrsize,keepaspectratio]{images/interactive-1-preview.png}}%
{\small{}Specify static image with \mono{@preview} attribute,\\Or create and provide automatic screenshot as \mono{images/interactive-1-preview.png} via the \mono{mbx} script}%
\end{tcolorbox}%
\begin{tcolorbox}[qrstyle]%
{\hypersetup{urlcolor=black}\qrcode[height=\qrsize]{interactive-1.html}}%
\end{tcolorbox}%
\begin{tcolorbox}[captionstyle]%
\small \mono{www.desmos.com/calculator/eru5fcn1ct}\end{tcolorbox}%
\end{tcbraster}%
\tcblower
\end{figureptx}%
As you can see, it appears that as we add more terms to the partial sum, \(S_N\), it looks more and more like the original function \(f(x)=\frac{1}{2}-\abs{x-\frac{1}{2}}\). This would lead us to believe that the series converges to the function and that%
\begin{equation}
f(x)=\frac{4}{\pi^2}\sum_{k=0}^\infty\frac{\left(-1\right)^k}{\left(2k+1\right)^2}\sin\left(\left(2k+1\right)\pi x\right)\text{.}\label{x:men:PDE_sol}
\end{equation}
is a valid representation of \(f\) as a Fourier series.%
\par
Recall, that when we represented a function as a power series, we freely differentiated and integrated the series term by term as though it was a polynomial. Let's do the same with this Fourier series.%
\par
To start, notice that the derivative of%
\begin{equation*}
f(x)=\frac{1}{2}-\abs{x-\frac{1}{2}}
\end{equation*}
is given by%
\begin{equation*}
f^\prime(x) = \begin{cases}1\amp \text{ if } \,\text{ } 0\leq x\lt \frac{1}{2}\\ -1\amp \text{ if } \,\frac{1}{2}\lt x\leq 1 \end{cases} \text{.}
\end{equation*}
%
\par
This derivative does not exist at \(x=\frac{1}{2}\) and its graph is given by%
\begin{image}{0.125}{0.75}{0.125}%
\includegraphics[width=\linewidth]{images/Ch2fig1.png}
\end{image}%
If we differentiate the Fourier series term-by-term, we obtain%
\begin{equation*}
\frac{4}{\pi}\sum_{k=0}^\infty\frac{\left(-1\right)^k}{\left(2k+1\right)} \cos\left(\left(2k+1\right)\pi x\right)\text{.}
\end{equation*}
%
\par
Again, if we let \(C_N(x)=\frac{4}{\pi}\sum_{k=0}^N\frac{\left(-1\right)^k}{\left(2k+1\right)} \cos\left(\left(2k+1\right)\pi x\right)\) be the \(N^{th}\) partial sum of this Fourier cosine series and plot \(C_N(x)\) for \(N=1,2,5,50\), we obtain%
\begin{sidebyside}{2}{0.025}{0.025}{0.05}%
\begin{sbspanel}{0.45}[center]%
\includegraphics[width=\linewidth]{images/FourierEx6.png}
\end{sbspanel}%
\begin{sbspanel}{0.45}[center]%
\includegraphics[width=\linewidth]{images/FourierEx7.png}
\end{sbspanel}%
\end{sidebyside}%
\begin{sidebyside}{2}{0.025}{0.025}{0.05}%
\begin{sbspanel}{0.45}[center]%
\includegraphics[width=\linewidth]{images/FourierEx8.png}
\end{sbspanel}%
\begin{sbspanel}{0.45}[center]%
\includegraphics[width=\linewidth]{images/FourierEx9.png}
\end{sbspanel}%
\end{sidebyside}%
In fact, if we were to graph the series \(\frac{4}{\pi}\sum_{k=0}^\infty\frac{\left(-1\right)^k}{\left(2k+1\right)}\) cos\(\left(\left(2k+1\right)\pi x\right)\), we would obtain%
\begin{sidebyside}{1}{0.05}{0.05}{0}%
\begin{sbspanel}{0.9}[center]%
\includegraphics[width=\linewidth]{images/FourierEx10.png}
\end{sbspanel}%
\end{sidebyside}%
\par
Notice that this agrees with the graph of \(f^\prime\), except that \(f^\prime\) didn't exist at \(x=\frac{1}{2}\), and this series takes on the value \(0\) at \(x=\frac{1}{2}\). Notice also, that every partial sum of this series is continuous, since it is a finite combination of continuous cosine functions. This agrees with what you learned in calculus, the (finite) sum of continuous functions is always continuous. In the 1700's, this was also assumed to be true for infinite series, because every time a power series converged to a function, that function happened to be continuous. This never failed for power series, so this example was a bit disconcerting as it is an example of the sum of \emph{infinitely many} continuous functions which is, in this case, discontinuous. Was it possible that there was some power series which converged to a function which was not continuous? Even if there wasn't, what was the difference between power series and this Fourier series?%
\par
Even more disconcerting is what happens if we try differentiating the series%
\begin{equation*}
\frac{4}{\pi}\sum_{k=0}^\infty\frac{\left(-1\right)^k}{\left(2k+1\right)} \cos\left(\left(2k+1\right)\pi x\right)
\end{equation*}
term-by-term. Given the above graph of this series, the derivative of it should be constantly 0, except at \(x=\frac{1}{2}\), where the derivative wouldn't exist. Using the old adage that the derivative of a sum is the sum of the derivatives, we differentiate this series term-by-term to obtain the series%
\begin{equation*}
4\sum_{k=0}^\infty\left(-1\right)^{k+1}\sin\left(\left(2k+1\right)\pi x\right)\text{.}
\end{equation*}
%
\par
If we sum the first forty terms of this series, we get%
\begin{image}{0.125}{0.75}{0.125}%
\includegraphics[width=\linewidth]{images/FourierEx11.png}
\end{image}%
We knew that there might be a problem at \(x=\frac{1}{2}\) but this is crazy! The series seems to not be converging to zero at all!%
\begin{problem}{}{x:problem:prob_FourierDiverge}%
\index{Fourier Series}\index{Fourier Series!divergent Fourier series example} Show that when \(x=\frac{1}{4}\)%
\begin{equation*}
4\sum_{k=0}^\infty\left(-1\right)^{k+1} \sin\left(\left(2k+1\right)\pi x\right)=4\left(-\frac{1}{\sqrt{2}}+\frac{1}{\sqrt{2}}+\frac{1}{\sqrt{2}}- \frac{1}{\sqrt{2}}-\frac{1}{\sqrt{2}}+\cdots\right)\text{.}
\end{equation*}
%
\end{problem}
\hyperref[x:problem:prob_FourierDiverge]{Problem~{\xreffont\ref{x:problem:prob_FourierDiverge}}} shows that when we differentiate the series%
\begin{equation*}
\frac{4}{\pi}\sum_{k=0}^\infty\frac{\left(-1\right)^k}{\left(2k+1\right)} \cos\left(\left(2k+1\right)\pi x\right)
\end{equation*}
term by term, this differentiated series doesn't converge to anything at \(x=\frac{1}{4}\), let alone converge to zero. In this case, the old calculus rule that the derivative of a sum is the sum of the derivatives does not apply for this infinite sum, though it did apply before. As if the continuity issue wasn't bad enough before, this was even worse. Power series were routinely differentiated and integrated term-by-term. This was part of their appeal. They were treated like ``infinite polynomials.'' Either there is some power series lurking that refuses to behave nicely, or there is some property that power series have that not all Fourier series have.%
\par
Could it be that everything we did in \hyperref[x:chapter:PowerSeriesQuestions]{Chapter~{\xreffont\ref{x:chapter:PowerSeriesQuestions}}} was bogus?%
\par
Fortunately, the answer to that question is no. Power series are generally much more well-behaved than Fourier series. Whenever a power series converges, the function it converges to will be continuous. As long as one stays inside the interval of convergence, power series can be differentiated and integrated term-by-term. Power series have something going for them that your average Fourier series does not. (We need to develop the machinery to know what that something is.) None of this is any more obvious to us than it was to mathematicians at the beginning of the nineteenth century. What they did know was that relying on intuition was perilous and rigorous formulations were needed to either justify or dismiss these intuitions. In some sense, the nineteenth century was the ``morning after'' the mathematical party that went on throughout the eighteenth century.%
\begin{problem}{}{x:problem:prob_Fourier_Series-orthogonality}%
\index{\(\cos (nx)\)!orthogonality of}\index{orthogonality!of \(\cos nx\)} Let \(n\) and \(m\) be positive integers. Show%
\begin{equation*}
\int_{x=0}^1\cos\left(n\pi x\right)\cos\left(m\pi x\right)\dx{ x}=\left\{ \begin{matrix}0\amp \text{ if } n\neq m\\ \frac{1}{2}\amp \text{ if } n=m \end{matrix} \right.\text{.}
\end{equation*}
%
\end{problem}
\begin{problem}{}{x:problem:prob_fouriercoef}%
\index{Fourier Series!computing the coefficients} Use the result of \hyperref[x:problem:prob_Fourier_Series-orthogonality]{Problem~{\xreffont\ref{x:problem:prob_Fourier_Series-orthogonality}}} to show that if%
\begin{equation*}
f(x)=\sum_{n=1}^\infty B_n\cos\left(n\pi x\right)
\end{equation*}
on \([0,1]\), then%
\begin{equation*}
B_m=2\int_{x=0}^1f(x)\cos\left(m\pi x\right)\dx{ x}.{}
\end{equation*}
%
\end{problem}
\begin{problem}{}{g:problem:idp84}%
\index{Fourier Series!cosine series!the Fourier cosine series of \(f(x)=x-\frac{1}{2}\)} Apply the result of \hyperref[x:problem:prob_fouriercoef]{Problem~{\xreffont\ref{x:problem:prob_fouriercoef}}} to show that the Fourier cosine series of \(f(x)=x-\frac{1}{2}\) on \([0,1]\) is given by%
\begin{equation*}
\frac{-4}{\pi^2}\sum_{k=0}^\infty\frac{1}{\left(2k+1\right)^2}\cos \left((2k+1)\pi x\right)\text{.}
\end{equation*}
%
\par
Let \(C(x,N)=\frac{-4}{\pi^2}\sum_{k=0}^N\frac{1}{\left(2k+1\right)^2}\cos \left((2k+1)\pi x\right)\) and plot \(C(x,N)\) for \(N=1,2,5,50\) \(x\in[\,0,1]\). How does this compare to the function \(f(x)=x-\frac{1}{2}\) on \([\,0,1]\)? What if you plot it for \(x\in[\,0,2]?\)%
\end{problem}
\begin{problem}{}{g:problem:idp85}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Differentiate the series%
\begin{equation*}
\frac{-4}{\pi^2}\sum_{k=0}^\infty\frac{1}{\left(2k+1\right)^2}\cos \left((2k+1)\pi x\right)
\end{equation*}
term by term and plot various partial sums for that series on \([\,0,1]\). How does this compare to the derivative of \(f(x)=x-\frac{1}{2}\) on that interval?%
\item[(b)]Differentiate the series you obtained in part a and plot various partial sums of that on \([\,0,1]\). How does this compare to the second derivative of \(f(x)=x-\frac{1}{2}\) on that interval?%
\end{enumerate}
\end{problem}
\end{sectionptx}
\end{chapterptx}
\end{partptx}
%
%
\typeout{************************************************}
\typeout{Part III In Which We Find (Some) Answers}
\typeout{************************************************}
%
\begin{partptx}{In Which We Find (Some) Answers}{}{In Which We Find (Some) Answers}{}{}{x:part:FindingAnswers}
 %
%
\typeout{************************************************}
\typeout{Chapter 7 Convergence of Sequences and Series}
\typeout{************************************************}
%
\begin{chapterptx}{Convergence of Sequences and Series}{}{Convergence of Sequences and Series}{}{}{x:chapter:Convergence}
%
%
\typeout{************************************************}
\typeout{Section 7.1 Sequences of Real Numbers}
\typeout{************************************************}
%
\begin{sectionptx}{Sequences of Real Numbers}{}{Sequences of Real Numbers}{}{}{x:section:SeqRealNum}
In \hyperref[x:chapter:CalcIn17th18thCentury]{Chapter~{\xreffont\ref{x:chapter:CalcIn17th18thCentury}}}, we developed the equation \(1+x+x^2+x^3+\cdots=\frac{1}{1-x}\), and we mentioned there were limitations to this power series representation.  For example, substituting \(x=1\) and \(x=-1\) into this expression leads to%
\begin{equation*}
1+1+1+\cdots=\frac{1}{0} \text{ and }  1-1+1-1+\cdots=\frac{1}{2}\
\end{equation*}
which are rather hard to accept.  On the other hand, if we substitute \(x=\frac{1}{2}\) into the expression we get \(1+\frac{1}{2}+\left(\frac{1}{2}\right)^2+\left(\frac{1}{2}\right)^3+\cdots=2\) which seems more palatable until we think about it.  We can add two numbers together by the method we all learned in elementary school.  Or three.  Or any finite set of numbers, at least in principle.  But infinitely many?  What does that even mean? Before we can add infinitely many numbers together we must find a way to give meaning to the idea.%
\par
To do this, we examine an infinite sum by thinking of it as a sequence of finite partial sums. In our example, we would have the following sequence of partial sums.%
\begin{equation*}
\left(1,1+\frac{1}{2},1+\frac{1}{2}+\left(\frac{1}{2}\right)^2,1+ \frac{1}{2}+\left(\frac{1}{2}\right)^3,\ldots,\sum_{j=0}^n\left(\frac{1}{2} \right)^j,\ldots\right)\text{.}
\end{equation*}
%
\par
We can plot these sums on a number line to see what they tend toward as \(n\) gets large.%
\begin{image}{0.08}{0.84}{0.08}%
\includegraphics[width=\linewidth]{images/NumberLine.png}
\end{image}%
Since each partial sum is located at the midpoint between the previous partial sum and \(2\), it is reasonable to suppose that these sums tend to the number 2. Indeed, you probably have seen an expression such as \(\lim_{n\rightarrow\infty}\) \(\left(\sum_{j=0}^n\left(\frac{1}{2}\right)^j\right)=2\) justified by a similar argument. Of course, the reliance on such pictures and words is fine if we are satisfied with intuition. However, we must be able to make these intuitions rigorous without relying on pictures or nebulous words such as ``approaches.''%
\par
No doubt you are wondering ``What's wrong with the word `approaches'? It seems clear enough to me.'' This is often a sticking point. But if we think carefully about what we mean by the word ``approach'' we see that there is an implicit assumption that will cause us some difficulties later if we don't expose it.%
\par
To see this consider the sequence \(\left(1,\frac12,\frac13,\frac14,\ldots\right)\). Clearly it ``approaches'' zero, right? But, doesn't it also ``approach'' \(-1?\) It does, in the sense that each term gets closer to \(-1\) than the one previous. But it also ``approaches'' \(-2\), \(-3\), or even \(-1000\) in the same sense. That's the problem with the word ``approaches.'' It just says that we're getting closer to something than we were in the previous step. It does \emph{not} tell us that we are actually getting close. Since the moon moves in an elliptical orbit about the earth for part of each month it is ``approaching'' the earth. The moon gets clos\emph{er} to the earth but, thankfully, it does not get \emph{close} to the earth. The implicit assumption we alluded to earlier is this: When we say that the sequence \(\left(\frac1n\right)_{n=1}^\infty\) ``approaches'' zero we mean that it is getting \emph{close} not clos\emph{er}. Ordinarily this kind of vagueness in our language is pretty innocuous. When we say ``approaches'' in casual conversation we can usually tell from the context of the conversation whether we mean ``getting close to'' or ``getting closer to.'' But when speaking mathematically we need to be more careful, more explicit, in the language we use.%
\par
So how can we change the language we use so that this ambiguity is eliminated? Let's start out by recognizing, rigorously, what we mean when we say that a sequence converges to zero. For example, you would probably want to say that the sequence \(\left(1,\frac{1}{2},\frac{1}{3},\frac{1}{4},\,\ldots\right)=\left( \frac{1}{n}\right)_{n=1}^\infty\) converges to zero. Is there a way to give this meaning without relying on pictures or intuition?%
\par
One way would be to say that we can make \(\frac{1}{n}\) as close to zero as we wish, provided we make \(n\) large enough. But even this needs to be made more specific. For example, we can get \(\frac{1}{n}\) to within a distance of \(.1\) of \(0\) provided we make \(n>10\), we can get \(\frac{1}{n}\) to within a distance of \(.01\) of \(0\) provided we make \(n>100\), etc. After a few such examples it is apparent that given any arbitrary distance \(\eps>0\), we can get \(\frac{1}{n}\) to within \(\eps\) of \(0\) provided we make \(n>\frac{1}{\eps}\). This leads to the following definition.%
\begin{definition}{}{x:definition:def_ConvergeToZero}%
\index{sequences!convergence to zero} Let \(\left(s_n\right)=\left(s_1,s_2,s_3,\ldots\right)\) be a sequence of real numbers. We say that \(\left(\boldsymbol{s}_{\boldsymbol{n}}\right)\) \terminology{converges to 0} and write \(\lim_{n\rightarrow\infty}s_n=0\) provided for any \(\eps>0\), there is a real number \(N\) such that if \(n>N\), then \(|s_n|\lt \eps\).%
\end{definition}
\terminology{Notes on \hyperref[x:definition:def_ConvergeToZero]{Definition~{\xreffont\ref{x:definition:def_ConvergeToZero}}}}:%
\begin{enumerate}
\item{}This definition is the formal version of the idea we just talked about; that is, given an arbitrary distance \(\eps\), we must be able to find a specific number \(N\) such that \(s_n\) is within \(\eps\) of \(0\), whenever \(n>N\). The \(N\) is the answer to the question of how large is ``large enough'' to put \(s_n\) this close to \(0\).%
\item{}Even though we didn't need it in the example \(\left(\frac{1}{n}\right)\), the absolute value appears in the definition because we need to make the \emph{distance} from \(s_n\) to 0 smaller than \(\eps\). Without the absolute value in the definition, we would be able to ``prove'' such outrageous statements as \(\lim_{n\rightarrow\infty}-n=0\), which we obviously don't want.%
\item{}The statement \(|s_n|\lt \eps\) can also be written as \(-\eps\lt s_n\lt \eps\) or \(s_n\in\left(-\eps,\eps\right)\). (See the \hyperref[x:problem:prob_absolute_value]{Problem~{\xreffont\ref{x:problem:prob_absolute_value}}} below.) Any one of these equivalent formulations can be used to prove convergence. Depending on the application, one of these may be more advantageous to use than the others.%
\item{}Any time an \(N\) can be found that works for a particular \(\eps\), any number \(M>N\) will work for that \(\eps\) as well, since if \(n>M\) then \(n>N\).%
\end{enumerate}
%
\begin{problem}{}{x:problem:prob_absolute_value}%
\index{absolute value} Let \(a\) and \(b\) be real numbers with \(b>0\). Prove \(|a|\lt b\) if and only if \(-b\lt a\lt b\). Notice that this can be extended to \(|a|\leq b\) if and only if \(-b\leq a\leq b\).%
\end{problem}
To illustrate how this definition makes the above ideas rigorous, let's use it to prove that \(\displaystyle\lim_{n\rightarrow\infty}\frac{1}{n}=0\).%
\begin{proof}{}{g:proof:idp86}
Let \(\eps>0\) be given. Let \(N=\frac{1}{\eps}\). If \(n>N\), then \(n>\frac{1}{\eps}\) and so \(|\frac{1}{n}|=\frac{1}{n}\lt \eps\). Hence by definition, \(\lim_{n\rightarrow\infty}\frac{1}{n}=0\).%
\end{proof}
Notice that this proof is rigorous and makes no reference to vague notions such as ``getting smaller'' or ``approaching infinity.'' It has three components:%
\begin{enumerate}
\item{}provide the challenge of a distance \(\eps>0\),%
\item{}identify a real number \(N\), and%
\item{}show that this \(N\) works for this given \(\eps\).%
\end{enumerate}
There is also no explanation about where \(N\) came from. While it is true that this choice of \(N\) is not surprising in light of the ``scrapwork'' we did before the definition, the motivation for how we got it is not in the formal proof nor is it required.  In fact, such scrapwork is typically not included in a formal proof.  For example, consider the following.%
\begin{example}{}{g:example:idp87}%
Use the definition of convergence to zero to prove%
\begin{equation*}
\lim_{n\rightarrow\infty}\frac{\sin n}{n}=0\text{.}
\end{equation*}
%
\end{example}
\begin{proof}{}{g:proof:idp88}
Let \(\eps>0\). Let \(N=\frac{1}{\eps}\). If \(n>N\), then \(n>\frac{1}{\eps}\) and \(\frac{1}{n}\lt \eps\). Thus \(\abs{\frac{\sin(n)}{n}}\leq\frac{1}{n}\lt \eps\). Hence by definition, \(\lim_{n\rightarrow\infty}\frac{\sin n}{n}=0\).%
\end{proof}
Notice that the \(N\) came out of nowhere, but you can probably see the thought process that went into this choice: We needed to use the inequality \(\abs{\sin n}\leq 1\). Again this scrapwork is not part of the formal proof, but it is typically necessary for finding what \(N\) should be. You might be able to do the next problem without doing any scrapwork first, but don't hesitate to do scrapwork if you need it.%
\begin{problem}{}{g:problem:idp89}%
\index{convergence!of a sequence!convergence to zero drill} Use the definition of convergence to zero to prove the following.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\(\displaystyle\lim_{n\rightarrow\infty}\frac{1}{n^2}=0\)%
\item[(b)]\(\displaystyle\lim_{n\rightarrow\infty}\frac{1}{\sqrt{n}}=0\)%
\end{enumerate}
\end{problem}
As the sequences get more complicated, doing scrapwork ahead of time will become more necessary.%
\begin{example}{}{x:example:sec_defin-conv-sequ}%
Use the definition of convergence to zero to prove%
\begin{equation*}
\lim_{n\rightarrow\infty}\frac{n+4}{n^2+1}=0\text{.}
\end{equation*}
%
\par
\terminology{SCRAPWORK}%
\par
Given an \(\eps>0\), we need to see how large to make \(n\) in order to guarantee that \(|\frac{n+4}{n^2+1}|\lt \eps\). First notice that \(\frac{n+4}{n^2+1}\lt \frac{n+4}{n^2}\). Also, notice that if \(n>4\), then \(n+4\lt n+n=2n\). So as long as \(n>4\), we have \(\frac{n+4}{n^2+1}\lt \frac{n+4}{n^2}\lt \frac{2n}{n^2}=\frac{2}{n}\). We can make this less than \(\eps\) if we make \(n>\frac{2}{\eps}\). This means we need to make \(n>4\) and \(n>\frac{2}{\eps}\), simultaneously. These can be done if we let \(N\) be the maximum of these two numbers. This sort of thing comes up regularly, so the notation \(N=\max\left(4,\frac{2}{\eps}\right)\) was developed to mean the maximum of these two numbers. Notice that if \(N=\max\left(4, \frac{2}{\eps}\right)\) then \(N\geq 4\) and \(N\geq\frac{2}{\eps}\). We're now ready for the formal proof.%
\end{example}
\begin{proof}{}{g:proof:idp90}
Let \(\eps>0\). Let \(N=\max\left(4,\frac{2}{\eps}\right)\). If \(n>N\), then \(n>4\) and \(n>\frac{2}{\eps}\). Thus we have \(n>4\) and \(\frac{2}{n}\lt \eps\). Therefore%
\begin{equation*}
\abs{\frac{n+4}{n^2+1}}=\frac{n+4}{n^2+1}\lt \frac{n+4}{n^2}\lt \frac{2n}{n^2}= \frac{2}{n}\lt \eps\text{.}
\end{equation*}
%
\par
Hence by definition, \(\displaystyle\lim_{n\rightarrow\infty}\frac{n+4}{n^2+1}=0\).%
\end{proof}
Again we emphasize that the scrapwork is not \alert{explicitly} a part of the formal proof.  However, if you look carefully, you can always find the scrapwork in the formal proof.%
\begin{problem}{}{g:problem:idp91}%
\index{convergence!of a sequence!convergence to zero drill} Use the definition of convergence to zero to prove%
\begin{equation*}
\lim_{n\rightarrow\infty}\frac{n^2+4n+1}{n^3}=0.{}
\end{equation*}
%
\end{problem}
\begin{problem}{}{x:problem:prob_sequences3}%
Let \(b\) be a nonzero real number with \(|b|\lt 1\) and let \(\eps>0\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Solve the inequality \(|b|^n\lt \eps\) for \(n\)%
\item[(b)]Use part (a) to prove \(\lim_{n\rightarrow\infty}b^n=0\).%
\end{enumerate}
\end{problem}
We can negate this definition to prove that a particular sequence does not converge to zero.%
\begin{example}{}{x:example:ex_zero-two-not-converge}%
Use the definition to prove that the sequence%
\begin{equation*}
\left(1+(-1)^n\right)_{n=0}^\infty=(2,0,2,0,2,\ldots)
\end{equation*}
does not converge to zero.%
\end{example}
Before we provide this proof, let's analyze what it means for a sequence \(\left(s_n\right)\) to \emph{not} converge to zero.  Converging to zero means that any time a distance \(\eps>0\) is given, we must be able to respond with a number \(N\) such that \(|s_n|\lt \eps\) for every \(n>N\).  To have this not happen, we must be able to find some \(\eps>0\) such that no choice of \(N\) will work.  Of course, if we find such an \(\eps\), then any smaller one will fail to have such an \(N\), but we only need one to mess us up.  If you stare at the example long enough, you see that any \(\eps\) with \(0\lt
\eps\leq 2\) will cause problems.  For our purposes, we will let \(\eps=2\).%
\begin{proof}{}{g:proof:idp92}
Let \(\eps=2\) and let \(N\in\NN\) be any integer. If we let \(k\) be any non-negative integer with \(k>\frac{N}{2}\), then \(n=2k>N\), but \(|1+(-1)^n|=2\). Thus no choice of \(N\) will satisfy the conditions of the definition for this \(\eps\), (namely that \(|1+(-1)^n|\lt 2\) for all \(n>N\)) and so \(\lim_{n\rightarrow\infty}\left(1+(-1)^n\right)\neq 0\).%
\end{proof}
\begin{problem}{}{x:problem:prob_sequences-not_converge_to_zero}%
\index{limit!definition of non-existence} Negate the definition of \(\lim_{n\rightarrow\infty}s_n=0\) to provide a formal definition for \(\lim_{n\rightarrow\infty}s_n\neq 0\).%
\end{problem}
\begin{problem}{}{x:problem:prob_sequences4}%
Use the definition to prove \(\lim_{n\rightarrow\infty}\frac{n}{n+100}\neq 0\).%
\end{problem}
Now that we have a handle on how to rigorously prove that a sequence converges to zero, let's generalize this to a formal definition for a sequence converging to something else. Basically, we want to say that a sequence \(\left(s_n\right)\) converges to a real number \(s\), provided the difference \(\left(s_n-s\right)\) converges to zero. This leads to the following definition:%
\begin{definition}{}{x:definition:def_ConvergenceOfASequence}%
\index{sequences!convergence}\index{convergence!of a sequence} Let \(\left(s_n\right)=\left(s_1,s_2,s_3,\ldots\right)\) be a sequence of real numbers and let \(s\) be a real number. We say that \(\left(\boldsymbol{s}_{\boldsymbol{n}}\right)\) \textbraceleft{}converges to\textbraceright{} \(\boldsymbol{s}\) and write \(\lim_{n\rightarrow\infty}s_n=s\) provided for any \(\eps>0\), there is a real number \(N\) such that if \(n>N\), then \(|s_n-s|\lt \eps\).%
\end{definition}
\textbraceleft{}Notes on \hyperref[x:definition:def_ConvergenceOfASequence]{Definition~{\xreffont\ref{x:definition:def_ConvergenceOfASequence}}}\textbraceright{}%
\par
%
\begin{enumerate}
\item{}Clearly%
\begin{equation*}
\lim_{n\rightarrow\infty}s_n=s \text{ if and only if } \lim_{n\rightarrow\infty}\left(s_n-s\right)=0\text{.}
\end{equation*}
%
\item{}Again notice that this says that we can make \(s_n\) as close to \(s\) as we wish (within \(\eps\)) by making \(n\) large enough (\(>N)\). As before, this definition makes these notions very specific.%
\item{}Notice that \(|s_n-s|\lt \eps\) can be written in the following equivalent forms%
\par
%
\begin{enumerate}
\item{}\(\displaystyle |s_n-s|\lt \eps\)%
\item{}\(\displaystyle -\eps\lt s_n-s\lt \eps\)%
\item{}\(\displaystyle s-\eps\lt s_n\lt s+\eps\)%
\item{}\(\displaystyle s_n\in\left(s-\eps,s+\eps\right)\)%
\end{enumerate}
%
\par
and we are free to use any one of these which is convenient at the time.%
\end{enumerate}
%
\par
As an example, let's use this definition to prove that the sequence in \hyperref[x:problem:prob_sequences4]{Problem~{\xreffont\ref{x:problem:prob_sequences4}}}, in fact, converges to 1.%
\begin{example}{}{x:example:example_SeriesConverge}%
Prove \(\displaystyle\lim_{n\rightarrow\infty}\frac{n}{n+100}=1\).%
\par
\terminology{SCRAPWORK}%
\par
Given an \(\eps>0\), we need to get \(\abs{\frac{n}{n+100}-1}\lt \eps\). This prompts us to do some algebra.%
\begin{equation*}
\left|\frac{n}{n+100}-1\right|=\left|\frac{n-(n+100)}{n+100}\right|\leq\frac{100}{n}\text{.}
\end{equation*}
%
\par
This in turn, seems to suggest that \(N=\frac{100}{\eps}\) should work.%
\end{example}
\begin{proof}{}{g:proof:idp93}
Let \(\eps>0\). Let \(N=\frac{100}{\eps}\). If \(n>N\), then \(n>\frac{100}{\eps}\) and so \(\frac{100}{n}\lt \eps\). Hence%
\begin{equation*}
\left|\frac{n}{n+100}-1\right|=\left|\frac{n-(n+100)}{n+100}\right|= \frac{100}{n+100}\lt \frac{100}{n}\lt \eps\text{.}
\end{equation*}
%
\par
Thus by definition \(\lim_{n\rightarrow\infty}\frac{n}{n+100} =1\).%
\end{proof}
Notice again that the scrapwork is not part of the formal proof and the author of a proof is not obligated to tell where the choice of \(N\) came from (although the thought process can usually be seen in the formal proof). The formal proof contains only the requisite three parts: provide the challenge of an arbitrary \(\eps>0\), provide a specific \(N\), and show that this \(N\) works for the given \(\eps\).%
\par
Also notice that given a specific sequence such as \(\left(\frac{n}{n+100}\right)\), the definition does not indicate what the limit would be if, in fact, it exists. Once an educated guess is made as to what the limit should be, the definition only verifies that this intuition is correct.%
\par
This leads to the following question: If intuition is needed to determine what a limit of a sequence should be, then what is the purpose of this relatively non-intuitive, complicated definition?%
\par
Remember that when these rigorous formulations were developed, intuitive notions of convergence were already in place and had been used with great success. This definition was developed to address the foundational issues. Could our intuitions be verified in a concrete fashion that was above reproach? This was the purpose of this non-intuitive definition. It was to be used to verify that our intuition was, in fact, correct and do so in a very prescribed manner. For example, if \(b>0\) is a fixed number, then you would probably say as \(n\) approaches infinity, \(b^{\left(\frac{1}{n}\right)}\) approaches \(b^0=1\). After all, we did already prove that \(\lim_{n\rightarrow\infty}\frac{1}{n}=0\). We should be able to back up this intuition with our rigorous definition.%
\begin{problem}{}{g:problem:idp94}%
\index{limit!\(\lim_{n\rightarrow\infty}b^{\left(\frac{1}{n}\right)}=1\) if \(b>0\)} Let \(b>0\). Use the definition to prove \(\limit{n}{\infty}{b^{\left(\frac{1}{n}\right)}}=1\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp95}{}\quad{}You will probably need to separate this into two cases: \(0\lt b\lt 1\) and \(b\geq 1\).%
\end{problem}
\begin{problem}{}{g:problem:idp96}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Provide a rigorous definition for \(\limit{n}{\infty}{s_n}\neq s\).%
\item[(b)]Use your definition to show that for any real number \(a\), \(\limit{n}{\infty}{\left(\left(-1\right)^n\right)}\neq a\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp97}{}\quad{}Choose \(\eps=1\) and use the fact that \(\Big|a-(-1)^n\Big|\lt 1\) is equivalent to \(\left(-1\right)^n-1\lt a\lt \left(-1\right)^n+1\) to show that no choice of \(N\) will work for this \(\eps\).%
\end{enumerate}
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 7.2 The Limit as a Primary Tool}
\typeout{************************************************}
%
\begin{sectionptx}{The Limit as a Primary Tool}{}{The Limit as a Primary Tool}{}{}{x:section:LimitAsPrimary}
As you've seen from the previous sections, the formal definition of the convergence of a sequence is meant to capture rigorously our intuitive understanding of convergence. However, the definition itself is an unwieldy tool. If only there was a way to be rigorous without having to run back to the definition each time. Fortunately, there is a way. If we can use the definition to prove some general rules about limits then we could use these rules whenever they applied and be assured that everything was still rigorous. A number of these should look familiar from calculus.%
\begin{problem}{}{g:problem:idp98}%
\index{limit!of a constant sequence}\index{sequences!constant sequences} Let \(\left(c\right)_{n=1}^\infty=(c,c,c,\ldots)\) be a constant sequence. Show that \(\lim_{n\rightarrow\infty}c=c\).%
\end{problem}
In proving the familiar limit theorems, the following will prove to be a very useful tool.%
\begin{lemma}{}{}{x:lemma:Tri-RevTri-Ineq}%
\index{Triangle Inequalities}%
%
\begin{description}
\item[{Triangle Inequality:}]\hypertarget{x:li:TriangleIneq}{}\index{Triangle Inequalities!Triangle Inequalitiy}Let \(a\) and \(b\) be real numbers. Then%
\begin{equation*}
|a+b|\leq|a|+|b|\text{.}
\end{equation*}
%
\item[{Reverse Triangle Inequality:}]\hypertarget{x:li:InvTriangleIneq}{}\index{Triangle Inequalities!Inverse Triangle Inequalitiy}Let \(a\) and \(b\) be real numbers. Then%
\begin{equation*}
|a|-|b|\leq\abs{a-b} {}
\end{equation*}
%
\end{description}
%
\end{lemma}
\begin{problem}{}{x:problem:Tri-RevTri-Ineq_prob}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Prove \hyperref[x:lemma:Tri-RevTri-Ineq]{Lemma~{\xreffont\ref{x:lemma:Tri-RevTri-Ineq}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp99}{}\quad{}For the Reverse Triangle Inequality, consider \(|a|=|a-b+b|\).%
\item[(b)]Show \(||a|-|b||\leq|a-b|\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp100}{}\quad{}You want to show \(|a|-|b|\leq|a-b|\) and \(-(|a|-|b|)\leq|a-b|\).%
\end{enumerate}
\end{problem}
\begin{theorem}{}{}{x:theorem:thm_SumOfSequences}%
\index{limit!termwise sums of} If \(\displaystyle\lim_{n\rightarrow\infty}a_n=a\) and \(\displaystyle\lim_{n\rightarrow\infty}b_n=b\), then \(\displaystyle\lim_{n\rightarrow\infty}\left(a_n+b_n\right)=a+b\).%
\end{theorem}
We will often informally state this theorem as ``the limit of a sum is the sum of the limits.'' However, to be absolutely precise, what it says is that if we already know that two sequences converge, then the sequence formed by summing the corresponding terms of those two sequences will converge and, in fact, converge to the sum of those individual limits. We'll provide the scrapwork for the proof of this and leave the formal write-up as an exercise. Note the use of the triangle inequality in the proof.%
\par
%
\begin{problem}{}{g:problem:idp101}%
\index{sequences!sums of} Prove \hyperref[x:theorem:thm_SumOfSequences]{Theorem~{\xreffont\ref{x:theorem:thm_SumOfSequences}}}.%
\par
\terminology{SCRAPWORK:}%
\par
If we let \(\eps>0\), then we want \(N\) so that if \(n>N\), then \(\big|\left(a_n+b_n\right)-\left(a+b\right)\big|\lt \eps\). We \emph{know} that \(\limit{n}{\infty}{a_n}=a\) and \(\limit{n}{\infty}{b_n}=b\), so we can make \(\big|a_n-a\big|\) and  \(\big|b_n-b\big|\) as small as we wish, provided we make \(n\) large enough. Let's go back to what we want, to see if we can close the gap between what we know and what we want.%
\begin{equation*}
\left|\left(a_n+b_n\right)-\left(a+b\right)\right|=\left|\left(a_n-a\right)+\left(b_n-b\right)\right|\leq\left|a_n-a\right|+\left|b_n-b\right|
\end{equation*}
by the triangle inequality. To make this whole thing less than \(\eps\), it makes sense to make each part less than \(\frac{\eps}{2}\). it makes sense to make each part less than \(\frac{\eps}{2}\). Fortunately, we can do that as the definitions of \(\limit{n}{\infty}{a_n}=a\) and \(\limit{n}{\infty}{b_n}=b\) allow us to make \(\big|a_n-a\big|\) and \(\big|b_n-b\big|\) arbitrarily small. Specifically, since \(\limit{n}{\infty}{a_n}=a\), there exists an \(N_1\) such that if \(n>N_1\) then \(\big|a_n-a\big|\lt \frac{\eps}{2}\). Also since \(\limit{n}{\infty}{b_n}=b\), there exists an \(N_2\) such that if \(n>N_2\) then \(\big|b_n-b\big|\lt \frac{\eps}{2}\). Since we want both of these to occur, it makes sense to let \(N=\)max\(\left(N_1,N_2\right)\). This should be the \(N\) that we seek.%
\end{problem}
\begin{theorem}{}{}{x:theorem:thm_LimitOfProduct}%
\index{limit!products of} If \(\displaystyle\lim_{n\rightarrow\infty}a_n=a\) and \(\displaystyle\lim_{n\rightarrow\infty}b_n=b\), then \(\displaystyle\lim_{n\rightarrow\infty}\left(a_n b_n\right)=a b\).%
\end{theorem}
\terminology{SCRAPWORK:} Given \(\eps>0\), we want \(N\) so that if \(n>N\), then \(\big|a_n  b_n-a  b\big|\lt \eps\). One of the standard tricks in analysis is to ``uncancel.'' In this case we will subtract and add a convenient term. Normally these would ``cancel out,'' which is why we say that we will uncancel to put them back in. You already saw an example of this in proving the Reverse Triangle Inequality (\hyperref[x:problem:Tri-RevTri-Ineq_prob]{Problem~{\xreffont\ref{x:problem:Tri-RevTri-Ineq_prob}}}). In the present case, consider%
\begin{align*}
\left|a_n  b_n-a  b\right|\amp =\left|a_n  b_n-a_n b+a_n b-a  b\right|\\
\amp \leq\left|a_n  b_n-a_n  b\right|+\left|a_n b-a b\right|\\
\amp =\left|a_n\right|\left|b_n-b\right|+\left|b\right|\left|a_n-a\right|\text{.}
\end{align*}
%
\par
We can make this whole thing less than \(\eps\), provided we make each term in the sum less than \(\frac{\eps}{2}\). We can make \(\big|b\big|\big|a_n-a\big|\lt \frac{\eps}{2}\) if we make \(\big|a_n-a\big|\lt \frac{\eps}{2|b|}\). But wait! What if \(b=0\)? We could handle this as a separate case or we can do the following ``slick trick.'' Notice that we can add one more line to the above string of inequalities: \(\left|a_n\right|\left|b_n-b\right|+\left|b\right|\left|a_n-a\right|\lt \left|a_n \right|\left|b_n-b\right|+\left(\left|b\right|+1\right)\left|a_n-a \right|\). Now we can make \(\left|a_n-a\right|\lt \frac{\eps}{2\left(|b|+1\right)}\) and not worry about dividing by zero.%
\par
Making \(\big|a_n\big|\big|b_n-b\big|\lt \frac{\eps}{2}\) requires a bit more finesse. At first glance, one would be tempted to try and make \(\big|b_n-b\big|\lt \frac{\eps}{2|a_n|}\). Even if we ignore the fact that we could be dividing by zero (which we could handle), we have a bigger problem. According to the definition of \(\lim_{n\rightarrow\infty}b_n=b\), we can make \(\big|b_n-b\big|\) smaller than any given fixed positive number, as long as we make \(n\) large enough (larger than some \(N\) which goes with a given epsilon). Unfortunately, \(\frac{\eps}{2|a_n|}\) is not fixed as it has the variable \(n\) in it; there is no reason to believe that a single \(N\) will work with all of these simultaneously. To handle this impasse, we need the following:%
\begin{lemma}{}{}{x:lemma:lemma_BoundedConvergent}%
\index{sequences!convergence of!convergent sequences are bounded}%
\alert{A Convergent Sequence Is Bounded.}%
\par
If \(\lim_{n\rightarrow\infty}a_n=a\), then there exists \(B>0\) such that \(|a_n|\leq B\) for all \(n\).%
\end{lemma}
\begin{problem}{}{x:problem:prob_BoundedConvergent}%
Prove \hyperref[x:lemma:lemma_BoundedConvergent]{Lemma~{\xreffont\ref{x:lemma:lemma_BoundedConvergent}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp102}{}\quad{}We know that there exists \(N\) such that if \(n>N\), then \(\abs{a_n-a}\lt 1\).  Let \(B=\)max\(\left(\abs{a_1},\abs{a_2},\ldots,\abs{a_{\lceil{N}\rceil}},\abs{a}+1\right)\), where \(\lceil{N}\rceil\) represents the smallest integer greater than or equal to \(N\).  Also, notice that this is not a convergence proof so it is not safe to think of \(N\) as a large number.\footnotemark{}%
\end{problem}
\footnotetext[1]{Actually, this is a dangerous habit to fall into even in convergence proofs.\label{g:fn:idp103}}%
Armed with this bound \(B\), we can add on one more inequality to the above scrapwork to get%
\begin{align*}
\left|a_n\cdot b_n-a\cdot b\right|\amp =\left|a_n\cdot b_n-a_n\cdot b+a_n\cdot b-a\cdot b\right|\\
\amp \leq\left|a_n\cdot b_n-a_n\cdot b\right|+\left|a_n\cdot b-a\cdot b\right|\\
\amp =\left|a_n\right|\left|b_n-b\right|+\left|b\right|\left|a_n-a\right|\\
\amp \lt B\left|b_n-b\right|+\left(\left|b\right|+1\right)\left|a_n-a\right|
\end{align*}
%
\par
At this point, we should be able to make the last line of this less than \(\eps\).%
\par
\terminology{END OF SCRAPWORK}%
\begin{problem}{}{g:problem:idp104}%
\index{sequences!termwise product of} Prove \hyperref[x:theorem:thm_LimitOfProduct]{Theorem~{\xreffont\ref{x:theorem:thm_LimitOfProduct}}}.%
\end{problem}
\begin{corollary}{}{}{x:corollary:cor_1}%
(Corollary to \hyperref[x:theorem:thm_LimitOfProduct]{Theorem~{\xreffont\ref{x:theorem:thm_LimitOfProduct}}}) If \(\displaystyle\lim_{n\rightarrow\infty}a_n=a\) and \(c\in\mathbb{R}\), then \(\displaystyle\lim_{n\rightarrow\infty}c\cdot a_n=c\cdot a\).%
\end{corollary}
\begin{problem}{}{g:problem:idp105}%
\index{sequences!constant multiples of}\index{limit!of a constant times a sequence} Prove  \hyperref[x:corollary:cor_1]{Corollary~{\xreffont\ref{x:corollary:cor_1}}}.%
\end{problem}
Just as \hyperref[x:theorem:thm_LimitOfProduct]{Theorem~{\xreffont\ref{x:theorem:thm_LimitOfProduct}}} says that the limit of a product is the product of the limits, we can prove the analogue for quotients.%
\begin{theorem}{}{}{x:theorem:thm_LimitOfQuotient}%
\index{limit!quotients of} Suppose \(\displaystyle\lim_{n\rightarrow\infty}a_n=a\) and \(\displaystyle\lim_{n\rightarrow\infty}b_n=b\). Also suppose \(b\neq 0\) and \(b_n\neq 0,\forall\,n\). Then \(\displaystyle\lim_{n\rightarrow\infty}\left(\frac{a_n}{b_n}\right)=\frac{a}{b}\).%
\end{theorem}
\begin{problem}{}{g:problem:idp106}%
\index{sequences!termwise quotient of} Prove \hyperref[x:theorem:thm_LimitOfQuotient]{Theorem~{\xreffont\ref{x:theorem:thm_LimitOfQuotient}}}.%
\par
\terminology{SCRAPWORK}%
\par
To prove this, let's look at the special case of trying to prove \(\lim_{n\rightarrow\infty}\left(\frac{1}{b_n}\right)=\frac{1}{b}\). The general case will follow from this and \hyperref[x:theorem:thm_LimitOfProduct]{Theorem~{\xreffont\ref{x:theorem:thm_LimitOfProduct}}}. Consider \(\big|\frac{1}{b_n}-\frac{1}{b}\big|=\frac{|b-b_n|}{|b_n||b|}\). We are faced with the same dilemma as before; we need to get \(\big|\frac{1}{b_n}\big|\) bounded above. This means we need to get \(|b_n|\) bounded away from zero (at least for large enough \(n\)).%
\par
This can be done as follows. Since \(b\neq 0\), then \(\frac{|b|}{2}>0\). Thus, by the definition of \(\lim_{n\rightarrow\infty}b_n=b\) , there exists \(N_1\) such that if \(n>N_1\), then \(|b\mathopen|-|b_n|\leq\big|b-b_n\big|\lt \frac{|b|}{2}\). Thus when \(n>N_1\), \(\frac{|b|}{2}\lt |b_n|\) and so \(\frac{1}{|b_n|}\lt \frac{2}{|b|}\). This says that for \(n>N_1\), \(\frac{|b-b_n|}{|b_n||b|}\lt \frac{2}{|b|^2}|b-b_n|\). We should be able to make this smaller than a given \(\eps>0\), provided we make \(n\) large enough.%
\end{problem}
These theorems allow us to compute limits of complicated sequences and rigorously verify that these are, in fact, the correct limits without resorting to the definition of a limit.%
\begin{problem}{}{g:problem:idp107}%
\index{identify the theorems used in a limit} Identify all of the theorems implicitly used to show that%
\begin{equation*}
\lim_{n\rightarrow\infty}\frac{3n^3-100n+1}{5n^3+4n^2-7}=\lim_{n \rightarrow\infty}\frac{n^3\left(3-\frac{100}{n^2}+\frac{1}{n^3}\right)}{n^3 \left(5+\frac{4}{n}-\frac{7}{n^3}\right)}=\frac{3}{5}\text{.}
\end{equation*}
%
\par
Notice that this presumes that all of the individual limits exist. This will become evident as the limit is decomposed.%
\end{problem}
There is one more tool that will prove to be valuable.%
\begin{theorem}{}{}{x:theorem:thm_SqueezeTheorem}%
\index{limit!Squeeze Theorem for Sequences}%
\terminology{Squeeze Theorem for Sequences}%
\par
Let \(\left(r_n\right),\left(s_n\right)\), and \(\left(t_n\right)\) be sequences of real numbers with \(r_n\leq s_n\leq t_n,\forall\) positive integers \(n\). Suppose \(\lim_{n\rightarrow\infty}r_n=s=\lim_{n\rightarrow\infty}t_n\). Then \(\left(s_n\right)\) must converge and \(\lim_{n\rightarrow\infty}s_n=s\).%
\end{theorem}
\begin{problem}{}{g:problem:idp108}%
Prove \hyperref[x:theorem:thm_SqueezeTheorem]{Theorem~{\xreffont\ref{x:theorem:thm_SqueezeTheorem}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp109}{}\quad{}This is probably a place where you would want to use \(s-\eps\lt s_n\lt s+\eps\) instead of \(|s_n-s|\lt
\eps\).%
\end{problem}
The Squeeze Theorem holds even if \(r_n\leq s_n\leq t_n\) holds for only sufficiently large \(n\); i.e., for \(n\) larger than some fixed \(N_0\). This is true because when you find an \(N_1\) that works in the original proof, this can be modified by choosing \(N=\)max\(\left(N_0,N_1\right)\). Also note that this theorem really says two things: \(\left(s_n\right)\) converges and it converges to \(s\). This subtle point affects how one should properly use the Squeeze Theorem.%
\begin{example}{}{x:example:example_SqzeEx}%
Prove \(\displaystyle\lim_{n\rightarrow\infty}\frac{n+1}{n^2}=0\).%
\end{example}
\begin{proof}{}{g:proof:idp110}
Notice that \(0\leq\frac{n+1}{n^2}\leq\frac{n+n}{n^2}=\frac{2}{n}\). Since \(\displaystyle\lim_{n\rightarrow\infty}0=0=\lim_{n\rightarrow\infty}\frac{2}{n}\), then by the Squeeze Theorem, \(\displaystyle\lim_{n\rightarrow\infty}\frac{n+1}{n^2}=0\).%
\end{proof}
Notice that this proof is completely rigorous. Also notice that this is the proper way to use the Squeeze Theorem. Here is an example of an \textbraceleft{}improper\textbraceright{} use of the Squeeze Theorem.%
\par
How \emph{not} to prove \hyperref[x:example:example_SqzeEx]{Example~{\xreffont\ref{x:example:example_SqzeEx}}}. Notice that%
\begin{equation*}
0\leq\frac{n+1}{n^2}\leq\frac{n+n}{n^2}=\frac{2}{n}\text{,}
\end{equation*}
so%
\begin{equation*}
0=\lim_{n\rightarrow\infty}0 \leq \lim_{n\rightarrow\infty}\frac{n+1}{n^2}\leq\lim_{n\rightarrow\infty}\frac{2}{n}=0
\end{equation*}
and%
\begin{equation*}
\lim_{n\rightarrow\infty}\frac{n+1}{n^2}=0\text{.}
\end{equation*}
%
\par
This is incorrect in form because it presumes that \(\lim_{n\rightarrow\infty}\frac{n+1}{n^2}\) exists, which we don't yet know. If we knew that the limit existed to begin with, then this would be fine. The Squeeze Theorem proves that the limit does in fact exist, but it must be so stated.%
\par
These general theorems will allow us to rigorously explore convergence of power series in the next chapter without having to appeal directly to the definition of convergence. However, you should remember that we used the definition to prove these results and there will be times when we will need to apply the definition directly. However, before we go into that, let's examine divergence a bit more closely.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 7.3 Divergence}
\typeout{************************************************}
%
\begin{sectionptx}{Divergence}{}{Divergence}{}{}{x:section:DivergentSeq}
In \hyperref[x:theorem:thm_rearrangements]{Theorem~{\xreffont\ref{x:theorem:thm_rearrangements}}} we saw that there is a rearrangment of the alternating Harmonic series which diverges to \(\infty\) or \(-\infty\). In that section we did not fuss over any formal notions of divergence. We assumed instead that you are already familiar with the concept of divergence, probably from taking calculus in the past.%
\par
However we are now in the process of building precise, formal definitions for the concepts we will be using so we define the divergence of a sequence as follows.%
\begin{definition}{}{x:definition:thm_divergence_of_a_sequence}%
\index{sequences!divergence of}\index{divergence!of a sequence} A sequence of real numbers \(\left(s_n\right)_{n=1}^\infty\) diverges if it does not converge to any \(a\in\RR\).%
\end{definition}
It may seem unnecessarily pedantic of us to insist on formally stating such an obvious definition. After all ``converge'' and ``diverge'' are opposites in ordinary English. Why wouldn't they be mathematically opposite too? Why do we have to go to the trouble of formally defining both of them? Since they are opposites defining one implicitly defines the other doesn't it?%
\par
One way to answer that criticism is to state that in mathematics we \emph{always} work from precisely stated definitions and tightly reasoned logical arguments.%
\par
But this is just more pedantry. It is a way of saying, ``Because we said so'' all dressed up in imposing language. We need to do better than that.%
\par
One reason for providing formal definitions of both convergence and divergence is that in mathematics we frequently co-opt words from natural languages like English and imbue them with mathematical meaning that is only tangentially related to the original English definition. When we take two such words which happen to be opposites in English and give them mathematical meanings which are \emph{not} opposites it can be very confusing, especially at first.%
\par
This is what happened with the words ``open'' and ``closed.'' These are opposites in English: ``not open'' is ``closed,'' ``not closed'' is ``open,'' and there is nothing which is both open and closed. But recall that an open interval on the real line, \((a,b)\), is one that does not include \emph{either} of its endpoints while a closed interval, \([a,b]\), is one that includes both of them.%
\par
These may seem like opposites at first but they are not. To see this observe that the interval \((a,b]\) is neither open nor closed since it only contains one of its endpoints.\footnote{It is also true that \((-\infty,\infty)\) is \emph{both} open and closed, but an explanation of this would take us too far afield.\label{g:fn:idp111}} If ``open'' and ``closed'' were mathematically opposite then \emph{every} interval would be either open or closed.%
\par
Mathematicians have learned to be extremely careful about this sort of thing. In the case of convergence and divergence of a series, even though these words are actually opposites mathematically (every sequence either converges or diverges and no sequence converges \emph{and} diverges) it is better to say this explicitly so there can be no confusion.%
\par
A sequence \(\left(a_n\right)_{n=1}^\infty\) can only converge to a real number, \(a\), in one way: by getting arbitrarily close to \(a\). However there are several ways a sequence might diverge.%
\begin{example}{}{g:example:idp112}%
Consider the sequence, \(\left(n\right)_{n=1}^\infty\). This clearly diverges by getting larger and larger \(\ldots\) Ooops! Let's be careful. The sequence \(\left(1-\frac1n\right)_{n=1}^\infty\) gets larger and larger too, but it converges. What we meant to say was that the terms of the sequence \(\left(n\right)_{n=1}^\infty\) become \emph{arbitrarily} large as \(n\) increases.%
\par
This is clearly a divergent sequence but it may not be clear how to prove this formally. Here's one way.%
\par
To show divergence we must show that the sequence satisfies the \emph{negation} of the definition of convergence. That is, we must show that for every \(r\in\RR\) there is an \(\eps>0\) such that for every \(N\in\RR\), there is an \(n>N\) with \(\left|n-r\right|\ge\eps\).%
\par
So let \(\eps=1\), and let \(r\in\RR\) be given. Let \(N=r+2\). Then for every \(n>N\) \(\abs{n-r} > \abs{(r+2)-r}=2>1\). Therefore the sequence diverges.%
\end{example}
This seems to have been rather more work than we should have to do for such a simple problem. Here's another way which highlights this particular type of divergence.%
\par
First we'll need a new definition:%
\begin{definition}{}{x:definition:def_DivToInf}%
\index{sequences!divergence to \(\infty\)}\index{\(\infty\)!positive infinity!divergence to} A sequence, \(\left(a_n\right)_{n=1}^\infty\), \terminology{diverges to positive infinity} if for every real number \(r\), there is a real number \(N\) such that \(n>N\imp a_n>r\).%
\par
\index{\(\infty\)!negative infinity!divergence to} A sequence, \(\left(a_n\right)_{n=1}^\infty\), \terminology{diverges to negative infinity} if for every real number \(r\), there is a real number \(N\) such that \(n>N\imp a_n\lt r\).%
\par
\index{\(\infty\)!divergence to} A sequence is said to \terminology{diverge to infinity} if it diverges to either positive or negative infinity.%
\end{definition}
In practice we want to think of \(\abs{r}\) as a very large number. This definition says that a sequence diverges to infinity if it becomes arbitrarily large as \(n\) increases, and similarly for divergence to negative infinity.%
\begin{problem}{}{g:problem:idp113}%
\index{sequences!the sequence of positive integers diverges to infinity} Show that \(\left(n\right)_{n=1}^\infty\) diverges to infinity.%
\end{problem}
\begin{problem}{}{g:problem:idp114}%
\index{sequences!divergence to \(\infty\)}\index{divergence!divergence to infinity implies divergence} Show that if \(\left(a_n\right)_{n=1}^\infty\) diverges to infinity then \(\left(a_n\right)_{n=1}^\infty\) diverges.%
\end{problem}
We will denote divergence to infinity as%
\begin{equation*}
\limit{n}{\infty}{a_n}=\pm\infty\text{.}
\end{equation*}
%
\par
However, strictly speaking this is an abuse of notation since the symbol \(\infty\) does not represent a real number. This notation can be very problematic since it looks so much like the notation we use to denote convergence: \(\limit{n}{\infty}{a_n}=a\).%
\par
Nevertheless, the notation is appropriate because divergence to infinity is ``nice'' divergence in the sense that it shares many of the properties of convergence, as the next problem shows.%
\begin{problem}{}{g:problem:idp115}%
Suppose \(\limit{n}{\infty}{a_n}=\infty\) and \(\limit{n}{\infty}{b_n}=\infty\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Show that \(\limit{n}{\infty}{a_n+b_n}=\infty\)%
\item[(b)]Show that \(\limit{n}{\infty}{a_nb_n}=\infty\)%
\item[(c)]Is it true that \(\limit{n}{\infty}{\frac{a_n}{b_n}}=\infty?\) Explain.%
\end{enumerate}
\end{problem}
Because divergence to positive or negative infinity shares some of the properties of convergence it is easy to get careless with it.  Remember that even though we write \(\limit{n}{\infty}{a_n}=\infty\) this is still a divergent sequence in the sense that \(\limit{n}{\infty}{a_n}\) does not exist.  The symbol \(\infty\) does not represent a real number.  This is just a convenient notational shorthand telling us that the sequence diverges by becoming arbitrarily large.%
\begin{problem}{}{g:problem:idp116}%
Suppose \(\limit{n}{\infty}{a_n}=\infty\) and \(\limit{n}{\infty}{b_n}=-\infty\) and \(\alpha\in\RR\).  Prove or give a counterexample:%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\(\limit{n}{\infty}{a_n+b_n}=\infty\)%
\item[(b)]\(\limit{n}{\infty}{a_nb_n}=-\infty\)%
\item[(c)]\(\limit{n}{\infty}{\alpha a_n}=\infty\)%
\item[(d)]\(\limit{n}{\infty}{\alpha b_n}=-\infty\)%
\end{enumerate}
\end{problem}
Finally, a sequence can diverge in other ways as the following problem displays.%
\begin{problem}{}{g:problem:idp117}%
Show that each of the following sequences diverge.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\(\left(\left(-1\right)^n\right)_{n=1}^\infty\)%
\item[(b)]\(\left(\left(-1\right)^nn\right)_{n=1}^\infty\)%
\item[(c)]\(a_n = \begin{cases}1\amp \text{ if \(n=2^p\) for some \(p\in\NN\) } \\ \frac1n\amp \text{ otherwise. } \end{cases}\)%
\end{enumerate}
\end{problem}
\begin{problem}{}{g:problem:idp118}%
Suppose that \(\left(a_n\right)_{n=1}^\infty\) diverges but not to infinity and that \(\alpha\) is a real number. What conditions on \(\alpha\) will guarantee that:%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\(\left(\alpha a_n\right)_{n=1}^\infty\) converges?%
\item[(b)]\(\left(\alpha a_n\right)_{n=1}^\infty\) diverges?%
\end{enumerate}
\end{problem}
\begin{problem}{}{g:problem:idp119}%
Show that if \(\abs{r}>1\) then \(\left(r^n\right)_{n=1}^\infty\) diverges. Will it diverge to infinity?%
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 7.4 Additional Problems}
\typeout{************************************************}
%
\begin{sectionptx}{Additional Problems}{}{Additional Problems}{}{}{g:section:idp120}
\begin{problem}{}{g:problem:idp121}%
Prove that if \(\lim_{n\rightarrow\infty}s_n=s\) then \(\lim_{n\rightarrow\infty}|s_n|=|s|\). Prove that the converse is true when \(s=0\), but it is not necessarily true otherwise.%
\end{problem}
\begin{problem}{}{g:problem:idp122}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Let \(\left(s_n\right)\) and \(\left(t_n\right)\) be sequences with \(s_n\leq t_n,\forall n\). Suppose \(\lim_{n\rightarrow\infty}s_n=s\) and \(\lim_{n\rightarrow\infty}t_n=t\).%
\par
Prove \(s\leq t\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp123}{}\quad{}Assume for contradiction, that \(s>t\) and use the definition of convergence with \(\eps=\frac{s-t}{2}\) to produce an \(n\) with \(s_n>t_n\).%
\item[(b)]Prove that if a sequence converges, then its limit is unique.  That is, prove that if \(\lim_{n\rightarrow\infty}s_n=s\) and \(\lim_{n\rightarrow\infty}s_n=t\), then \(s=t\).%
\end{enumerate}
\end{problem}
\begin{problem}{}{g:problem:idp124}%
Prove that if the sequence \(\left(s_n\right)\) is bounded then \(\lim_{n\rightarrow\infty}\left(\frac{s_n}{n}\right)=0\).%
\end{problem}
\begin{problem}{}{x:problem:prob_series-geometric}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Prove that if \(x\neq 1\), then%
\begin{equation*}
1+x+x^2+\cdots+x^n=\frac{1-x^{n+1}}{1-x}\text{.}
\end{equation*}
%
\item[(b)]Use (a) to prove that if \(|x|\lt 1\), then \(\lim_{n\rightarrow\infty}\left(\sum_{j=0}^nx^j\right)=\frac{1}{1-x}\).%
\end{enumerate}
\end{problem}
\begin{problem}{}{g:problem:idp125}%
\index{limit!of ratios of polynomials} Prove%
\begin{equation*}
\lim_{n\rightarrow\infty}\frac{a_0+a_1n+a_2n^2+
\cdots+a_kn^k}{b_0+b_1n+b_2n^2+\cdots+b_kn^k}=\frac{a_k}{b_k}\text{,}
\end{equation*}
provided \(b_k\neq 0\). [Notice that since a polynomial only has finitely many roots, then the denominator will be non-zero when \(n\) is sufficiently large.]%
\end{problem}
\begin{problem}{}{g:problem:idp126}%
Prove that if \(\lim_{n\rightarrow\infty}s_n=s\) and \(\lim_{n\rightarrow\infty}\left(s_n-t_n\right)=0\), then \(\lim_{n\rightarrow\infty}t_n=s\).%
\end{problem}
\begin{problem}{}{g:problem:idp127}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Prove that if \(\lim_{n\rightarrow\infty}s_n=s\) and \(s\lt
t\), then there exists a real number \(N\) such that if \(n>N\) then \(s_n\lt t\).%
\item[(b)]Prove that if \(\lim_{n\rightarrow\infty}s_n=s\) and \(r\lt
s\), then there exists a real number \(M\) such that if \(n>M\) then \(r\lt s_n\).%
\end{enumerate}
\end{problem}
\begin{problem}{}{x:problem:prob_RatioTest}%
Suppose \(\left(s_n\right)\) is a sequence of positive numbers such that%
\begin{equation*}
\lim_{n\rightarrow\infty}\left(\frac{s_{n+1}}{s_n}\right)=L\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Prove that if \(L\lt 1\), then \(\lim_{n\rightarrow\infty}s_n=0\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp128}{}\quad{}Choose \(R\) with \(L\lt R\lt 1\).  By the previous problem, \(\exists\) \(N\) such that if \(n>N\), then \(\frac{s_{n+1}}{s_n}\lt R\).  Let \(n_0>N\) be fixed and show \(s_{n_0+k}\lt R^ks_{n_0}\). Conclude that \(\lim_{k\rightarrow\infty}s_{n_0+k}=0\) and let \(n=n_0+k\).%
\item[(b)]Let \(c\) be a positive real number. Prove \(\displaystyle\lim_{n\rightarrow\infty}\left(\frac{c^n}{n!}\right)=0\).%
\end{enumerate}
\end{problem}
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Chapter 8 Convergence of the Taylor Series: A ``Tayl'' of Three Remainders}
\typeout{************************************************}
%
\begin{chapterptx}{Convergence of the Taylor Series: A ``Tayl'' of Three Remainders}{}{Convergence of the Taylor Series: A ``Tayl'' of Three Remainders}{}{}{x:chapter:TaylorSeries}
%
%
\typeout{************************************************}
\typeout{Section 8.1 The Integral Form of the Remainder}
\typeout{************************************************}
%
\begin{sectionptx}{The Integral Form of the Remainder}{}{The Integral Form of the Remainder}{}{}{x:section:TaylorSeries-IntFormRem}
Now that we have a rigorous definition of the convergence of a sequence, let's apply this to Taylor series. Recall that the Taylor series of a function \(f(x)\) expanded about the point \(a\) is given by%
\begin{equation*}
\sum_{n=0}^\infty\frac{f^{(n)}(a)}{n!}(x-a)^n=f(a)+\frac{f^{\,\prime}(a)}{1!}(x-a)+\frac{f^{\,\prime\prime}(a)}{2!}(x-a)^2+\cdots
\end{equation*}
%
\par
When we say that \(f(x)=\sum_{n=0}^\infty\frac{f^{(n)}(a)}{n!}(x-a)^n\) for a particular value of \(x\), what we mean is that the sequence of partial sums%
\begin{equation*}
\left(\sum_{j=0}^n\frac{f^{(j)}(a)}{j!}(x-a)^j\right)_{n=0}^\infty =\\ \left(f(a),\,f(a)+\frac{f^{\,\prime}(a)}{1!}(x-a),f(a)+\frac{f^{\, \prime}(a)}{1!}(x-a)+\frac{f^{\,\prime\prime}(a)}{2!}(x-a)^2,\ldots\,\right)
\end{equation*}
converges to the number \(f(x)\). Note that the index in the summation was changed to \(j\) to allow \(n\) to represent the index of the sequence of partial sums. As intimidating as this may look, bear in mind that for a fixed real number \(x\), this is still a sequence of real numbers so, that saying \(f(x)=\sum_{n=0}^\infty\frac{f^{(n)}(a)}{n!}(x-a)^n\) means that \(\lim_{n\rightarrow\infty}\left(\sum_{j=0}^n\frac{f^{(j)}(a)}{j!}(x-a)^j\right)=f(x)\) and in the previous chapter we developed some tools to examine this phenomenon. In particular, we know that \(\lim_{n\rightarrow\infty}\left(\sum_{j=0}^n\frac{f^{(j)}(a)}{j!}(x-a)^j\right)=f(x)\) is equivalent to%
\begin{equation*}
\lim_{n\rightarrow\infty}\Biggl[f(x)-\left(\sum_{j=0}^n \frac{f^{(j)}(a)}{j!}(x-a)^j\right)\Biggr]=0\text{.}
\end{equation*}
%
\par
We have seen an example of this already. \hyperref[x:problem:prob_series-geometric]{Problem~{\xreffont\ref{x:problem:prob_series-geometric}}} of the last chapter basically had you show that  the geometric series, \(1+x+x^2+x^3+\cdots\) converges to \(\frac{1}{1-x}\),for \(|x|\lt 1\) by showing that \(\limit{n}{\infty}{\Biggl[\frac{1}{1-x}-\left(\sum_{j=0}^nx^j\right)\Biggr]=0}\).%
\par
There is generally not a readily recognizable closed form for the partial sum for a Taylor series. The geometric series is a special case. Fortunately, for the issue at hand (convergence of a Taylor series), we don't need to analyze the series itself. What we need to show is that the difference between the function and the \(n\)th partial sum converges to zero. This difference is called the \terminology{remainder (of the Taylor series)}. (Why?)%
\par
While it is true that the remainder is simply%
\begin{equation*}
f(x)-\left(\sum_{j=0}^n\frac{f^{(j)}(a)}{j!}(x-a)^j\right)\text{,}
\end{equation*}
this form is not easy to work with. Fortunately, a number of alternate versions of this remainder are available. We will explore these in this chapter.%
\par
Recall the result from \hyperref[x:theorem:TaylorsTheorem]{Theorem~{\xreffont\ref{x:theorem:TaylorsTheorem}}} from \hyperref[x:chapter:PowerSeriesQuestions]{Chapter~{\xreffont\ref{x:chapter:PowerSeriesQuestions}}},%
\begin{align*}
f(x)=f(a)+\frac{f^{\,\prime}(a)}{1!}(x-a)+\frac{f^{\,\prime\prime}(a)}{2!}(x-a)^2\amp +\cdots+\frac{f^{(n)}(a)}{n!}(x-a)^n\\
\amp +\frac{1}{n!}\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}\text{.}
\end{align*}
%
\par
We can use this by rewriting it as%
\begin{equation*}
f(x)-\left(\sum_{j=0}^n\frac{f^{(j)}(a)}{j!}(x-a)^j\right)=\frac{1}{n!}\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}\text{.}
\end{equation*}
%
\par
The expression \(\frac{1}{n!}\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}\) is called the \emph{integral form of the remainder for the Taylor series} of \(f(x)\), and the Taylor series will converge to \(f(x)\) exactly when the sequence \(\lim_{n\rightarrow\infty}\left(\frac{1}{n!}\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}\text{ } \right)\) converges to zero. It turns out that this form of the remainder is often easier to handle than the original \(f(x)-\left(\sum_{j=0}^n\frac{f^{(j)}(a)}{j!}(x-a)^j\right)\) and we can use it to obtain some general results.%
\begin{theorem}{}{}{x:theorem:thm_TaylorSeries}%
\terminology{Taylor's Series}%
\par
\index{Taylor's Series} If there exists a real number \(B\) such that \(|f^{(n+1)}(t)|\leq B\) for all nonnegative integers \(n\) and for all \(t\) on an interval containing \(a\) and \(x\), then%
\begin{equation*}
\lim_{n\rightarrow\infty}\left(\frac{1}{n!}\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}\right)=0
\end{equation*}
and so%
\begin{equation*}
f(x)=\sum_{n=0}^\infty\frac{f^{(n)}(a)}{n!}(x-a)^n.{}
\end{equation*}
%
\end{theorem}
In order to prove this, it might help to first prove the following.%
\begin{lemma}{}{}{x:lemma:lemma_TriangleForIntegral}%
If \(f\) and \(\abs{f}\) are integrable functions and \(a\leq b\), then%
\begin{equation*}
\left|\int_{t=a}^bf(t)\dx{ t}\right|\leq\int_{t=a}^b|f(t)|\dx{ t}. {}
\end{equation*}
%
\end{lemma}
\begin{problem}{}{g:problem:idp129}%
\index{Triangle Inequality!for Integrals} Prove \hyperref[x:lemma:lemma_TriangleForIntegral]{Lemma~{\xreffont\ref{x:lemma:lemma_TriangleForIntegral}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp130}{}\quad{}\(-|f(t)|\leq f(t)\leq|f(t)|\).%
\end{problem}
\begin{problem}{}{g:problem:idp131}%
\index{Taylor's Series!\(f^{(n)}\lt B,\forall\  n\in\NN\imp\) Taylor series converges} Prove \hyperref[x:theorem:thm_TaylorSeries]{Theorem~{\xreffont\ref{x:theorem:thm_TaylorSeries}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp132}{}\quad{}You might want to use \hyperref[x:problem:prob_RatioTest]{Problem~{\xreffont\ref{x:problem:prob_RatioTest}}} of \hyperref[x:chapter:Convergence]{Chapter~{\xreffont\ref{x:chapter:Convergence}}}.  Also there are two cases to consider: \(a\lt x\) and \(x\lt a\) (the case \(x=a\) is trivial).  You will find that this is true in general.  This is why we will often indicate that \(t\) is between \(a\) and \(x\) as in the theorem.  In the case \(x\lt a\), notice that%
\begin{align*}
\left|\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}\right|\amp =\left|(-1)^{n+1}\int_{t=x}^af^{(n+1)}(t)(t-x)^n\dx{ t}\right|\\
\amp =\left|\int_{t=x}^af^{(n+1)}(t)(t-x)^n\dx{ t}\right|.
\end{align*}
%
\end{problem}
\begin{problem}{}{x:problem:prob_Taylor_Series-using}%
Use \hyperref[x:theorem:thm_TaylorSeries]{Theorem~{\xreffont\ref{x:theorem:thm_TaylorSeries}}} to prove that for any real number \(x\)%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\(\displaystyle\sin x=\sum_{n=0}^\infty\frac{(-1)^nx^{2n+1}}{(2n+1)!}\)%
\item[(b)]\(\displaystyle\cos x= \sum_{n=0}^\infty\frac{(-1)^nx^{2n}}{(2n)!}\)%
\item[(c)]\(\displaystyle e^x=\sum_{n=0}^\infty\frac{x^n}{n!}\)%
\end{enumerate}
\end{problem}
\hyperref[x:task:prob_Taylor_Series-using-c]{Problem~{\xreffont\ref{x:problem:prob_Taylor_Series-using}}.{\xreffont\ref{x:task:prob_Taylor_Series-using-c}}} shows that the Taylor series of \(e^x\) expanded at zero converges to \(e^x\) for any real number \(x\). \hyperref[x:theorem:thm_TaylorSeries]{Theorem~{\xreffont\ref{x:theorem:thm_TaylorSeries}}} can be used in a similar fashion to show that%
\begin{equation*}
e^x=\sum_{n=0}^\infty\frac{e^a(x-a)^n}{n!}
\end{equation*}
for any real numbers \(a\) and \(x\).%
\par
Recall that \hyperref[x:chapter:CalcIn17th18thCentury]{Chapter~{\xreffont\ref{x:chapter:CalcIn17th18thCentury}}} we showed that if we define the function \(E(x)\) by the power series \(\sum_{n=0}^\infty\frac{x^n}{n!}\) then \(E(x+y)=E(x)E(y)\). This, of course, is just the familiar addition property of integer coefficients extended to any real number. In \hyperref[x:chapter:CalcIn17th18thCentury]{Chapter~{\xreffont\ref{x:chapter:CalcIn17th18thCentury}}} we had to \emph{assume} that defining \(E(x)\) as a series was meaningful because we did not address the convergence of the series in that chapter. Now that we know the series converges for any real number we see that the definition%
\begin{equation*}
f(x) = e^x = \sum_{n=0}^\infty\frac{x^n}{n!}
\end{equation*}
is in fact valid.%
\par
Assuming that we can differentiate this series term-by-term\footnote{We can, but that proof will have to wait for a few more chapters.\label{g:fn:idp133}} it is straightforward to show that \(f^\prime(x) = f(x)\). Along with Taylor's formula this can then be used to show that \(e^{a+b}=e^ae^b\) more elegantly than the rather cumbersome proof in \hyperref[x:mrow:eq_ExponentAdditionProperty]{equation~({\xreffont\ref{x:mrow:eq_ExponentAdditionProperty}})}, as the following problem shows.%
\begin{problem}{}{g:problem:idp134}%
\index{\(e^x\)!\(e^{a+b}=e^ae^b\)} Recall that if \(f(x)=e^x\) then \(f^\prime(x) = e^x\). Use this along with the Taylor series expansion of \(e^x\) about \(a\) to show that%
\begin{equation*}
e^{a+b}=e^ae^b.
\end{equation*}
%
\end{problem}
\hyperref[x:theorem:thm_TaylorSeries]{Theorem~{\xreffont\ref{x:theorem:thm_TaylorSeries}}} is a nice ``first step'' toward a rigorous theory of the convergence of Taylor series, but it is not applicable in all cases. For example, consider the function \(f(x)=\sqrt{1+x}\). As we saw in \hyperref[x:chapter:CalcIn17th18thCentury]{Chapter~{\xreffont\ref{x:chapter:CalcIn17th18thCentury}}}, \hyperref[x:problem:prob_SqrtSeriesProb]{Problem~{\xreffont\ref{x:problem:prob_SqrtSeriesProb}}}, this function's Maclaurin series (the binomial series for \(\left(1+x\right)^{1/2})\)appears to be converging to the function for \(x\in(-1,1)\). While this is, in fact, true, the above proposition does not apply. If we consider the derivatives of \(f(t)=(1+t)^{1/2}\), we obtain:%
\begin{align*}
f^\prime(t)\amp =\frac{1}{2}(1+t)^{\frac{1}{2}-1}\\
f^{\prime\prime}(t)\amp =\frac{1}{2}\left(\frac{1}{2}-1\right)(1+t)^{\frac{1}{2}-2}\\
f^{\prime\prime\prime}(t)\amp =\frac{1}{2}\left(\frac{1}{2}-1\right)\left(\frac{1}{2}-2 \right)(1+t)^{\frac{1}{2}-3}\\
\amp \vdots\\
f^{(n+1)}(t)\amp =\frac{1}{2}\left(\frac{1}{2}-1\right)\left(\frac{1}{2}-2\right) \cdots\left(\frac{1}{2}-n\right)(1+t)^{\frac{1}{2}-(n+1)}\text{.}
\end{align*}
%
\par
Notice that%
\begin{equation*}
\left|f^{(n+1)}(0)\right|=\frac{1}{2}\left(1-\frac{1}{2}\right)\left(2-\frac{1}{2}\right)\cdots\left(n-\frac{1}{2}\right)\text{.}
\end{equation*}
%
\par
Since this sequence grows without bound as \(n\rightarrow\infty\), then there is no chance for us to find a number \(B\) to act as a bound for all of the derviatives of \(f\) on any interval containing 0 and\(\) \(x\), and so the hypothesis of \hyperref[x:theorem:thm_TaylorSeries]{Theorem~{\xreffont\ref{x:theorem:thm_TaylorSeries}}} will never be satisfied. We need a more delicate argument to prove that%
\begin{equation*}
\sqrt{1+x}=1+\frac{1}{2}x+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)}{2!}x^2+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)\left(\frac{1}{2}-2\right)}{3!}x^3+\cdots
\end{equation*}
is valid for \(x\in(-1,1)\). To accomplish this task, we will need to express the remainder of the Taylor series differently. Fortunately, there are at least two such alternate forms.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 8.2 Lagrange's Form of the Remainder}
\typeout{************************************************}
%
\begin{sectionptx}{Lagrange's Form of the Remainder}{}{Lagrange's Form of the Remainder}{}{}{x:section:TaylorSeries-LagrFormRem}
Joseph-Louis Lagrange \index{Lagrange, Joseph-Louis} provided an alternate form for the remainder in Taylor series in his 1797 work \textbraceleft{}Th\textbackslash{}a'eorie des functions analytiques.\textbraceright{} Lagrange's form of the remainder is as follows.%
\begin{theorem}{}{}{x:theorem:thm_LagrangeRemainder}%
\terminology{Lagrange's Form of the Remainder}%
\par
\index{Lagrange's form of the remainder} Suppose \(f\) is a function such that \(f^{(n+1)}(t)\) is continuous on an interval containing \(a\) and \(x\). Then%
\begin{equation*}
f(x)-\left(\sum_{j=0}^n\frac{f^{(j)}(a)}{j!}(x-a)^j\right)=\frac{f^{\, (n+1)}(c)}{(n+1)!}(x-a)^{n+1}
\end{equation*}
where \(c\) is some number between \(a\) and \(x\).%
\end{theorem}
\begin{proof}{}{g:proof:idp135}
Note first that the result is true when \(x=a\) as both sides reduce to 0 (in that case \(c=x=a\).) We will prove the case where \(a\lt x\); the case \(x\lt a\) will be an exercise.%
\par
First, we already have%
\begin{equation*}
f(x)-\left(\sum_{j=0}^n\frac{f^{(j)}(a)}{j!}(x-a)^j\right)=\frac{1}{n!} \int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}
\end{equation*}
so it suffices to show that%
\begin{equation*}
\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}=\,\frac{f^{\,(n+1)}(c)}{n+1}(x-a)^{n+1}
\end{equation*}
for some \(c\) with \(c\in[\,a,x]\). To this end, let%
\begin{equation*}
M=\max_{a\le t\le x}\left(f^{(n+1)}(t)\right)
\end{equation*}
and%
\begin{equation*}
m=\min_{a\le t\le x}\left(f^{(n+1)}(t)\right)\text{.}
\end{equation*}
%
\par
Note that for all \(t\in[\,a,x]\), we have \(m\leq f^{(n+1)}(t)\leq M\). Since \(x-t\geq 0\), this gives us%
\begin{equation}
m\left(x-t\right)^n\leq f^{(n+1)}(t)(x-t)^n\leq M(x-t)^n\label{x:men:eq_LagRem1}
\end{equation}
and so%
\begin{equation}
\int_{t=a}^xm\left(x-t\right)^n\dx{ t}\leq\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}\leq \int_{t=a}^xM(x-t)^n\dx{ t}\text{.}\label{x:men:eq_LagRem2}
\end{equation}
%
\par
Computing the outside integrals, we have%
\begin{equation*}
m\int_{t=a}^x\left(x-t\right)^n\dx{ t}\leq\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}\leq M\int_{t=a}^x(x-t)^n\dx{ t}
\end{equation*}
%
\begin{equation}
m\frac{(x-a)^{n+1}}{n+1}\leq\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}\leq M\frac{(x-a)^{n+1}}{n+1}\label{x:men:eq_LagRem3}
\end{equation}
%
\begin{equation*}
m\leq\frac{\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}}{\left(\frac{(x-a)^{n+1}}{n+1} \right)}\leq M\text{.}
\end{equation*}
%
\par
Since%
\begin{equation*}
\frac{\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}}{\left(\frac{(x-a)^{n+1}}{n+1} \right)}
\end{equation*}
is a value that lies between the maximum and minimum of \(f^{(n+1)}\) on \([\,a,x]\), then by the Intermediate Value Theorem, there must exist a number \(c\in[\,a,x]\) with%
\begin{equation*}
f^{(n+1)}(c)=\frac{\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}}{\left( \frac{(x-a)^{n+1}}{n+1}\right)}\text{.}
\end{equation*}
%
\par
This gives us%
\begin{equation*}
\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}=\,\frac{f^{\,(n+1)}(c)}{n+1}(x-a)^{n+1}\text{.}
\end{equation*}
%
\par
And the result follows.%
\end{proof}
\begin{problem}{}{g:problem:idp136}%
\index{Lagrange's form of the remainder!\(x\lt a\)} Prove \hyperref[x:theorem:thm_LagrangeRemainder]{Theorem~{\xreffont\ref{x:theorem:thm_LagrangeRemainder}}} for the case where \(x\lt a\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp137}{}\quad{}Note that%
\begin{equation*}
\int_{t=a}^xf^{(n+1)}(t)(x-t)^n\dx{ t}=(-1)^{n+1}\int_{t=x}^af^{(n+1)}(t)(t-x)^n\dx{ t}\text{.}
\end{equation*}
%
\par
Use the same argument on this integral.  It will work out in the end.  Really!  You just need to keep track of \emph{all} of the negatives.%
\end{problem}
This is \emph{not} Lagrange's proof.  He did not use the integral form of the remainder.  However, this is similar to Lagrange's proof in that he also used the Intermediate Value Theorem (IVT) \index{Intermediate Value Theorem (IVT)} and Extreme Value Theorem (EVT) \index{Extreme Value Theorem (EVT)} much as we did.  In Lagrange's day, these were taken to be obviously true for a continuous function and we have followed Lagrange's \index{Lagrange, Joseph-Louis} lead by assuming the IVT and the EVT. However, in mathematics we need to keep our assumptions few and simple.  The IVT and the EVT do not satisfy this need in the sense that both can be proved from simpler ideas.  We will return to this in \hyperref[x:chapter:IVTandEVT]{Chapter~{\xreffont\ref{x:chapter:IVTandEVT}}}.%
\par
Also, a word of caution about this: Lagrange's form of the remainder is \(\frac{f^{\,(n+1)}(c)}{(n+1)!}\) \((x-a)^{n+1}\), where \(c\) is some number between \(a\) and \(x\).  The proof does not indicate what this \(c\) might be and, in fact, this \(c\) changes as \(n\) changes. All we know is that this \(c\) lies between \(a\) and \(x\).  To illustrate this issue and its potential dangers, consider the following problem where we have a chance to compute the value of \(c\) for the function \(f(x)=\frac{1}{1+x}\).%
\begin{problem}{}{g:problem:idp138}%
This problem investigates the Taylor series representation%
\begin{equation*}
\frac{1}{1+x}=1-x+x^2-x^3+\cdots\text{.}
\end{equation*}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Use the fact that \(\frac{1-(-x)^{n+1}}{1+x}=1-x+x^2-x^3+\cdots+(-x)^n\) to compute the remainder \(\frac{1}{1+x}-\left(1-x+x^2-x^3+\cdots+(-x)^n\right)\). Specifically, compute this remainder when \(x=1\) and conclude that the Taylor series does not converge to \(\frac{1}{1+x}\) when \(x=1\).%
\item[(b)]Compare the remainder in part a with the Lagrange form of the remainder to determine what \(c\) is when \(x=1\).%
\item[(c)]Consider the following argument: If \(f(x)=\frac{1}{1+x}\), then%
\begin{equation*}
f^{(n+1)}(c)=\frac{(-1)^{n+1}(n+1)!}{(1+c)^{n+2}}
\end{equation*}
so the Lagrange form of the remainder when \(x=1\) is given by%
\begin{equation*}
\frac{(-1)^{n+1}(n+1)!}{(n+1)!(1+c)^{n+2}}=\frac{(-1)^{n+1}}{(1+c)^{n+2}}
\end{equation*}
where \(c\in[\,0,1]\).  It can be seen in part b that \(c\neq 0\).  Thus \(1+c>1\) and so by \hyperref[x:problem:prob_sequences3]{Problem~{\xreffont\ref{x:problem:prob_sequences3}}} of \hyperref[x:chapter:Convergence]{Chapter~{\xreffont\ref{x:chapter:Convergence}}}, the Lagrange remainder converges to \(0\) as \(n\rightarrow\infty\).  This argument would suggest that the Taylor series converges to \(\frac{1}{1+x}\) for \(x=1\).  However, we know from part (a) that this is incorrect.  What is wrong with the argument?%
\end{enumerate}
\end{problem}
Even though there are potential dangers in misusing the Lagrange form of the remainder, it is a useful form.  For example, armed with the Lagrange form of the remainder, we can prove the following theorem.%
\begin{theorem}{}{}{x:theorem:thm_BinomialSeriesConverges}%
\index{series!Binomial Series, the}%
\index{Binomial Series, the!converges on the interval \([0,1]\)}%
The binomial series%
\begin{equation*}
1+\frac{1}{2}x+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)}{2!}x^2+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)\left(\frac{1}{2}-2\right)}{3!}x^3+\cdots
\end{equation*}
converges to \(\sqrt{1+x}\) for \(x\in[0,1]\).%
\end{theorem}
\begin{proof}{}{g:proof:idp139}
First note that the binomial series is, in fact, the Taylor series for the function \(f(x)=\sqrt{1+x}\) expanded about \(a=0\). If we let \(x\) be a fixed number with \(0\leq x\leq 1\), then it suffices to show that the Lagrange form of the remainder converges to \(0\). With this in mind, notice that%
\begin{equation*}
f^{(n+1)}(t)=\left(\frac{1}{2}\right)\left(\frac{1}{2}-1\right)\cdots\left(\frac{1}{2}-n\right)\left(1+t\right)^{\frac{1}{2}-(n+1)}
\end{equation*}
and so the Lagrange form of the remainder is%
\begin{equation*}
\frac{f^{(n+1)}(c)}{(n+1)!}x^{n+1}= \frac{\left(\frac{1}{2}\right)\left(\frac{1}{2}-1\right)\cdots \left(\frac{1}{2}-n\right)}{(n+1)!}\frac{x^{n+1}}{(1+c)^{n+\frac{1}{2}}}
\end{equation*}
where \(c\) is some number between \(0\) and \(x\). Since \(0\leq x\leq 1\) and \(1+c\geq 1\), then we have \(\frac{1}{1+c}\leq 1\), and so%
\begin{align*}
0\amp \leq \left|\frac{\left(\frac{1}{2}\right)\left(\frac{1}{2}-1\right)\cdots\left(\frac{1}{2}-n\right)}{(n+1)!}\frac{x^{n+1}}{(1+c)^{n+\frac{1}{2}}}\right|\\
\amp =\frac{\left(\frac{1}{2}\right)\left(1-\frac{1}{2}\right)\cdots\left(n-\frac{1}{2}\right)}{(n+1)!}\frac{x^{n+1}}{(1+c)^{n+\frac{1}{2}}}\\
\amp =\frac{\left(\frac{1}{2}\right)\left(\frac{1}{2}\right)\left(\frac{3}{2}\right)\left(\frac{5}{2}\right)\cdots\left(\frac{2n-1}{2}\right)}{(n+1)!}\left(x^{n+1}\right)\frac{1}{(1+c)^{n+\frac{1}{2}}}\\
\amp \leq\frac{1\cdot 1\cdot 3\cdot5\cdot\,\cdots\,\cdot\left(2n-1\right)}{2^{^{n+1}}(n+1)!}\\
\amp =\frac{1\cdot 3\cdot 5\cdot\,\cdots\,\cdot\left(2n-1\right)\cdot 1}{2\cdot4\cdot 6\cdot\,\cdots\,\cdot 2n\cdot\left(2n+2\right)}\\
\amp =\frac{1}{2}\cdot\frac{3}{4}\cdot\frac{5}{6}\cdot\cdots\,\cdot\frac{2n-1}{2n}\cdot\frac{1}{2n+2}\\
\amp \leq\frac{1}{2n+2}\text{.}
\end{align*}
%
\par
Since \(\lim_{n\rightarrow\infty}\frac{1}{2n+2}=0=\,\lim_{n\rightarrow\infty}0\), then by the Squeeze Theorem,%
\begin{equation*}
\lim_{n\rightarrow\infty}\abs{\frac{f^{(n+1)}(c)}{(n+1)!}x^{n+1}}=0, \text{ so } \lim_{n\rightarrow\infty}\left(\frac{f^{(n+1)}(c)}{(n+1)!}x^{n+1}\right)=0\text{.}
\end{equation*}
%
\par
Thus the Taylor series%
\begin{equation*}
1+\frac{1}{2}x+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)}{2!}x^2+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)\left(\frac{1}{2}-2\right)}{3!}x^3+\cdots
\end{equation*}
converges to \(\sqrt{1+x}\) for \(0\leq x\leq 1\).%
\end{proof}
Unfortunately, this proof will not work for \(-1\lt x\lt 0\). In this case, the fact that \(x\leq c\leq 0\) makes\(\,1+c\leq 1\). Thus \(\frac{1}{1+c}\geq 1\) and so the inequality%
\begin{equation*}
\frac{\left(\frac{1}{2}\right)\left(\frac{1}{2}\right)\left(\frac{3}{2}\right)\left(\frac{5}{2}\right)\cdots\left(\frac{2n-1}{2}\right)}{(n+1)!}\frac{|x|^{n+1}}{(1+c)^{n+\frac{1}{2}}}\leq\frac{1\cdot 1\cdot 3\cdot 5\cdot\,\cdots\,\cdot\left(2n-1\right)}{2^{^{n+1}}(n+1)!}
\end{equation*}
may not hold.%
\begin{problem}{}{x:problem:prob_Taylor_Series-Binomial_Series_and}%
\index{series!Binomial Series, the}\index{Binomial Series is a Taylor series} Show that if \(-\frac{1}{2}\leq x\leq c\leq 0\), then \(|\frac{x}{1+c}|\leq 1\) and modify the above proof to show that the binomial series converges to \(\sqrt{1+x}\) for \(x\in\left[-\frac{1}{2},0\right]\).%
\end{problem}
To take care of the case where \(-1\lt x\lt -\frac{1}{2}\), we will use yet another form of the remainder for Taylor series. However before we tackle that, we will use the Lagrange form of the remainder to address something mentioned in \hyperref[x:chapter:PowerSeriesQuestions]{Chapter~{\xreffont\ref{x:chapter:PowerSeriesQuestions}}}. Recall that we noticed that the series representation%
\begin{equation*}
\frac{1}{1+x}=1-x+x^2-x^3+\cdots
\end{equation*}
did not work when \(x=1\), however we noticed that the series obtained by integrating term by term did seem to converge to the antiderivative of \(\frac{1}{1+x}\). Specifically, we have the Taylor series%
\begin{equation*}
\ln\left(1+x\right)=x-\frac{1}{2}x^2+\frac{1}{3}x^3-\cdots\text{.}
\end{equation*}
%
\par
Substituting \(x=1\) into this provided the convergent series \(1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots\). We made the claim that this, in fact, converges to \(\ln 2\), but that this was not obvious. The Lagrange form of the remainder gives us the machinery to prove this.%
\begin{problem}{}{g:problem:idp140}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Compute the Lagrange form of the remainder for the Maclaurin series for \(\ln\left(1+x\right)\).%
\item[(b)]Show that when \(x=1\), the Lagrange form of the remainder converges to \(0\) and so the equation \(\ln 2=1-\frac{1}{2}+\frac{1}{3}-\frac{1}{4}+\cdots\) is actually correct.%
\end{enumerate}
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 8.3 Cauchy's Form of the Remainder}
\typeout{************************************************}
%
\begin{sectionptx}{Cauchy's Form of the Remainder}{}{Cauchy's Form of the Remainder}{}{}{x:section:TaylorSeries-CauchyFormRem}
In his 1823 work, \textit{Rsume des leons donnes  l'ecole royale polytechnique sur le calcul infintsimal,} Augustin Cauchy \index{Cauchy, Augustin} provided another form of the remainder for Taylor series.%
\begin{theorem}{}{}{x:theorem:thm_CauchyRemainder}%
\terminology{Cauchy's Form of the Remainder}%
\par
\index{sequences!Cauchy sequences!Cauchy's remainder} Suppose \(f\) is a function such that \(f^{(n+1)}(t)\) is continuous on an interval containing \(a\) and \(x\). Then%
\begin{equation*}
f(x)-\left(\sum_{j=0}^n\frac{f^{(j)}(a)}{j!}(x-a)^j\right)=\frac{f^{\, (n+1)}(c)}{n!}(x-c)^n(x-a)
\end{equation*}
where \(c\) is some number between \(a\) and \(x\).%
\end{theorem}
\begin{problem}{}{g:problem:idp141}%
\index{Cauchy, Augustin!Cauchy Remainder} Prove \hyperref[x:theorem:thm_CauchyRemainder]{Theorem~{\xreffont\ref{x:theorem:thm_CauchyRemainder}}} using an argument similar to the one used in the proof of \hyperref[x:theorem:thm_LagrangeRemainder]{Theorem~{\xreffont\ref{x:theorem:thm_LagrangeRemainder}}}. Don't forget there are two cases to consider.%
\end{problem}
Using Cauchy's form of the remainder, we can prove that the binomial series%
\begin{equation*}
1+\frac{1}{2}x+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)}{2!}x^2+\frac{\frac{1}{2}\left(\frac{1}{2}-1\right)\left(\frac{1}{2}-2\right)}{3!}x^3+\cdots
\end{equation*}
converges to \(\sqrt{1+x}\) for \(x\in(-1,0).\) \footnote{Strictly speaking we only need to show this for \(x\in(-1,-1/2).\)In \hyperref[x:problem:prob_Taylor_Series-Binomial_Series_and]{problem~{\xreffont\ref{x:problem:prob_Taylor_Series-Binomial_Series_and}}} we covered\(x\in (-1/2,0).\)\label{g:fn:idp142}} With this in mind, let \(x\) be a fixed number with \(-1\lt x\lt 0\) and consider that the binomial series is the Maclaurin series for the function \(f(x)=(1+x)^{\frac{1}{2}}\). As we saw before,%
\begin{equation*}
f^{(n+1)}(t)=\left(\frac{1}{2}\right)\left(\frac{1}{2}-1\right)\cdots\left(\frac{1}{2}-n\right)\left(1+t\right)^{\frac{1}{2}-(n+1)}\text{,}
\end{equation*}
so the Cauchy form of the remainder is given by%
\begin{equation*}
0\le\abs{\frac{f^{(n+1)}(c)}{n!}(x-c)^n(x-0)}= \abs{\frac{\left(\frac{1}{2}\right)\left(\frac{1}{2}-1\right)\cdots\left(\frac{1}{2}-n\right)}{n!}\frac{(x-c)^n}{(1+c)^{n+\frac{1}{2}}}\cdot x}
\end{equation*}
where \(c\) is some number with \(x\leq c\leq 0\). Thus we have%
\begin{align*}
0\amp \leq\left|\frac{\left(\frac{1}{2}\right)\left(\frac{1}{2}-1\right)\cdots\left(\frac{1}{2}-n\right)}{n!}\frac{(x-c)^nx}{(1+c)^{n+\frac{1}{2}}}\right|\\
\amp =\frac{\left(\frac{1}{2}\right)\left(1-\frac{1}{2}\right)\cdots\left(n-\frac{1}{2}\right)}{n!}\frac{|x-c|^n|x|}{(1+c)^{n+\frac{1}{2}}}\\
\amp =\frac{\left(\frac{1}{2}\right)\left(\frac{1}{2}\right)\left(\frac{3}{2}\right)\left(\frac{5}{2}\right)\cdots\left(\frac{2n-1}{2}\right)}{n!}\frac{(c-x)^n}{(1+c)^n}\frac{|\,x|}{\sqrt{1+c}}\\
\amp =\frac{1\cdot 1\cdot 3\cdot 5\cdot\,\cdots\,\cdot\left(2n-1\right)}{2^{^{n+1}}n!}\left(\frac{c-x}{1+c}\right)^n\frac{|\,x|}{\sqrt{1+c}}\\
\amp =\frac{1\cdot 1\cdot 3\cdot 5\cdot\,\cdots\,\cdot\left(2n-1\right)}{2\cdot 2\cdot 4\cdot 6\cdot\,\cdots\,\cdot 2n}\left(\frac{c-x}{1+c}\right)^n\frac{|\,x|}{\sqrt{1+c}}\\
\amp =\frac{1}{2}\cdot\frac{1}{2}\cdot\frac{3}{4}\cdot\frac{5}{6}\cdot\cdots\,\cdot\frac{2n-1}{2n}\cdot\left(\frac{c-x}{1+c}\right)^n\frac{|\,x|}{\sqrt{1+c}}\\
\amp \leq\left(\frac{c-x}{1+c}\right)^n\frac{|\,x|}{\sqrt{1+c}}\text{.}
\end{align*}
%
\par
Notice that if \(-1\lt x\leq c\),\(\) then \(0\lt 1+x\leq 1+c\). Thus \(0\lt \frac{1}{1+c}\leq\frac{1}{1+x}\) and \(\frac{1}{\sqrt{1+c}}\leq\frac{1}{\sqrt{1+x}}\). Thus we have%
\begin{equation*}
0\leq\left|\frac{\left(\frac{1}{2}\right)\left(\frac{1}{2}-1\right)\cdots\left(\frac{1}{2}-n\right)}{n!}\frac{(x-c)^nx}{(1+c)^{n+\frac{1}{2}}}\right|\leq\left(\frac{c-x}{1+c}\right)^n\frac{|\,x|}{\sqrt{1+x}}\text{.}
\end{equation*}
%
\begin{problem}{}{g:problem:idp143}%
\index{Binomial Series, the!\(g(c)=\frac{c-x}{1+c}\) is increasing}\index{\(g(c)=\frac{c-x}{1+c}\) is increasing on \([x,0]\)} Suppose \(-1\lt x\leq c\leq 0\) and consider the function \(g(c)=\frac{c-x}{1+c}\). Show that on \([x,0]\), \(g\) is increasing and use this to conclude that for \(-1\lt x\leq c\leq 0\),%
\begin{equation*}
\frac{c-x}{1+c}\leq|x|\text{.}
\end{equation*}
%
\par
Use this fact to finish the proof that the binomial series converges to \(\sqrt{1+x}\) for \(-1\lt x\lt 0\).%
\end{problem}
The proofs of both the Lagrange form and the Cauchy form of the remainder for Taylor series made use of two crucial facts about continuous functions. First, we assumed the Extreme Value Theorem: Any continuous function on a closed%
\begin{figureptx}{Augustin Cauchy}{g:figure:idp144}{}%
\index{Cauchy, Augustin!portrait of}%
\begin{image}{0.36}{0.28}{0.36}%
\includegraphics[width=\linewidth]{images/Cauchy.png}
\end{image}%
\tcblower
\end{figureptx}%
bounded interval assumes its maximum and minimum somewhere on the interval. Second, we assumed that any continuous function satisfied the Intermediate Value Theorem: If a continuous function takes on two different values, then it must take on any value between those two values.%
\par
Mathematicians in the late \(1700\)s and early \(1800\)ccccs typically considered these facts to be intuitively obvious.  This was natural since our understanding of continuity at that time was, solely, intuitive.  Intuition is a useful tool, but as we have seen before it is also unreliable.  For example consider the following function.%
\begin{align}
f(x)= 
\begin{cases}
x\sin\left(\frac{1}{x}\right),\amp \text{if } x\neq 0,\\
0, \amp \text{ if } x=0 
\end{cases} \text{.}\label{x:mrow:xsin1x}
\end{align}
%
\par
Is this function continuous at 0?  Near zero its graph looks like this:%
\begin{figureptx}{\hyperref[x:mrow:xsin1x]{Formula~({\xreffont\ref{x:mrow:xsin1x}})} near \(0\).}{g:figure:idp145}{}%
\begin{image}{0}{1}{0}%
\includegraphics[width=\linewidth]{images/Ch5fig4.png}
\end{image}%
\tcblower
\end{figureptx}%
but this graph must be taken with a grain of salt as \(\sin
\left(\frac{1}{x}\right)\) oscillates infinitely often as \(x\) nears zero.%
\par
No matter what your guess may be, it is clear that it is hard to analyze such a function armed with only an intuitive notion of continuity.  We will revisit this example in the next chapter.%
\par
As with convergence, continuity is more subtle than it first appears.%
\par
We put convergence on solid ground by providing a completely analytic definition in the previous chapter.  What we need to do in the next chapter is provide a completely rigorous definition for continuity.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 8.4 Additional Problems}
\typeout{************************************************}
%
\begin{sectionptx}{Additional Problems}{}{Additional Problems}{}{}{x:section:TaylorSeries-AddProb}
\begin{problem}{}{g:problem:idp146}%
Find the Integral form, Lagrange form, and Cauchy form of the remainder for Taylor series for the following functions expanded about the given values of \(\,a\).%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\(f(x)=e^x\), \(a=0\)%
\item[(b)]\(f(x)=\sqrt{x}\), \(a=1\)%
\item[(c)]\(f(x)=(1+x)^\alpha\), \(a=0\)%
\item[(d)]\(f(x)=\frac{1}{x}\), \(a=3\)%
\item[(e)]\(f(x)=\ln x,\ a=2\)%
\item[(f)]\(f(x)=\cos x, a=\frac{\pi}{2}\)%
\end{enumerate}
\end{problem}
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Chapter 9 Continuity: What It Isn't and What It Is}
\typeout{************************************************}
%
\begin{chapterptx}{Continuity: What It Isn't and What It Is}{}{Continuity: What It Isn't and What It Is}{}{}{x:chapter:Continuity}
%
%
\typeout{************************************************}
\typeout{Section 9.1 An Analytic Definition of Continuity}
\typeout{************************************************}
%
\begin{sectionptx}{An Analytic Definition of Continuity}{}{An Analytic Definition of Continuity}{}{}{x:section:Continuity-AnalyticDef}
Before the invention of calculus, the notion of continuity was treated intuitively if it was treated at all.  At first pass, it seems a very simple idea based solidly in our experience of the real world.  Standing on the bank we see a river flow past us continuously, not by tiny jerks.  Even when the flow might seem at first to be discontinuous, as when it drops precipitously over a cliff, a closer examination shows that it really is not. As the water approaches the cliff it speeds up.  When it finally goes over it accelerates very quickly but no matter how fast it goes it moves continuously, moving from here to there by occupying every point in between.  This is continuous motion. It never disappears over there and instantaneously reappears over here.  That would be discontinuous motion.%
\par
Similarly, a thrown stone flies continuously (and smoothly) from release point to landing point, passing through each point in its path.%
\par
But wait.%
\par
If the stone passes through discrete points it must be doing so by teeny tiny little jerks, mustn't it?  Otherwise how would it get from one point to the next?  Is it possible that motion in the real world, much like motion in a movie, is really composed of tiny jerks from one point to the next but that these tiny jerks are simply too small and too fast for our senses to detect?%
\par
If so, then the real world is more like the rational number line (\(\QQ\)) from \hyperref[x:chapter:NumbersRealRational]{Chapter~{\xreffont\ref{x:chapter:NumbersRealRational}}} than the real number line (\(\RR\)).  In that case, motion really consists of jumping discretely over the ``missing'' points (like \(\sqrt{2}\)) as we move from here to there. That may seem like a bizarre idea to you \textemdash{} it does to us as well \textemdash{} but the idea of continuous motion is equally bizarre.  It's just a little harder to see why.%
\par
The real world will be what it is regardless of what we believe it to be, but fortunately in mathematics we are \emph{not} constrained to live in it.  So we won't even try.  We will simply postulate that no such jerkiness exists; that all motion is continuous.%
\par
However we \emph{are} constrained to live with the logical consequences of our assumptions, once they are made.  These will lead us into some very deep waters indeed.%
\par
The intuitive treatment of continuity was maintained throughout the 1700's as it was not generally perceived that a truly rigorous definition was necessary.  Consider the following definition given by Euler in 1748.%
\begin{quote}%
A continuous curve is one such that its nature can be expressed by a single function of \(x.\) If a curve is of such a nature that for its various parts . . . different functions of \(x\) are required for its expression, . . . , then we call such a curve discontinuous.%
\end{quote}
However, the complexities associated with Fourier series and the types of functions that they represented caused mathematicians in the early \(1800\)s to rethink their notions of continuity.  As we saw in \hyperref[x:part:Interregnum]{Part~{\xreffont\ref{x:part:Interregnum}}}, the graph of the function defined by the Fourier series%
\begin{equation*}
\frac{4}{\pi}\sum_{k=0}^\infty\frac{\left(-1\right)^k}{\left(2k+1\right)} \cos \left(\left(2k+1\right)\pi x\right)
\end{equation*}
looked like this:%
\begin{figureptx}{}{g:figure:idp147}{}%
\begin{image}{0.125}{0.75}{0.125}%
\includegraphics[width=\linewidth]{images/Ch5fig1.png}
\end{image}%
\tcblower
\end{figureptx}%
This function went against Euler's notion of what a continuous function should be.  Here, an infinite sum of continuous cosine curves provided a single expression which resulted in a ``discontinuous'' curve.  But as we've seen this didn't happen with power series and an intuitive notion of continuity is inadequate to explain the difference.  Even more perplexing is the following situation.  Intuitively, one would think that a continuous curve should have a tangent line at at least one point.  It may have a number of jagged points to it, but it should be ``smooth'' somewhere.  An example of this would be \(f(x)=x^{2/3}\).  Its graph is given by%
\begin{figureptx}{}{g:figure:idp148}{}%
\begin{image}{0.125}{0.75}{0.125}%
\includegraphics[width=\linewidth]{images/Ch5fig2.png}
\end{image}%
\tcblower
\end{figureptx}%
This function is not differentiable at the origin but it is differentiable everywhere else.  One could certainly come up with examples of functions which fail to be differentiable at any number of points but, intuitively, it would be reasonable%
\begin{figureptx}{Karl Weierstrass}{g:figure:idp149}{}%
\index{Weierstrass, Karl!portrait of}%
\begin{image}{0.325}{0.35}{0.325}%
\includegraphics[width=\linewidth]{images/Weierstrass.png}
\end{image}%
\tcblower
\end{figureptx}%
to expect that a continuous function should be differentiable \emph{somewhere}.  We might conjecture the following:%
\begin{conjecture}{}{}{x:conjecture:conj_ContImplyDiff}%
If \(f\) is continuous on an interval \(I\) then there is some \(a\in I\), such that \(f^\prime(a)\) exists.%
\end{conjecture}
Surprisingly, in \(1872\), Karl Weierstrass \index{Weierstrass, Karl} showed that the above conjecture is \alert{FALSE}. He did this by displaying the counterexample:%
\begin{equation*}
f(x)=\sum_{n=0}^\infty b^n\cos(a^n\pi x)\text{.}
\end{equation*}
%
\par
Weierstrass showed that if \(a\) is an odd integer, \(b\in(0,1)\), and \(ab>1+\frac{3}{2}\pi\), then \(f\) is continuous everywhere, but is nowhere differentiable.  Such a function is somewhat ``fractal'' in nature, and it is clear that a definition of continuity relying on intuition is inadequate to study it.%
\begin{problem}{}{g:problem:idp150}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Given \(f(x)=\sum_{n=0}^\infty\left(\frac{1}{2}\right)^n\cos\left(a^n\pi
x\right)\), what is the smallest value of \(a\) for which \(f\) satisfies Weierstrass' criterion to be continuous and nowhere differentiable.%
\item[(b)]Let \(f(x,N)=\sum_{n=0}^N\left(\frac{1}{2}\right)^n\cos\left(13^n\pi
x\right)\) and use a computer algebra system to plot \(f(x,N)\) for \(N=0,1,2,3,4,10\) and \(x\in[0,1]\).%
\item[(c)]Plot \(f(x,10)\) for \(x\in[\,0,c]\), where \(c=0.1,0.01,0.001,0.0001,0.00001\).  Based upon what you see in parts b and c, why would we describe the function to be somewhat ``fractal'' in nature?%
\end{enumerate}
\end{problem}
Just as it was important to define convergence with a rigorous definition without appealing to intuition or geometric representations, it is imperative that we define continuity in a rigorous fashion not relying on graphs.%
\par
The first appearance of a definition of continuity which did not rely on geometry or intuition was given in 1817 by Bernhard Bolzano \index{Bolzano, Bernhard} in a paper published in the Proceedings of the Prague Scientific Society entitled \textit{Rein analytischer Beweis des Lehrsatzes dass zwieschen je zwey Werthen, die ein entgegengesetztes Resultat gewaehren, wenigstens eine reele Wurzel der Gleichung liege} (Purely Analytic Proof of the Theorem that Between Any Two Values that Yield Results of Opposite Sign There Will be at Least One Real Root of the Equation).%
\begin{figureptx}{Bernhard Bolzano}{g:figure:idp151}{}%
\index{Bolzano, Bernhard!portrait of}%
\begin{image}{0.325}{0.35}{0.325}%
\includegraphics[width=\linewidth]{images/Bolzano.png}
\end{image}%
\tcblower
\end{figureptx}%
From the title it should be clear that in this paper Bolzano is proving the Intermediate Value Theorem.  To do this he needs a completely analytic definition of continuity.  The substance of Bolzano's idea is that if \(f\) is continuous at a point \(a\) then \(f(x)\) should be ``close to'' \(f(a)\) whenever \(x\) is ``close enough to'' \(a\).  More precisely, Bolzano said that \(f\) is continuous at \(a\) provided \(\abs{f(x)-f(a)}\) can be made smaller than any given quantity provided we make \(\abs{x-a}\) sufficiently small.%
\par
The language Bolzano uses is very similar to the language Leibniz \index{Leibniz, Gottfried Wilhelm} used when he postulated the existence of infinitesimally small numbers. Leibniz said that infinitesimals are ``smaller than any given quantity but not zero.'' Bolzano says that ``\(\abs{f(x)-f(a)}\) can be made smaller than any given quantity provided we make \(\abs{x-a}\) sufficiently small.'' But Bolzano stops short of saying that \(\abs{x-a}\) is \emph{infinitesimally} small.  Given \(a\), we can choose \(x\) so that \(\abs{x-a}\) is smaller than any real number we could name, say \(b\), provided we name \(b\) \emph{first}, but for any given choice of \(x\), \(\abs{x-a}\), and \(b\) are both still real numbers.  Possibly very small real numbers to be sure, but real numbers nonetheless.  Infinitesimals have no place in Bolzano's construction.%
\par
\index{Bolzano, Bernhard} Bolzano's paper was not well known when Cauchy \index{Cauchy, Augustin} proposed a similar definition in his \emph{Cours d'analyse}~\hyperlink{x:biblio:bradley09__cauch_cours}{[{\xreffont 1}]} of 1821 so it is usually Cauchy who is credited with this definition, but even Cauchy's definition is not quite tight enough for modern standards.  It was Karl Weierstrass in 1859 who finally gave the modern definition.%
\begin{definition}{}{x:definition:def_continuity}%
\index{continuity!definition of}\index{continuity} We say that a function \textbraceleft{}\(\boldsymbol{f}\) is continuous at \(\boldsymbol{a}\)\textbraceright{} provided that for any \(\eps>0\), there exists a \(\delta>0\) such that if \(\abs{x-a}\lt
\delta\) then \(|f(x)-f(a)|\lt \eps\).%
\end{definition}
Notice that the definition of continuity of a function is done point-by-point.  A function can certainly be continuous at some points while discontinuous at others.  When we say that \(f\) is continuous on an interval, then we mean that it is continuous at every point of that interval and, in theory, we would need to use the above definition to check continuity at each individual point.%
\par
\index{Extreme Value Theorem (EVT)!continuity and}\index{continuity!Extreme Value Theorem (EVT) and}\index{Intermediate Value Theorem (IVT)!continuity and}\index{continuity!Intermediate Value Theorem and} Our definition fits the bill in that it does not rely on either intuition or graphs, but it is this very non-intuitiveness that makes it hard to grasp.  It usually takes some time to become comfortable with this definition, let alone use it to prove theorems such as the Extreme Value Theorem and Intermediate Value Theorem.  So let's go slowly to develop a feel for it.%
\par
This definition spells out a completely black and white procedure: you give me a positive number \(\eps\), and I must be able to find a positive number \(\delta\) which satisfies a certain property.  If I can always do that then the function is continuous at the point of interest.%
\par
This definition also makes very precise what we mean when we say that \(f(x)\) should be ``close to'' \(f(a)\) whenever \(x\) is ``close enough to'' \(a\).  For example, intuitively we know that \(f(x)=x^2\) should be continuous at \(x=2\).  This means that we should be able to get \(x^2\) to within, say, \(\eps=.1\) of \(4\) provided we make \(x\) close enough to \(2\).  Specifically, we want \(3.9\lt x^2\lt 4.1\).  This happens exactly when \(\sqrt{3.9}\lt x\lt \sqrt{4.1}\).  Using the fact that \(\sqrt{3.9}\lt 1.98\) and \(2.02\lt \sqrt{4.1}\), then we can see that if we get \(x\) to within \(\delta=.02\) of \(2\), then \(\sqrt{3.9}\lt 1.98\lt x\lt 2.02\lt
\sqrt{4.1}\) and so \(x^2\) will be within .\(1\) of \(\,4\).  This is very straightforward.  What makes this situation more difficult is that we must be able to do this for any \(\eps>0\).%
\par
Notice the similarity between this definition and the definition of convergence of a sequence.  Both definitions have the challenge of an \(\eps>0\).  In the definition of \(\lim_{n\rightarrow\infty}s_n=s\), we had to get \(s_n\) to within \(\eps\) of \(s\) by making \(n\) large enough.  For sequences, the challenge lies in making \(\abs{s_n-s}\) sufficiently small.  More precisely, given \(\eps>0\) we need to decide how large \(n\) should be to guarantee that \(\abs{s_n-s}\lt \eps\).%
\par
In our definition of continuity, we still need to make something small (namely \(\abs{f(x)-f(a)}\lt \eps\)), only this time, we need to determine how close \(x\) must be to \(a\) to ensure this will happen instead of determining how large \(n\) must be.%
\par
What makes \(f\) continuous at \(a\) is the arbitrary nature of \(\eps\) (as long as it is positive).  As \(\eps\) becomes smaller, this forces \(f(x)\) to be closer to \(f(a)\).  That we can always find a positive distance \(\delta\) to work is what we mean when we say that we can make \(f(x)\) as close to \(f(a)\) as we wish, provided we get \(x\) close enough to \(a\).  The sequence of pictures below illustrates that the phrase ``for any \(\eps>0\), there exists a \(\delta>0\) such that if \(|\,x-a|\lt \delta\) then \(|f(x)-f(a)|\lt \eps\)'' can be replaced by the equivalent formulation ``for any \(\eps>0\), there exists a \(\delta>0\) such that if \(a-\delta\lt x\lt a+\delta\) then \(f(a)-\eps\lt f(x)\lt
f(a)+\eps\).'' This could also be replaced by the phrase ``for any \(\eps>0\), there exists a \(\delta>0\) such that if \(x\in(a-\delta,a+\delta)\) then \(f(x)\in(f(a)-\eps,f(a)+\eps)\).'' All of these equivalent formulations convey the idea that we can get \(f(x)\) to within \(\eps\) of \(f(a)\), provided we make \(x\) within \(\delta\) of \(a\), and we will use whichever formulation suits our needs in a particular application.%
\begin{sidebyside}{2}{0.025}{0.025}{0.05}%
\begin{sbspanel}{0.45}[center]%
\includegraphics[width=\linewidth]{images/Ch5fig3a.png}
\end{sbspanel}%
\begin{sbspanel}{0.45}[center]%
\includegraphics[width=\linewidth]{images/Ch5fig3b.png}
\end{sbspanel}%
\end{sidebyside}%
\begin{sidebyside}{2}{0.025}{0.025}{0.05}%
\begin{sbspanel}{0.45}[center]%
\includegraphics[width=\linewidth]{images/Ch5fig3c.png}
\end{sbspanel}%
\begin{sbspanel}{0.45}[center]%
\includegraphics[width=\linewidth]{images/Ch5fig3d.png}
\end{sbspanel}%
\end{sidebyside}%
The precision of the definition is what allows us to examine continuity without relying on pictures or vague notions such as ``nearness'' or ``getting closer to.'' We will now consider some examples to illustrate this precision.%
\begin{example}{}{g:example:idp152}%
Use the definition of continuity to show that \(f(x)=x\) is continuous at any point \(a\).%
\end{example}
If we were to draw the graph of this line, then you would likely say that this is obvious.  The point behind the definition is that we can back up your intuition in a rigorous manner.%
\begin{proof}{}{g:proof:idp153}
Let \(\eps>0\). Let \(\delta=\eps\). If \(|\,x-a|\lt \delta\), then%
\begin{equation*}
|f(x)-f(a)|=|\,x-a|\lt \eps
\end{equation*}
%
\par
Thus by the definition, \(f\) is continuous at \(a\).%
\end{proof}
\begin{problem}{}{g:problem:idp154}%
\index{continuity!\(f(x) = mx +b\) is continuous everywhere} Use the definition of continuity to show that if \(m\) and \(b\) are fixed (but unspecified) real numbers then the function%
\begin{equation*}
f(x) = mx+b
\end{equation*}
is continuous at every real number \(a\).%
\end{problem}
\begin{example}{}{g:example:idp155}%
Use the definition of continuity to show that \(f(x)=x^2\) is continuous at \(a=0\).%
\end{example}
\begin{proof}{}{g:proof:idp156}
Let \(\eps>0\). Let \(\delta=\sqrt{\eps}\). If \(|\,x-0|\lt \delta\), then \(|\,x|\lt \sqrt{\eps}\). Thus%
\begin{equation*}
\abs{x^2-0^2}=|\,x|^2\lt \left(\sqrt{\eps}\right)^2=\eps\text{.}
\end{equation*}
%
\par
Thus by the definition, \(f\) is continuous at \(0\).%
\end{proof}
Notice that in these proofs, the challenge of an \(\eps>0\) was first given.  This is because the choice of \(\delta\) must depend upon \(\eps\).  Also notice that there was no explanation for our choice of \(\delta\).  We just supplied it and showed that it worked.  As long as \(\delta>0\), then this is all that is required.  In point of fact, the \(\delta\) we chose in each example was not the only choice that worked; any smaller \(\delta\) would work as well.%
\begin{problem}{}{g:problem:idp157}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Given a particular \(\eps>0\) in the definition of continuity, show that if a particular \(\delta_0>0\) satisfies the definition, then any \(\delta\) with \(0\lt \delta\lt \delta_0\) will also work for this \(\eps\).%
\item[(b)]Show that if a \(\delta\) can be found to satisfy the conditions of the definition of continuity for a particular \(\eps_0>0\), then this \(\delta\) will also work for any \(\,\eps\) with \(0\lt \eps_0\lt
\eps\).%
\end{enumerate}
\end{problem}
It wasn't explicitly stated in the definition but when we say ``if \(\abs{x-a}\lt \delta\) then \(|f(x)-f(a)|\lt
\eps\),'' we should be restricting ourselves to \(x\) values which are in the domain of the function \(f\), otherwise \(f(x)\) doesn't make sense.  We didn't put it in the definition because that definition was complicated enough without this technicality.  Also in the above examples, the functions were defined everywhere so this was a moot point.  We will continue with the convention that when we say ``if \(|\,x-a|\lt \delta\) then \(|f(x)-f(a)|\lt \eps\),'' we will be restricting ourselves to \(x\) values which are in the domain of the function \(f\).  This will allow us to examine continuity of functions not defined for all \(x\) without restating this restriction each time.%
\begin{problem}{}{x:problem:prob_extended_sqrt_is_continuous_at_zero}%
\index{continuity!\(\pm\sqrt{x}\) is continuous at zero} Use the definition of continuity to show that%
\begin{equation*}
f(x)= \begin{cases}\sqrt{x} \amp  \text{ if }  x\ge0\\ -\sqrt{-x} \amp  \text{ if }  x\lt 0 \end{cases}
\end{equation*}
is continuous at \(a=0\).%
\end{problem}
\begin{problem}{}{g:problem:idp158}%
\index{\(\sqrt{x}\)!is continuous at zero} Use the definition of continuity to show that \(f(x)=
\sqrt{x}\) is continuous at \(a=0\).  How is this problem different from \hyperref[x:problem:prob_extended_sqrt_is_continuous_at_zero]{problem~{\xreffont\ref{x:problem:prob_extended_sqrt_is_continuous_at_zero}}}? How is it similar?%
\end{problem}
Sometimes the \(\delta\) that will work for a particular \(\eps\) is fairly obvious to see, especially after you've gained some experience.  This is the case in the above examples (at least after looking back at the proofs).  However, the task of finding a \(\delta\) to work is usually not so obvious and requires some scrapwork.  This scrapwork is vital toward producing a \(\delta\), but again is not part of the polished proof.  This can be seen in the following example.%
\begin{example}{}{x:example:example_SqrtContinuous}%
Use the definition of continuity to prove that \(f(x)=\sqrt{x}\) is continuous at \(a=1\).%
\par
\terminology{SCRAPWORK}%
\par
As before, the scrapwork for these problems often consists of simply working backwards.  Specifically, given an \(\eps>0\), we need to find a \(\delta>0\) so that \(|\sqrt{x}-\sqrt{1}|\lt \eps\), whenever \(|\,x-1|\lt
\delta\).  We work backwards from what we want, keeping an eye on the fact that we can control the size of \(\abs{x-1}\).%
\begin{equation*}
|\sqrt{x}-\sqrt{1}|=|\frac{\left(\sqrt{x}-1\right)\left(\sqrt{x}+1\right)}{\sqrt{x}+1}|=\frac{|\,x-1|}{\sqrt{x}+1}\lt |\,x-1|\text{.}
\end{equation*}
%
\par
This seems to suggest that we should make \(\delta=\eps\). We're now ready for the formal proof.%
\end{example}
\begin{proof}{}{g:proof:idp159}
Let \(\eps>0\). Let \(\delta=\eps\). If \(|\,x-1|\lt \delta\), then \(|\,x-1|\lt \eps\), and so%
\begin{equation*}
\abs{\sqrt{x}-\sqrt{1}}=|\frac{\left(\sqrt{x}-1\right)\left(\sqrt{x}+1\right)}{ \sqrt{x}+1}|=\frac{|x-1|}{\sqrt{x}+1}\lt \abs{x-1}\lt \eps\text{.}
\end{equation*}
%
\par
Thus by definition, \(f(x)=\sqrt{x}\) is continuous at \(1\).%
\end{proof}
\begin{figureptx}{Paul Halmos}{g:figure:idp160}{}%
\index{Halmos, Paul!portrait of}%
\begin{image}{0.325}{0.35}{0.325}%
\includegraphics[width=\linewidth]{images/Halmos.png}
\end{image}%
\tcblower
\end{figureptx}%
Bear in mind that someone reading the formal proof will not have seen the scrapwork, so the choice of \(\delta\) might seem rather mysterious.  However, you are in no way bound to motivate this choice of \(\delta\) and usually you should not, unless it is necessary for the formal proof.  All you have to do is find this \(\delta\) and show that it works.  Furthermore, to a trained reader, your ideas will come through when you demonstrate that your choice of \(\delta\) works.%
\par
Now reverse this last statement.  \emph{As} a trained reader, when you read the proof of a theorem it is \emph{your} responsibility to find the scrapwork, to see how the proof works and understand it fully.  As the renowned mathematical expositor Paul Halmos \index{Halmos, Paul} (1916-2006) said, ``Don't just read it; fight it! Ask your own questions, look for your own examples, discover your own proofs. Is the hypothesis necessary? Is the converse true? What happens in the classical special case? What about the degenerate cases? Where does the proof use the hypothesis?''%
\par
This is the way to learn mathematics.  It is really the only way.%
\begin{problem}{}{g:problem:idp161}%
Use the definition of continuity to show that \(f(x)=\sqrt{x}\) is continuous at any positive real number \(a\).%
\end{problem}
\begin{problem}{}{g:problem:idp162}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Use a unit circle to show that for \(0\leq\theta\lt
\frac{\pi}{2}\), \(\sin \theta\leq\theta\) and \(1-\cos \theta\leq\theta\) and conclude \(\abs{\sin
\theta}\leq\abs{\theta}\) and \(\abs{1-\cos
\theta}\leq\abs{\theta}\) for \(-\frac{\pi}{2}\lt
\theta\) \(\lt \frac{\pi}{2}\).%
\item[(b)]Use the definition of continuity to prove that \(f(x)=\sin x\) is continuous at any point \(a\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp163}{}\quad{}\(\sin x=\sin\left(x-a+a\right)\).%
\end{enumerate}
\end{problem}
\begin{problem}{}{g:problem:idp164}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Use the definition of continuity to show that \(f(x)=e^x\) is continuous at \(a=0\).%
\item[(b)]Show that \(f(x)=e^x\) is continuous at any point \(a\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp165}{}\quad{}Rewrite \(e^x-e^a\) as \(e^{a+(x-a)}-e^a\) and use what you proved in part a.%
\end{enumerate}
\end{problem}
In the above problems, we used the definition of continuity to verify our intuition about the continuity of familiar functions.  The advantage of this analytic definition is that it can be applied when the function is not so intuitive.  Consider, for example, the function given at the end of the last chapter.%
\begin{equation*}
f(x)= \begin{cases}
x\,\sin\left(\frac{1}{x}\right),\amp \text{ if } x\neq 0\\
0, \amp \text{ if } x=0 
\end{cases}  \text{.}
\end{equation*}
%
\par
Near zero, the graph of \(f(x)\) looks like this:%
\begin{image}{0.125}{0.75}{0.125}%
\includegraphics[width=\linewidth]{images/Ch5fig4.png}
\end{image}%
As we mentioned in the previous chapter, since sin\(\left(\frac{1}{x}\right)\) oscillates infinitely often as \(x\) nears zero this graph must be viewed with a certain amount of suspicion.  However our completely analytic definition of continuity shows that this function is, in fact, continuous at 0.%
\begin{problem}{The Topologist's Sine Function.}{g:problem:idp166}%
\index{Topologist's sine function!is continuous at zero} Use the definition of continuity to show that%
\begin{equation*}
f(x)= \begin{cases}x\,\sin\left(\frac{1}{x}\right),\amp \text{ if } x\neq 0\\ 0\text{,} \amp \text{ if } x=0 \end{cases}
\end{equation*}
is continuous at \(0\).%
\end{problem}
Even more perplexing is the function defined by%
\begin{equation*}
D(x)= \begin{cases}
x, \amp \text{ if } x\text{ is rational } \\
0, \amp \text{ if } x\text{ is irrational } . 
\end{cases} 
\end{equation*}
%
\par
To the naked eye, the graph of this function looks like the lines \(y=0\) and \(y=x\).  Of course, such a graph would not be the graph of a function.  Actually, both of these lines have holes in them.  Wherever there is a point on one line there is a ``hole'' on the other.  Each of these holes are the width of a single point (that is, their ``width'' is zero!) so they are invisible to the naked eye (or even magnified under the most powerful microscope available).  This idea is illustrated in the following graph%
\begin{image}{0.2}{0.6}{0.2}%
\includegraphics[width=\linewidth]{images/Ch5fig5.png}
\end{image}%
Can such a function so ``full of holes'' actually be continuous anywhere?  It turns out that we can use our definition to show that this function is, in fact, continuous at \(0\) and at no other point.%
\begin{problem}{}{g:problem:idp167}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Use the definition of continuity to show that the function%
\begin{equation*}
D(x)= \begin{cases}
x,\amp \text{ if } x\text{ is rational } \\
0,\amp \text{ if } x\text{ is irrational } \end{cases} 
\end{equation*}
is continuous at \(0\).%
\item[(b)]Let \(a\neq 0\).  Use the definition of continuity to show that \(D\) is not continuous at \(a\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp168}{}\quad{}You might want to break this up into two cases where \(a\) is rational or irrational.  Show that no choice of \(\delta>0\) will work for \(\eps=|\,a|\).  Note that \hyperref[x:theorem:thm_IrrationalBetweenIrrationals]{Theorem~{\xreffont\ref{x:theorem:thm_IrrationalBetweenIrrationals}}} of \hyperref[x:chapter:NumbersRealRational]{Chapter~{\xreffont\ref{x:chapter:NumbersRealRational}}} will probably help here.%
\end{enumerate}
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 9.2 Sequences and Continuity}
\typeout{************************************************}
%
\begin{sectionptx}{Sequences and Continuity}{}{Sequences and Continuity}{}{}{x:section:SequencesAndContinuity}
There is an alternative way to prove that the function%
\begin{equation*}
D(x)=\left\{ \begin{matrix}x\text{,} \amp \text{ if } x\text{ is rational } \\ 0\text{,} \amp \text{ if } x\text{ is irrational } \end{matrix} \right.
\end{equation*}
is not continuous at \(a\neq 0\).  We will examine this by looking at the relationship between our definitions of convergence and continuity.  The two ideas are actually quite closely connected, as illustrated by the following very useful theorem.%
\begin{theorem}{}{}{x:theorem:thm_LimDefOfContinuity}%
\index{continuity!via limits} The function \(f\) is continuous at \(a\) if and only if \(f\) satisfies the following property:%
\begin{equation*}
\forall\text{ sequences } \left(x_n\right)\text{, if } \,\,\lim_{n\rightarrow\infty}x_n=a \text{ then} \lim_{n\rightarrow\infty}f(x_n)=f(a).{}
\end{equation*}
%
\end{theorem}
\hyperref[x:theorem:thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{x:theorem:thm_LimDefOfContinuity}}} says that in order for \(f\) to be continuous, it is necessary and sufficient that any sequence \(\left(x_n\right)\) converging to \(a\) must force the sequence \(\left(f(x_n)\right)\) to converge to \(f(a)\).  A picture of this situation is below though, as always, the formal proof will not rely on the diagram.%
\begin{image}{0.315}{0.37}{0.315}%
\includegraphics[width=\linewidth]{images/Ch5fig6.png}
\end{image}%
This theorem is especially useful for showing that a function \(f,\) is not continuous at a point \(a\); all we need to do is exhibit a sequence \(\left(x_n\right)\) converging to \(a\) such that the sequence \(\lim_{n\rightarrow\infty}f(x_n)\) does \emph{not} converge to \(f(a)\).  Let's demonstrate this idea before we tackle the proof of \hyperref[x:theorem:thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{x:theorem:thm_LimDefOfContinuity}}}.%
\begin{example}{}{x:example:example_HeavisideNotContinuous}%
Use \hyperref[x:theorem:thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{x:theorem:thm_LimDefOfContinuity}}} to prove that%
\begin{equation*}
f(x)= \begin{cases}\frac{|x|}{x}\text{,} \amp \text{ if } x\neq 0\\ 0\text{,} \amp \text{ if } x=0 \end{cases}
\end{equation*}
is not continuous at \(0\).%
\end{example}
\begin{proof}{}{g:proof:idp169}
First notice that \(f\) can be written as%
\begin{equation*}
f(x)= \begin{cases}1\amp \text{ if } x>0\\ -1\amp \text{ if } x\lt 0\\ 0\amp \text{ if } x=0 \end{cases} \text{.}
\end{equation*}
%
\par
To show that \(f\) is not continuous at \(0\), all we need to do is create a single sequence \(\left(x_n\right)\)which converges to \(0\), but for which the sequence \(\left(f\left(x_n\right)\right)\) does not converge to \(f(0)=0\).  For a function like this one, just about any sequence will do, but let's use \(\left(\frac{1}{n}\right)\), just because it is an old familiar friend.%
\par
We have \(\displaystyle\lim_{n\rightarrow\infty}\frac{1}{n}=0\), but \(\displaystyle\lim_{n\rightarrow\infty}f\left(\frac{1}{n}\right)=\lim_{n\rightarrow
\infty}1=1\neq 0=f(0)\).  Thus by \hyperref[x:theorem:thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{x:theorem:thm_LimDefOfContinuity}}}, \(f\) is not continuous at \(0\).%
\end{proof}
\begin{problem}{}{g:problem:idp170}%
\index{continuity!Heaviside's function is not continuous at zero} Use \hyperref[x:theorem:thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{x:theorem:thm_LimDefOfContinuity}}} to show that%
\begin{equation*}
f(x)= \begin{cases}
\frac{\abs{x}}{x},\amp \text{ if } x\neq 0\\
a, \amp \text{ if } x=0 \end{cases}  
\end{equation*}
is not continuous at \(0\), no matter what value \(a\) is.%
\end{problem}
\begin{problem}{}{g:problem:idp171}%
Use \hyperref[x:theorem:thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{x:theorem:thm_LimDefOfContinuity}}} to show that%
\begin{equation*}
D(x)= \begin{cases}
x, \amp \text{ if } x\text{ is rational } \\
0, \amp \text{ if } x\text{ is irrational } \end{cases}  
\end{equation*}
is not continuous at \(a\neq 0\).%
\end{problem}
\begin{problem}{}{g:problem:idp172}%
The function \(T(x)=\sin\left(\frac{1}{x}\right)\) is often called the topologist's sine curve.  Whereas \(\sin
x\) has roots at \(n\pi\), \(n\in\ZZ\) and oscillates infinitely often as \(x\rightarrow\pm\infty\), \(T\) has roots at \(\frac{1}{n\pi},\,n\in\ZZ,\,n\neq
0\), and oscillates infinitely often as \(x\) approaches zero.  A rendition of the graph follows.%
\begin{image}{0.125}{0.75}{0.125}%
\includegraphics[width=\linewidth]{images/Ch5fig7.png}
\end{image}%
Notice that \(T\) is not even defined at \(x=0\). We can extend \(T\) to be defined at 0 by simply choosing a value for \(T(0):\)%
\begin{equation*}
T(x)= \begin{cases}
\sin\left(\frac{1}{x}\right),\amp \text{ if } x\neq 0\\ 
b,\amp \text{ if } x=0 \end{cases} \text{.}
\end{equation*}
%
\par
Use \hyperref[x:theorem:thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{x:theorem:thm_LimDefOfContinuity}}} to show that \(T\) is not continuous at \(0\), no matter what value is chosen for \(b\).%
\end{problem}
\begin{proof}{Sketch of the Proof of Theorem~{\xreffont\ref*{x:theorem:thm_LimDefOfContinuity}}.}{g:proof:idp173}
We've seen how we can use \hyperref[x:theorem:thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{x:theorem:thm_LimDefOfContinuity}}}, now we need to prove \hyperref[x:theorem:thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{x:theorem:thm_LimDefOfContinuity}}}.  The forward direction is fairly straightforward.  So we assume that \(f\) is continuous at \(a\) and start with a sequence \(\left(x_n\right)\) which converges to \(a\). What is left to show is that \(\lim_{n\rightarrow\infty}f(x_n)=f(a)\).  If you write down the definitions of \(f\) being continuous at \(a\), \(\lim_{n\rightarrow\infty}x_n=a\), and \(\lim_{n\rightarrow\infty}f(x_n)=f(a)\), you should be able to get from what you are assuming to what you want to conclude.%
\par
To prove the converse, it is convenient to prove its contrapositive.  That is, we want to prove that if \(f\) is not continuous at \(a\) then we can construct a sequence \(\left(x_n\right)\) that converges to \(a\) but \(\left(f(x_n)\right)\)does not converge to \(f(a)\). First we need to recognize what it means for \(f\) to not be continuous at \(a\).  This says that somewhere there exists an \(\eps>0\), such that no choice of \(\delta>0\) will work for this.  That is, for any such \(\delta\), there will exist \(x\), such that \(|\,x-a|\lt \delta\), but \(|f(x)-f(a)|\geq\eps\). With this in mind, if \(\delta=1\), then there will exist an \(x_1\) such that \(|\,x_1-a|\lt 1\), but \(|f(x_1)-f(a)|\geq\eps\).  Similarly, if \(\delta=\frac{1}{2}\), then there will exist an \(x_2\) such that \(|\,x_2-a|\lt \frac{1}{2}\), but \(|\,f(x_2)-f(a)|\geq\eps\).  If we continue in this fashion, we will create a sequence \(\left(x_n\right)\) such that \(|\,x_n-a|\lt \frac{1}{n}\), but \(|f(x_n)-f(a)|\geq\eps\).  This should do the trick.%
\end{proof}
\begin{problem}{}{g:problem:idp174}%
\index{continuity!via limits}\index{limit!\(\limit{x}{a}{f(x)}=f(a)\) implies \(f(x)\) is continuous} Turn the ideas of the previous two paragraphs into a formal proof of \hyperref[x:theorem:thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{x:theorem:thm_LimDefOfContinuity}}}.%
\end{problem}
\hyperref[x:theorem:thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{x:theorem:thm_LimDefOfContinuity}}} is a very useful result. It is a bridge between the ideas of convergence and continuity so it allows us to bring all of the theory we developed in \hyperref[x:chapter:Convergence]{Chapter~{\xreffont\ref{x:chapter:Convergence}}} to bear on continuity questions. For example consider the following.%
\begin{theorem}{}{}{x:theorem:thm_ContSumProd}%
\index{continuous functions!sum of continuous functions is continuous} Suppose \(f\) and \(g\) are both continuous at \(a\). Then \(f+g\) and \(f\cdot g\) are continuous at \(a\).%
\end{theorem}
\begin{proof}{}{g:proof:idp175}
We could use the definition of continuity to prove \hyperref[x:theorem:thm_ContSumProd]{Theorem~{\xreffont\ref{x:theorem:thm_ContSumProd}}}, but \hyperref[x:theorem:thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{x:theorem:thm_LimDefOfContinuity}}} makes our job much easier.  For example, to show that \(f+g\) is continuous, consider any sequence \(\left(x_n\right)\) which converges to \(a\).  Since \(f\) is continuous at \(a\), then by \hyperref[x:theorem:thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{x:theorem:thm_LimDefOfContinuity}}}, \(\lim_{n\rightarrow\infty}f(x_n)=f(a)\).  Likewise, since \(g\) is continuous at \(a\), then \(\lim_{n\rightarrow\infty}g(x_n)=g(a)\).%
\par
By \hyperref[x:theorem:thm_SumOfSequences]{Theorem~{\xreffont\ref{x:theorem:thm_SumOfSequences}}} of \hyperref[x:chapter:Convergence]{Chapter~{\xreffont\ref{x:chapter:Convergence}}},\(\)%
\begin{alignat*}{1}
\lim_{n\rightarrow\infty}(f+g)(x_n)\amp=\lim_{n\rightarrow\infty} \left(f(x_n)+g(x_n)\right)\\
\amp =\lim_{n\rightarrow\infty}f(x_n)+\,\lim_{n \rightarrow\infty}g(x_n)\\
\amp =f(a)+g(a)\\
\amp =(f+g)(a)\text{.}
\end{alignat*}
Thus by \hyperref[x:theorem:thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{x:theorem:thm_LimDefOfContinuity}}}, \(f+g\) is continuous at \(a\).  The proof that \(f\cdot g\) is continuous at \(a\) is similar.%
\end{proof}
\begin{problem}{}{g:problem:idp176}%
Use \hyperref[x:theorem:thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{x:theorem:thm_LimDefOfContinuity}}} to show that if \(f\) and \(g\) are continuous at \(a\), then \(f\cdot g\) is continuous at \(a\).%
\end{problem}
By employing \hyperref[x:theorem:thm_ContSumProd]{Theorem~{\xreffont\ref{x:theorem:thm_ContSumProd}}} a finite number of times, we can see that a finite sum of continuous functions is continuous. That is, if \(f_1,\,f_2,\,\ldots,\,f_n\) are all continuous at \(a\) then \(\sum_{j=1}^nf_j\) is continuous at \(a\). But what about an infinite sum? Specifically, suppose \(f_1,\,f_2,f_3,\ldots\) are all continuous at \(a\). Consider the following argument.%
\par
Let \(\eps>0\). Since \(f_j\) is continuous at \(a\), then there exists \(\delta_j>0\) such that if \(|\,x-a|\lt \delta_j\), then \(|f_j(x)-f_j(a)|\lt \frac{\eps}{2^j}\). Let \(\delta=\)min\(\left(\delta_1,\,\delta_2,\,\ldots\right)\). If \(|\,x-a|\lt \delta\), then%
\begin{equation*}
\left|\sum_{j=1}^\infty f_j(x)-\sum_{j=1}^\infty f_j(a)\right|=\left|\sum_{j=1}^\infty\left(f_j(x)-f_j(a)\right)\right|
\end{equation*}
%
\begin{equation*}
\leq\,\sum_{j=1}^\infty|f_j(x)-f_j(a)|\lt \sum_{j=1}^\infty\frac{ \eps}{2^j}=\eps\text{.}
\end{equation*}
%
\par
Thus by definition, \(\sum_{j=1}^\infty f_j\) is continuous at \(a\).%
\par
This argument seems to say that an infinite sum of continuous functions must be continuous (provided it converges). However we know that the Fourier series%
\begin{equation*}
\frac{4}{\pi}\sum_{k=0}^\infty\frac{\left(-1\right)^k}{\left(2k+1\right)}\cos\left(\left(2k+1\right)\pi x\right)
\end{equation*}
is a counterexample to this, as it is an infinite sum of continuous functions which does not converge to a continuous function. Something fundamental seems to have gone wrong here. Can you tell what it is?%
\par
This is a question we will spend considerable time addressing in \hyperref[x:chapter:PowerSeriesRedux]{Chapter~{\xreffont\ref{x:chapter:PowerSeriesRedux}}} (in particular, see \hyperref[x:problem:prob_Cauchy_s_incorrect_proof]{problem~{\xreffont\ref{x:problem:prob_Cauchy_s_incorrect_proof}}}) so if you don't see the difficulty, don't worry; you will. In the meantime keep this problem tucked away in your consciousness. It is, as we said, fundamental.%
\par
\hyperref[x:theorem:thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{x:theorem:thm_LimDefOfContinuity}}} will also handle quotients of continuous functions. There is however a small detail that needs to be addressed first. Obviously, when we consider the continuity of \(f/g\) at \(a\),\(\)we need to assume that \(g(a)\neq 0\). However, \(g\) may be zero at other values. How do we know that when we choose our sequence \(\left(x_n\right)\) converging to \(a\) that \(g(x_n)\) is not zero? This would mess up our idea of using the corresponding theorem for sequences (\hyperref[x:theorem:thm_LimitOfQuotient]{Theorem~{\xreffont\ref{x:theorem:thm_LimitOfQuotient}}} from \hyperref[x:chapter:Convergence]{Chapter~{\xreffont\ref{x:chapter:Convergence}}}). This can be handled with the following lemma.%
\begin{lemma}{}{}{x:lemma:lem_BoundedAwayFromZero}%
If \(g\) is continuous at \(a\) and \(g(a)\neq 0\), then there exists \(\delta>0\) such that \(g(x)\neq 0\) for all \(x\in(a-\delta,a+\delta)\).%
\end{lemma}
\begin{problem}{}{g:problem:idp177}%
Prove \hyperref[x:lemma:lem_BoundedAwayFromZero]{Lemma~{\xreffont\ref{x:lemma:lem_BoundedAwayFromZero}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp178}{}\quad{}Consider the case where \(g(a)>0\). Use the definition with \(\eps=\frac{g(a)}{2}\). The picture is below; make it formal.%
\begin{image}{0.125}{0.75}{0.125}%
\includegraphics[width=\linewidth]{images/Ch5fig8.png}
\end{image}%
For the case \(g(a)\lt 0\), consider the function \(-g\).%
\end{problem}
A consequence of this lemma is that if we start with a sequence \(\left(x_n\right)\) converging to \(a\), then for \(n\) sufficiently large, \(g(x_n)\neq 0\).%
\begin{problem}{}{g:problem:idp179}%
Use \hyperref[x:theorem:thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{x:theorem:thm_LimDefOfContinuity}}}, to prove that if \(f\) and \(g\) are continuous at \(a\) and \(g(a)\neq 0\), then \(f/g\) is continuous at \(a\).%
\end{problem}
\begin{theorem}{}{}{x:theorem:thm_ContComp}%
\index{continuous functions!the composition of continuous functions is continuous}%
Suppose \(f\) is continuous at \(a\) and \(g\) is continuous at \(f(a)\).  Then \(g\circ f\) is continuous at \(a.\) (Note that \((g\circ f)(x)=g(f(x))\).)%
\end{theorem}
\begin{problem}{}{g:problem:idp180}%
Prove \hyperref[x:theorem:thm_ContComp]{Theorem~{\xreffont\ref{x:theorem:thm_ContComp}}}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Using the definition of continuity.%
\item[(b)]Using \hyperref[x:theorem:thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{x:theorem:thm_LimDefOfContinuity}}}.%
\end{enumerate}
\end{problem}
The above theorems allow us to build continuous functions from other continuous functions.  For example, knowing that \(f(x)=x\) and \(g(x)=c\) are continuous, we can conclude that any polynomial,%
\begin{equation*}
p(x)=a_nx^n+a_{n-1}x^{n-1}+\cdots+a_1x+a_0
\end{equation*}
is continuous as well. We also know that functions such as \(f(x)=\sin\left(e^x\right)\) are continuous without having to rely on the definition.%
\begin{problem}{}{g:problem:idp181}%
Show that each of the following is a continuous function at every point in its domain.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Any polynomial.%
\item[(b)]Any rational function. (A rational function is defined to be a ratio of polynomials.)%
\item[(c)]\(\cos x\).%
\item[(d)]The other trig functions: \(\tan(x)\), \(\cot(x)\), \(\sec(x)\), and \(\csc(x)\).%
\end{enumerate}
\end{problem}
\begin{problem}{}{g:problem:idp182}%
What allows us to conclude that \(f(x)=\sin\left(e^x\right)\) is continuous at any point \(a\) without referring back to the definition of continuity?%
\end{problem}
\hyperref[x:theorem:thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{x:theorem:thm_LimDefOfContinuity}}} can also be used to study the convergence of sequences.  For example, since \(f(x)=e^x\) is continuous at any point and \(\lim_{n\rightarrow\infty}\frac{n+1}{n}=1\), then \(\lim_{n\rightarrow\infty}e^{\left(\frac{n+1}{n}\right)}=e\). This also illustrates a certain way of thinking about continuous functions.  They are the ones where we can ``commute'' the function and a limit of a sequence.  Specifically, if \(f\) is continuous at \(a\) and \(\lim_{n\rightarrow\infty}x_n=a\), then \(\lim_{n\rightarrow\infty}f(x_n)=f(a)=f\left(\lim_{n\rightarrow\infty}x_n\right)\).%
\begin{problem}{}{g:problem:idp183}%
Compute the following limits. Be sure to point out how continuity is involved.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\(\displaystyle\lim_{n\rightarrow\infty}\sin\left(\frac{n\pi}{2n+1}\right)\)%
\item[(b)]\(\displaystyle\lim_{n\rightarrow\infty}\sqrt{\frac{n}{n^2+1}}\)%
\item[(c)]\(\displaystyle\lim_{n\rightarrow\infty}e^{\left(\text{ sin } \left(1/n\right)\right)}\)%
\end{enumerate}
\end{problem}
Having this rigorous formulation of continuity is necessary for proving the Extreme Value Theorem and the Mean Value Theorem. However there is one more piece of the puzzle to address before we can prove these theorems.%
\par
We will do this in the next chapter, but before we go on it is time to define a fundamental concept that was probably one of the first you learned in calculus: limits.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 9.3 The Definition of the Limit of a Function}
\typeout{************************************************}
%
\begin{sectionptx}{The Definition of the Limit of a Function}{}{The Definition of the Limit of a Function}{}{}{x:section:Continuity-DefLimit}
Since these days the limit concept is generally regarded as the starting point for calculus, you might think it is a little strange that we've chosen to talk about continuity first. But historically, the formal definition of a limit came after the formal definition of continuity. In some ways, the limit concept was part of a unification of all the ideas of calculus that were studied previously and, subsequently, it became the basis for all ideas in calculus. For this reason it is logical to make it the first topic covered in a calculus course.%
\par
To be sure, limits were always lurking in the background. In his attempts to justify his calculations, Newton \index{Newton, Isaac} used what he called his doctrine of ``Ultimate Ratios.'' Specifically the ratio \(\frac{(x+h)^2-x^2}{h} = \frac{2xh+h^2}{h} = 2x+h\) becomes the ultimate ratio \(2x\) at the last instant of time before \(h\) - an ``evanescent quantity'' - vanishes~(\hyperlink{x:biblio:grabiner81__origin_cauch_rigor_calculy}{[{\xreffont 4}]}, p. 33). Similarly Leibniz's\index{Leibniz, Gottfried Wilhelm} ``infinitely small'' differentials \(\dx{ x}\) and \(\dx{ y}\) can be seen as an attempt to get ``arbitrarily close'' to \(x\) and \(y\), respectively. This is the idea at the heart of calculus: to get arbitrarily close to, say, \(x\) without actually reaching it.%
\par
As we saw in \hyperref[x:chapter:PowerSeriesQuestions]{Chapter~{\xreffont\ref{x:chapter:PowerSeriesQuestions}}}, Lagrange \index{Lagrange, Joseph-Louis} tried to avoid the entire issue of ``arbitrary closesness,'' both in the limit and differential forms when, in 1797, he attempted to found calculus on infinite series.%
\par
Although Lagrange's \index{Lagrange, Joseph-Louis} efforts failed, they set the stage for Cauchy \index{Cauchy, Augustin} to provide a definition of derivative which in turn relied on his precise formulation of a limit. Consider the following example: to determine the slope of the tangent line (derivative) of \(f(x) = \sin x\) at \(x=0\). We consider the graph of the difference quotient \(D(x) =\frac{\sin x }{x}\).%
\begin{image}{0.22}{0.56}{0.22}%
\includegraphics[width=\linewidth]{images/SinGraph.png}
\end{image}%
From the graph, it appears that \(D(0) =1\) but we must be careful.  \(D(0)\) doesn't even exist!  Somehow we must convey the idea that \(D(x)\) will approach \(1\) as \(x\) approaches \(0\), even though the function is not defined at \(0\).  Cauchy's idea was that the limit of \(D(x)\) would equal \(1\) because we can make \(D(x)\) differ from 1 by as little as we wish~(\hyperlink{x:biblio:jahnke03__histor_analy}{[{\xreffont 6}]}, p. 158).%
\par
Karl Weierstrass \index{Weierstrass, Karl} made these ideas precise in his lectures on analysis at the University of Berlin (1859-60) and provided us with our modern formulation.%
\begin{definition}{}{x:definition:def_limit}%
\index{limit} We say \(\limit{x}{a}{f(x)} =L\) provided that for each \(\eps>0\), there exists \(\delta>0\) such that if \(0\lt \abs{x-a}\lt \delta\) then \(\abs{f(x)-L}\lt \eps\).%
\end{definition}
Before we delve into this, notice that it is very similar to the definition of the continuity of \(f(x)\) at \(x=a\). In fact we can readily see that \(f \text{ is continuous at } x=a \text{ if and only if } \limit{x}{a}{f(x)} = f(a)\).%
\par
There are two differences between this definition and the definition of continuity and they are related.  The first is that we replace the value \(f(a)\) with \(L\).  This is because the function may not be defined at \(a\).  In a sense the limiting value \(L\) is the value \(f\) would have \emph{if it were defined and continuous at \(a\).} The second is that we have replaced%
\begin{equation*}
\abs{x-a}\lt \delta
\end{equation*}
with%
\begin{equation*}
0\lt \abs{x-a}\lt \delta\text{.}
\end{equation*}
%
\par
Again, since \(f\) needn't be defined at \(a\), we will not even consider what happens when \(x=a\). This is the only purpose for this change.%
\par
As with the definition of the limit of a sequence, this definition does not determine what \(L\) is, it only verifies that your guess for the value of the limit is correct.%
\par
Finally, a few comments on the differences and similiarities between this limit and the limit of a sequence are in order, if for no other reason than because we use the same notation (\(\lim\)) for both.%
\par
When we were working with sequences in \hyperref[x:chapter:Convergence]{Chapter~{\xreffont\ref{x:chapter:Convergence}}} and wrote things like \(\limit{n}{\infty}{a_n}\) we were thinking of \(n\) as an integer that got bigger and bigger.  To put that more mathematically, the limit parameter \(n\) was taken from the set of positive integers, or \(n\in \NN\).%
\par
For both continuity and the limit of a function we write things like \(\limit{x}{a}{f(x)}\) and think of \(x\) as a variable that gets arbitrarily close to the number \(a\).  Again, to be more mathematical in our language we would say that the limit parameter \(x\) is taken from the \(\ldots\) Well, actually, this is interesting isn't it?  Do we need to take \(x\) from \(\QQ\) or from \(\RR?\) The requirement in both cases is simply that we be able to choose \(x\) arbitrarily close to \(a\). From \hyperref[x:theorem:thm_IrrationalBetweenIrrationals]{Theorem~{\xreffont\ref{x:theorem:thm_IrrationalBetweenIrrationals}}} of \hyperref[x:chapter:NumbersRealRational]{Chapter~{\xreffont\ref{x:chapter:NumbersRealRational}}} we see that this is possible whether \(x\) is rational or not, so it seems either will work.  This leads to the pardoxical sounding conclusion that we do not need a continuum (\(\RR\)) to have continuity.  This seems strange.%
\par
Before we look at the above example, let's look at some algebraic examples to see the definition in use.%
\begin{example}{}{g:example:idp184}%
Consider the function \(D(x)=\frac{x^2-1}{x-1}\), \(x\neq 1\).  You probably recognize this as the difference quotient used to compute the derivative of \(f(x)=x^2\) at \(x=1\), so we strongly suspect that \(\limit{x}{1}{\frac{x^2-1}{x-1}}=2\).  Just as when we were dealing with limits of sequences, we should be able to use the definition to verify this.  And as before, we will start with some scrapwork.%
\par
\terminology{SCRAPWORK}%
\par
Let \(\eps>0\).  We wish to find a \(\delta>0\) such that if \(0\lt \abs{x-1}\lt \delta\) then \(\abs{\frac{x^2-1}{x-1}-2}\lt \eps\).  With this in mind, we perform the following calculations%
\begin{equation*}
\abs{\frac{x^2-1}{x-1}-2}=\abs{(x+1)-2} = \abs{x-1}\text{.}
\end{equation*}
%
\par
Now we have a handle on \(\delta\) that will work in the definition and we'll give the formal proof that%
\begin{equation*}
\limit{x}{1}{\frac{x^2-1}{x-1}}=2\text{.}
\end{equation*}
%
\end{example}
\begin{proof}{}{g:proof:idp185}
Let \(\eps>0\) and let \(\delta=\eps\). If \(0\lt \abs{x-1}\lt \delta\), then%
\begin{equation*}
\abs{\frac{x^2-1}{x-1}-2}=\abs{(x+1)-2}=\abs{x-1}\lt \delta=\eps\text{.}\qedhere
\end{equation*}
%
\end{proof}
As in our previous work with sequences and continuity, notice that the scrapwork is not part of the formal proof (though it was necessary to determine an appropriate \(\delta)\).  Also, notice that \(0\lt \abs{x-1}\) was not really used except to ensure that \(x\neq 1\).%
\begin{problem}{}{g:problem:idp186}%
Use the definition of a limit to verify that%
\begin{equation*}
\limit{x}{a}{\frac{x^2-a^2}{x-a}}=2a.{}
\end{equation*}
%
\end{problem}
\begin{problem}{}{g:problem:idp187}%
Use the definition of a limit to verify each of the following limits.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\(\limit{x}{1}{\frac{x^3-1}{x-1}}=3\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp188}{}\quad{}%
\begin{align*}
\abs{\frac{x^3-1}{x-1}-3} \amp = \abs{x^2+x+1-3}\\
\amp \leq\abs{x^2-1}+\abs{x-1}\\
\amp =\abs{(x-1+1)^2-1}+\abs{x-1}\\
\amp =\abs{(x-1)^2+2(x-1)}+\abs{x-1}\\
\amp \leq\abs{x-1}^2 + 3\abs{x-1}\text{.}
\end{align*}
%
\item[(b)]\(\limit{x}{1}{\frac{\sqrt{x}-1}{x-1}}=1/2\)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp189}{}\quad{}%
\begin{align*}
\abs{\frac{\sqrt{x}-1}{x-1}-\frac12}\amp = \abs{\frac{1}{\sqrt{x}+1}-\frac12}\\
\amp =\abs{\frac{2-\left(\sqrt{x}+1\right)}{2\left(\sqrt{x}+1\right)}}\\
\amp =\abs{\frac{1-x}{2\left(1+\sqrt{x}\right)^2}}\\
\amp \leq\frac12\abs{x-1}.{}
\end{align*}
%
\end{enumerate}
\end{problem}
Let's go back to the original problem: to show that \(\limit{x}{0}{\textstyle\frac{\sin x}{x}}=1\).%
\par
While rigorous, our definition of continuity is quite cumbersome. We really need to develop some tools we can use to show continuity rigorously without having to refer directly to the definition. We have already seen in \hyperref[x:theorem:thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{x:theorem:thm_LimDefOfContinuity}}} one way to do this. Here is another. The key is the observation we made after the definition of a limit:%
\begin{equation*}
f \text{ is continuous at } x=a \text{ if and only if } \limit{x}{a}{f(x)}=f(a)\text{.}
\end{equation*}
%
\par
Read another way, we could say that \(\limit{x}{a}{f(x)}=L\) provided that if we redefine \(f(a)=L\) (or define \(f(a)=L\) in the case where \(f(a)\) is not defined) then \(f\) becomes continuous at \(a\). This allows us to use all of the machinery we proved about continuous functions and limits of sequences.%
\par
For example, the following corollary to \hyperref[x:theorem:thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{x:theorem:thm_LimDefOfContinuity}}} comes virtually for free once we've made the observation above.%
\begin{corollary}{}{}{x:corollary:cor_limit-by-sequences}%
\(\limit{x}{a}{f(x)}=L\) if and only if \(f\) satisfies the following property:%
\begin{equation*}
\forall \text{ sequences }  (x_n), x_n\ne a, \text{ if } \limit{n}{\infty}{x_n}=a \text{ then }   \limit{n}{\infty}{f(x_n)}=L. {}
\end{equation*}
%
\end{corollary}
Armed with this, we can prove the following familiar limit theorems from calculus.%
\begin{theorem}{}{}{x:theorem:thm_CalcLimits}%
\index{limit!properties of} Suppose \(\limit{x}{a}{f(x)}=L\) and \(\limit{x}{a}{g(x)}=M\), then%
\begin{enumerate}[label=(\alph*)]
\item{}\(\displaystyle \limit{x}{a}{\left(f(x)+g(x)\right)}=L+M\)%
\item{}\(\displaystyle \limit{x}{a}{\left(f(x)\cdot g(x)\right)}=L\cdot M\)%
\item{}\(\limit{x}{a}{\left(\frac{f(x)}{g(x)}\right)}=L/M\) provided \(M\ne0\) and \(g(x)\ne{}0\), for \(x\) sufficiently close to a (but not equal to \(a\)).%
\end{enumerate}
%
\end{theorem}
We will prove part (a) to give you a feel for this and let you prove parts (b) and (c).%
\begin{proof}{}{g:proof:idp190}
Let \(\left(x_n\right)\) be a sequence such that \(x_n\ne
a\) and \(\limit{n}{\infty}{x_n}=a\).  Since \(\limit{x}{a}{f(x)} = L\) and \(\limit{x}{a}{g(x)} =
M\) we see that \(\limit{n}{\infty}{f(x_n)} = L\) and \(\limit{n}{\infty}{g(x_n)} = M\).  By \hyperref[x:theorem:thm_SumOfSequences]{Theorem~{\xreffont\ref{x:theorem:thm_SumOfSequences}}} of \hyperref[x:chapter:Convergence]{Chapter~{\xreffont\ref{x:chapter:Convergence}}}, we have \(\limit{n}{\infty}{f(x_n)+g(x_n)}=L+M\).  Since \(\left\{x_n\right\}\) was an arbitrary sequence with \(x_n\ne a\) and \(\limit{n}{\infty}{x_n} = a\) we have%
\begin{equation*}
\limit{x}{a}{f(x)+g(x)} = L+M\text{.}\qedhere
\end{equation*}
%
\end{proof}
\begin{problem}{}{g:problem:idp191}%
\index{limit!properties of}\index{verify limit laws from calculus} Prove parts (b) and (c) of \hyperref[x:theorem:thm_CalcLimits]{Theorem~{\xreffont\ref{x:theorem:thm_CalcLimits}}}.%
\end{problem}
More in line with our current needs, we have a reformulation of the Squeeze Theorem.%
\begin{theorem}{}{}{x:theorem:thm_SqueezeTheoremFunctions}%
\alert{Squeeze Theorem for functions}%
\par
\index{Squeeze Theorem!for functions} Suppose \(f(x)\le g(x) \le h(x)\), for \(x\) sufficiently close to \(a\) (but not equal to \(a\)). If \(\limit{x}{a}{f(x)}=L=\limit{x}{a}{h(x)}\), then \(\limit{x}{a}{g(x)}=L\) also.%
\end{theorem}
\begin{problem}{}{g:problem:idp192}%
\index{Squeeze Theorem!for functions} Prove \hyperref[x:theorem:thm_SqueezeTheoremFunctions]{Theorem~{\xreffont\ref{x:theorem:thm_SqueezeTheoremFunctions}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp193}{}\quad{}Use \hyperref[x:theorem:thm_SqueezeTheorem]{Theorem~{\xreffont\ref{x:theorem:thm_SqueezeTheorem}}}, the Squeeze Theorem for sequences from \hyperref[x:chapter:Convergence]{Chapter~{\xreffont\ref{x:chapter:Convergence}}}.%
\end{problem}
Returning to \(\limit{x}{0}{\frac{\sin x}{x}}\) we'll see that the Squeeze Theorem is just what we need.  First notice that since \(D(x)=\sin x/x\) is an even function, we only need to focus on \(x>0\) in our inequalities.  Consider the unit circle.%
\begin{image}{0.2}{0.6}{0.2}%
\includegraphics[width=\linewidth]{images/UnitCircle.png}
\end{image}%
\begin{problem}{}{g:problem:idp194}%
Use the fact that%
\begin{equation*}
\text{ area } (\Delta OAC)\lt \text{ area } (\text{ sector } OAC)\lt \text{ area } (\Delta OAB)
\end{equation*}
to show that if \(0\lt x\lt \pi/2\), then \(\cos x\lt
\sin x/x\lt 1\).  Use the fact that all of these functions are even to extend the inequality for \(-\pi/2\lt x\lt
0\) and use the Squeeze Theorem to show \(\limit{x}{0}{\textstyle\frac{\sin x}{x}}=1\).%
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 9.4 The Derivative, An Afterthought}
\typeout{************************************************}
%
\begin{sectionptx}{The Derivative, An Afterthought}{}{The Derivative, An Afterthought}{}{}{x:section:Continuity-DerivativeAfterthought}
No, the derivative isn't really an afterthought.  Along with the integral it is, in fact, one of the most powerful and useful mathematical objects ever devised and we've been working very hard to provide a solid, rigorous foundation for it.  In that sense it is a primary focus of our investigations.%
\par
On the other hand, now that we have built up all of the machinery we need to define and explore the concept of the derivative it will appear rather pedestrian alongside ideas like the convergence of power series, Fourier series, and the bizarre properties of \(\QQ\) and \(\RR\).%
\par
You spent an entire semester learning about the properties of the derivative and how to use them to explore the properties of functions so we will not repeat that effort here.  Instead we will define it formally in terms of the ideas and techniques we've developed thus far.%
\begin{definition}{The Derivative.}{x:definition:def_derivative}%
\index{derivative}%
\index{differentiation}%
Given a function \(f(x)\) defined on an interval \((a,b)\) we define%
\begin{equation*}
f^\prime(x) = \limit{h}{0}{\frac{f(x+h)-f(x)}{h}}.{}
\end{equation*}
%
\end{definition}
There are a few fairly obvious facts about this definition which are nevertheless worth noticing explicitly:%
\par
%
\begin{enumerate}
\item{}The derivative is \emph{defined at a point.} If it is defined at every point in an interval \((a,b)\) then we say that the derivative exists at every point on the interval.%
\item{}Since it is defined at a point it is at least theoretically possible for a function to be differentiable at a single point in its entire domain.%
\item{}Since it is defined as a limit and not all limits exist, functions are not necessarily differentiable.%
\item{}Since it is defined as a limit, \hyperref[x:corollary:cor_limit-by-sequences]{Corollary~{\xreffont\ref{x:corollary:cor_limit-by-sequences}}} applies. That is, \(f^\prime(x)\) exists if and only if \(\forall \text{ sequences } (h_n),\, h_n\ne 0\), if \(\limit{n}{\infty}{h_n}=0\) then%
\begin{equation*}
f^\prime{(x)} =
\limit{n}{\infty}{\frac{f(x+h_n)-f(x)}{h_n}} \text{.}
\end{equation*}
Since \(\limit{n}{\infty}{h_n}=0\) this could also be written as%
\begin{equation*}
f^\prime{(x)} = \limit{h_n}{0}{\frac{f(x+h_n)-f(x)}{h_n}}\text{.}
\end{equation*}
%
\end{enumerate}
%
\begin{theorem}{}{}{x:theorem:thm_DiffImpCont}%
\index{continuity!implied by differentiability}%
\index{differentiation!differentiability implies continuity}%
\alert{Differentiability Implies Continuity}%
\par
If \(f\) is differentiable at a point \(c\) then \(f\) is continuous at \(c\) as well.%
\end{theorem}
\begin{problem}{}{g:problem:idp195}%
Prove \hyperref[x:theorem:thm_DiffImpCont]{Theorem~{\xreffont\ref{x:theorem:thm_DiffImpCont}}}%
\end{problem}
As we mentioned, the derivative is an extraordinarily useful mathematical tool but it is not our intention to learn to \emph{use} it here.  Our purpose here is to define it rigorously (done) and to show that our formal definition does in fact recover the useful properties you came to know and love in your calculus course.%
\par
The first such property is known as Fermat's Theorem.%
\begin{theorem}{Fermat's Theorem.}{}{x:theorem:thm_FermatsTheorem}%
\index{Fermat's Theorem}%
Suppose \(f\) is differentiable in some interval \((a,b)\) containing \(c\).  If \(f(c)\ge f(x)\) for every \(x\) in \((a,b)\), then \(f^\prime(c)=0\).%
\end{theorem}
\begin{proof}{}{g:proof:idp196}
Since \(f^\prime(c)\) exists we know that if \(\left(h_n\right)_{n=1}^\infty\) converges to zero then the sequence \(a_n = \frac{f\left(c+h_n\right)-f(c)}{h_n}\) converges to \(f^\prime(c)\).  The proof consists of showing that \(f^\prime(c)\leq 0\) \emph{and} that \(f^\prime(c)\geq
0\) from which we conclude that \(f^\prime(c)= 0\).  We will only show the first part.  The second is left as an exercise.%
\par
\emph{Claim:} \(f^\prime(c)\leq 0\).%
\par
Let \(n_0\) be sufficiently large that \(\frac{1}{n_0}\lt b-c\) and take \(\left(h_n\right)=\left(\frac{1}{n}\right)_{n=n_0}^\infty\). Then \(f\left(c+\frac1n\right)-f(c) \leq 0\) and \(\frac1n>0\), so that%
\begin{equation*}
\frac{f\left(c+h_n\right)-f(c)}{h_n}\leq 0, \ \ \forall n=n_0, n_0+1, \ldots
\end{equation*}
%
\par
Therefore \(f^\prime(c) = \limit{h_n}{0}{\frac{f\left(c+h_n\right)-f(c)}{h_n}} \leq 0\) also.%
\end{proof}
\begin{problem}{}{g:problem:idp197}%
\index{Fermat's Theorem!if \(f(a)\) is a maximum then \(f^\prime(a)=0\)} Show that \(f^\prime(c) \geq 0\) and conclude that \(f^\prime(c) =0\).%
\end{problem}
\begin{problem}{}{g:problem:idp198}%
\index{Fermat's Theorem!if \(f(a)\) is a minimum then \(f^\prime(a)=0\)} Show that if \(f(c) \leq f(x)\) for all \(x\) in some interval \((a,b)\) then \(f^\prime(c) =0\) too.%
\end{problem}
Many of the most important properties of the derivative follow from what is called the Mean Value Theorem (\terminology{MVT}) which we now state.%
\begin{theorem}{}{}{x:theorem:thm_MVT}%
\terminology{The Mean Value Theorem}%
\par
\index{Mean Value Theorem, the} Suppose \(f^\prime\) exists for every \(x\in(a,b)\) and \(f\) is continuous on \([a,b]\). Then there is a real number \(c\in(a,b)\) such that%
\begin{equation*}
f^\prime(c)=\frac{f(b)-f(a)}{b-a}.{}
\end{equation*}
%
\end{theorem}
However, it would be difficult to prove the MVT right now. So we will first state and prove Rolle's Theorem, which can be seen as a special case of the MVT. The proof of the MVT will then follow easily.%
\par
Michel Rolle first stated the following theorem in 1691. Given this date and the nature of the theorem it would be reasonable to suppose that Rolle was one of the early developers of calculus but this is not so. In fact, Rolle was disdainful of both Newton \index{Newton, Isaac} and Leibniz's\index{Leibniz, Gottfried Wilhelm} versions of calculus, once deriding them as a collection of ``ingenious fallacies.'' It is a bit ironic that his theorem is so fundamental to the modern development of the calculus he ridiculed.%
\begin{theorem}{}{}{x:theorem:thm_Rolle_s_Theorem}%
\index{Rolle's Theorem}%
\terminology{Rolle's Theorem}%
\par
Suppose \(f^\prime\) exists for every \(x\in(a,b)\), \(f\) is continuous on \([a,b]\), and%
\begin{equation*}
f(a)=f(b)\text{.}
\end{equation*}
%
\par
Then there is a real number \(c\in(a,b)\) such that%
\begin{equation*}
f^\prime(c)=0\text{.}
\end{equation*}
%
\end{theorem}
\begin{proof}{}{g:proof:idp199}
Since \(f\) is continuous on \([a,b]\) we see, by the Extreme Value Theorem,\index{Extreme Value Theorem (EVT)!Rolle's Theorem, and}\footnote{Any proof that relies on the Extreme Value Theorem is not complete until the EVT has been proved. We'll get to this in \hyperref[x:chapter:IVTandEVT]{Chapter~{\xreffont\ref{x:chapter:IVTandEVT}}}.\label{g:fn:idp200}} that \(f\) has both a maximum and a minimum on \([a,b]\). Denote the maximum by \(M\) and the minimum by \(m\). There are several cases:%
\begin{description}
\item[{Case 1:}]\(f(a)=f(b)=M=m\). In this case \(f(x)\) is constant (why?). Therefore \(f^\prime(x)=0\) for every \(x\in(a,b)\).%
\item[{Case 2:}]\(f(a)=f(b)=M\neq m\). In this case there is a real number \(c\in(a,b)\) such that \(f(c)\) is a local minimum. By Fermat's Theorem, \(f^\prime(c)=0\).%
\item[{Case 3:}]\(f(a)=f(b)=m\neq M\). In this case there is a real number \(c\in(a,b)\) such that \(f(c)\) is a local maximum. By Fermat's Theorem, \(f^\prime(c)=0\).%
\item[{Case 4:}]\(f(a)=f(b)\) is neither a maximum nor a minimum. In this case there is a real number \(c_1\in(a,b)\) such that \(f(c_1)\) is a local maximum, and a real number \(c_2\in(a,b)\) such that \(f(c_2)\) is a local minimum. By Fermat's Theorem, \(f^\prime(c_1)=f^\prime(c_2)=0\).%
\end{description}
%
\end{proof}
With Rolle's Theorem in hand we can prove the MVT which is really a corollary to Rolle's Theorem or, more precisely, it is a generalization of Rolle's Theorem. To prove it we only need to find the right function to apply Rolle's Theorem to. The following figure shows a function, \(f(x)\), cut by a secant line, \(L(x)\), from \((a, f(a))\) to \((b,f(b))\).%
\begin{image}{0.22}{0.56}{0.22}%
\includegraphics[width=\linewidth]{images/MVT.png}
\end{image}%
The vertical difference from \(f(x)\) to the secant line, indicated by \(\phi(x)\) in the figure should do the trick. You take it from there.%
\begin{problem}{}{g:problem:idp201}%
\index{Mean Value Theorem, the} Prove the Mean Value Theorem.%
\end{problem}
The Mean Value Theorem is extraordinarily useful. Almost all of the properties of the derivative that you used in calculus follow more or less easily from it. For example the following is true.%
\begin{corollary}{}{}{x:corollary:cor_PosDerivIncFunc1}%
If \(f^\prime(x) > 0\) for every \(x\) in the interval \((a,b)\) then for every \(c,d\in(a,b)\) where \(d>c\) we have%
\begin{equation*}
f(d) > f(c) \text{.}
\end{equation*}
%
\par
That is, \(f\) is increasing on \((a,b)\).%
\end{corollary}
\begin{proof}{}{g:proof:idp202}
Suppose \(c\) and \(d\) are as described in the corollary. Then by the Mean Value Theorem there is some number, say \(\alpha\in(c,d)\subseteq(a,b)\) such that%
\begin{equation*}
f^\prime(\alpha)=\frac{f(d)-f(c)}{d-c}\text{.}
\end{equation*}
%
\par
Since \(f^\prime(\alpha)>0\) and \(d-c>0\) we have \(f(d)-f(c)>0\), or \(f(d)>f(c)\).%
\end{proof}
\begin{problem}{}{g:problem:idp203}%
\index{differentiation!positive implies increasing}\index{if \(f^\prime\lt 0\) on an interval then \(f\) is decreasing} Show that if \(f^\prime(x) \lt 0\) for every \(x\) in the interval \((a,b)\) then \(f\) is decreasing on \((a,b)\).%
\end{problem}
\begin{corollary}{}{}{x:corollary:cor_PosDerivIncFunc2}%
Suppose \(f\) is differentiable on some interval \((a,b)\), \(f^\prime\) is continuous on \((a,b)\), and that \(f^\prime(c)>0\) for some \(c\in (a,b)\). Then there is an interval, \(I\subset (a,b)\), containing \(c\) such that for every \(x,
y\) in \(I\) where \(x\ge y\), \(f(x)\ge f(y)\).%
\end{corollary}
\begin{problem}{}{g:problem:idp204}%
\index{differentiation!\(f^\prime(a)>0\) implies \(f\) is increasing nearby} Prove \hyperref[x:corollary:cor_PosDerivIncFunc2]{Corollary~{\xreffont\ref{x:corollary:cor_PosDerivIncFunc2}}}.%
\end{problem}
\begin{problem}{}{g:problem:idp205}%
\index{differentiation!\(f^\prime(a)\lt 0\) implies \(f\) is decreasing nearby} Show that if \(f\) is differentiable on some interval \((a,b)\) and that \(f^\prime(c)\lt 0\) for some \(c\in (a,b)\) then there is an interval, \(I\subset (a,b)\), containing \(c\) such that for every \(x, y\) in \(I\) where \(x\le y\), \(f(x)\le f(y)\).%
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 9.5 Additional Problems}
\typeout{************************************************}
%
\begin{sectionptx}{Additional Problems}{}{Additional Problems}{}{}{x:section:Continuity-AddProb}
\begin{problem}{}{g:problem:idp206}%
Use the definition of continuity to prove that the constant function \(g(x)=c\) is continuous at any point \(a\).%
\end{problem}
\begin{problem}{}{g:problem:idp207}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Use the definition of continuity to prove that \(\ln x\) is continuous at \(1\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp208}{}\quad{}You may want to use the fact \(\abs{\ln x}\lt \eps\,\Leftrightarrow-\eps\lt \ln x\lt \eps\) to find a \(\delta\).%
\item[(b)]Use part (a) to prove that \(\ln x\) is continuous at any positive real number \(a\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp209}{}\quad{}\(\ln(x)=\ln(x/a)+\ln(a)\). This is a combination of functions which are continuous at \(a\). Be sure to explain how you know that \(\ln(x/a)\) is continuous at \(a\).%
\end{enumerate}
\end{problem}
\begin{problem}{}{g:problem:idp210}%
\index{continuity!formal definition of discontinuity} Write a formal definition of the statement \(f\) is not continuous at \(a\), and use it to prove that the function \(f(x)= \begin{cases}x\amp \text{ if } x\neq 1\\ 0\amp \text{ if } x=1 \end{cases}\) is not continuous at \(a=1\).%
\end{problem}
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Chapter 10 Intermediate and Extreme Values}
\typeout{************************************************}
%
\begin{chapterptx}{Intermediate and Extreme Values}{}{Intermediate and Extreme Values}{}{}{x:chapter:IVTandEVT}
%
%
\typeout{************************************************}
\typeout{Section 10.1 Completeness of the Real Number System}
\typeout{************************************************}
%
\begin{sectionptx}{Completeness of the Real Number System}{}{Completeness of the Real Number System}{}{}{x:section:IVTandEVT-Completeness}
Recall that in deriving the Lagrange and Cauchy forms of the remainder for Taylor series, we made use of the Extreme Value Theorem (\terminology{EVT}) and Intermediate Value Theorem (\terminology{IVT}). In \hyperref[x:chapter:Continuity]{Chapter~{\xreffont\ref{x:chapter:Continuity}}}, we produced an analytic definition of continuity that we can use to prove these theorems. To provide the rest of the necessary tools we need to explore the make-up of the real number system. To illustrate what we mean, suppose that we only used the rational number system. We could still use our definition of continuity and could still consider continuous functions such as \(f(x)=x^2\). Notice that \(2\) is a value that lies between \(f(1)=1\)and \(f(2)=4\).%
\begin{image}{0.315}{0.37}{0.315}%
\includegraphics[width=\linewidth]{images/Ch6fig1.png}
\end{image}%
The IVT says that somewhere between \(1\) and \(2\), \(f\) must take on the value \(2\). That is, there must exist some number \(c\in[1,2]\) such that \(f(c)=2\). You might say, ``Big deal! Everyone knows \(c=\sqrt{2}\) works.''%
\par
However, we are only working with rational numbers and \(\sqrt{2}\) \(\)is not rational. As we saw in \hyperref[x:chapter:NumbersRealRational]{Chapter~{\xreffont\ref{x:chapter:NumbersRealRational}}} the rational number system has holes in it, whereas the real number system doesn't. Again, ``Big deal! Let's just say that the real number system contains (square) roots.''%
\par
This sounds reasonable and it actually works for square roots, but consider the function \(f(x)=x-\cos x\). We know this is a continuous function. We also know that \(f(0)=-1\) and \(f(\frac{\pi}{2})=\frac{\pi}{2}\). According to the IVT, there should be some number \(c\in[\,0,\frac{\pi}{2}]\), where \(f(c)=0\). The graph is below.%
\begin{image}{0.315}{0.37}{0.315}%
\includegraphics[width=\linewidth]{images/Ch6fig2.png}
\end{image}%
The situation is not as transparent as before. What would this mysterious \(c\) be where the curve crosses the \(x\) axis? Somehow we need to convey the idea that the real number system is a continuum. That is, it has no ``holes'' in it.%
\par
How about this? Why don't we just say that it has no holes in it? Sometimes the simple answer works best! But not in this case. How are we going to formulate a rigorous proof based on this statement? Just like with convergence and continuity, what we need is a rigorous way to convey this idea that the real number system does not have any holes, that it is \emph{complete.}%
\par
We will see that there are several different, but equivalent ways to convey this notion of completeness. We will explore some of them in this chapter. For now we adopt the following as our \emph{Completeness Axiom} for the real number system.%
\begin{axiom}{Nested Interval Property of the Real Number System (\terminology{NIP}).}{}{g:axiom:idp211}%
Suppose we have two sequences of real numbers \(\left(x_n\right)\)and \(\left(y_n\right)\) satisfying the following conditions:%
\par
%
\begin{enumerate}
\item{}\(x_1\leq x_2\leq x_3\leq\ldots\) (this says that the sequence, \(\left(x_n\right)\), is non-decreasing)%
\item{}\(y_1\geq y_2\geq y_3\geq\ldots\) (this says that the sequence, \(\left(y_n\right)\), is non-increasing) %
\item{}\(\forall\) \(n\), \(x_n\leq y_n\)%
\item{}\(\displaystyle \lim_{n\rightarrow\infty}\left(y_n-x_n\right)=0\)%
\end{enumerate}
%
\par
Then there exists a unique number \(c\) such that \(x_n\leq
c\leq y_n\) for all \(n\).%
\end{axiom}
Geometrically, we have the following situation.%
\begin{image}{0.125}{0.75}{0.125}%
\includegraphics[width=\linewidth]{images/Ch6fig3.png}
\end{image}%
Notice that we have two sequences \(\left(x_n\right)\) and \(\left(y_n\right)\), one increasing (really non-decreasing) and one decreasing (non-increasing). These sequences do not pass each other. In fact, the following is true:%
\begin{problem}{}{g:problem:idp212}%
Let \((x_n), (y_n)\) be sequences as in the NIP. Show that for all \(n, m \in\NN\), \(x_n\le y_m\).%
\end{problem}
They are also coming together in the sense that \(\lim_{n\rightarrow\infty}\left(y_n-x_n\right)=0\).  The NIP says that in this case there is a unique real number \(c\) in the middle of all of this \textbackslash{}lbrack \(x_n\leq c\leq y_n\) for all \(n\)\textbackslash{}rbrack.%
\begin{image}{0.125}{0.75}{0.125}%
\includegraphics[width=\linewidth]{images/Ch6fig4.png}
\end{image}%
If there was no such \(c\) then there would be a hole where these two sequences come together.  The NIP guarantees that there is no such hole.  We do not need to prove this since an axiom is, by definition, a self evident truth.  We are taking it on faith that the real number system obeys this law.  The next problem shows that the completeness property distinguishes the real number system from the rational number system.%
\begin{problem}{}{g:problem:idp213}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Find two sequences of rational numbers \(\left(x_n\right)\)and \(\left(y_n\right)\) which satisfy properties 1-4 of the NIP and such that there is no rational number \(c\) satisfying the conclusion of the NIP.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp214}{}\quad{}Consider the decimal expansion of an irrational number.%
\item[(b)]Find two sequences of rational numbers \(\left(x_n\right)\) and \(\left(y_n\right)\) which satisfy properties 1-4 of the NIP and such that there is a rational number \(c\) satisfying the conclusion of the NIP.%
\end{enumerate}
\end{problem}
You might find the name ``Nested Interval Property'' to be somewhat curious. One way to think about this property is to consider that we have a sequence of ``nested closed intervals'' \([\,x_1,y_1]\supseteq[\,x_2,y_2]\supseteq[\,x_3,y_3]\supseteq\cdots\) whose lengths \(\,y_n-x_n\) are ``shrinking to \(0\).'' The conclusion is that the intersection of these intervals is non-empty and, in fact, consists of a single point. That is, \(\bigcap_{n=1}^\infty[x_n,y_n]=\{c\}\).%
\par
It appears that the sequences \(\left(x_n\right)\) and \(\left(y_n\right)\) in the NIP converge to \(c\). This is, in fact, true and can be proven rigorously. In what follows, this will prove to be a valuable piece of information.%
\begin{theorem}{}{}{x:theorem:thm_ConvergeToC}%
\index{Nested Interval Property (NIP)!endpoints}\index{limit!of interval endpoints in the NIP} Suppose that we have two sequences \(\left(x_n\right)\) and \(\left(y_n\right)\) satisfying all of the assumptions of the Nested Interval Property. If \(c\) is the unique number such that \(x_n\leq c\leq y_n\) for all \(n\), then \(\lim_{n\rightarrow\infty}x_n=c\) and \(\lim_{n\rightarrow\infty}y_n=c\).%
\end{theorem}
\begin{problem}{}{g:problem:idp215}%
\index{Nested Interval Property (NIP)!endpoints}\index{in the NIP the endpoints converge as sequences} Prove \hyperref[x:theorem:thm_ConvergeToC]{Theorem~{\xreffont\ref{x:theorem:thm_ConvergeToC}}}.%
\end{problem}
To illustrate the idea that the NIP ``plugs the holes'' in the real line, we will prove the existence of square roots of nonnegative real numbers.%
\begin{theorem}{}{}{x:theorem:thm_SqrtsExist}%
\index{Problem!square roots exist}%
Suppose \(a\in\mathbb{R},\,a\geq 0\). There exists a real number \(c\geq 0\) such that \(c^2=a\).%
\end{theorem}
Notice that we can't just say, ``Let \(c=\sqrt{a}\),'' since the idea is to show that this square root exists. In fact, throughout this proof, we cannot really use a square root symbol as we haven't yet proved that they (square roots) exist. We will give the idea behind the proof as it illustrates how the NIP is used.%
\begin{proof}{Sketch of Proof.}{g:proof:idp216}
Our strategy is to construct two sequences which will ``narrow in'' on the number \(c\) that we seek. With that in mind, we need to find a number \(x_1\) such that \(x_1^2\leq a\) and a number \(y_1\) such that \(y_1^2\geq a\). (Remember that we can't say \(\sqrt{x_1}\) or\(\sqrt{y_1}\).) There are many possibilities, but how about \(x_1=0\) and \(y_1=a+1?\) You can check that these will satisfy \(x_1^2\leq a\leq\) \(y_1^2\). Furthermore \(x_1\leq y_1\). This is the starting point.%
\par
The technique we will employ is often called a bisection technique, and is a useful way to set ourselves up for applying the NIP. Let \(m_1\) be the midpoint of the interval \([\,x_1,y_1]\). Then either we have \(m_1^2\leq a\) or \(m_1^2\geq a\). In the case \(m_1^2\leq a\), we really want \(m_1\) to take the place of \(x_1\) since it is larger than \(\,x_1\), but still represents an underestimate for what would be the square root of \(a\). This thinking prompts the following move. If \(m_1^2\leq a\), we will relabel things by letting \(x_2=m_1\) and \(y_2=y_1\). The situation looks like this on the number line.%
\begin{image}{0.22}{0.56}{0.22}%
\includegraphics[width=\linewidth]{images/Ch6fig5.png}
\end{image}%
In the other case where \(a\leq m_1^2\), we will relabel things by letting \(x_2=x_1\) and \(y_2=m_1\). The situation looks like this on the number line.%
\begin{image}{0.22}{0.56}{0.22}%
\includegraphics[width=\linewidth]{images/Ch6fig6.png}
\end{image}%
In either case, we've reduced the length of the interval where the square root lies to half the size it was before. Stated in more specific terms, in either case we have the same results:%
\begin{equation*}
x_1\leq x_2\leq y_2\leq y_1;\ \  x_1^2\leq a\leq y_1^2;\ \  x_2^2\leq a\leq y_2^2
\end{equation*}
and%
\begin{equation*}
y_2-x_2=\frac{1}{2}\left(y_1-x_1\right)\text{.}
\end{equation*}
%
\par
Now we play the same game, but instead we start with the interval \([x_2,y_2]\). Let \(m_2\) be the midpoint of \([x_2,y_2]\). Then we have \(m_2^2\leq a\) or \(m_2^2\geq a\). If \(m_2^2\leq a\), we relabel \(x_3=m_2\) and \(y_3=y_2\). If \(a\leq m_2^2\), we relabel \(x_3=x_2\) and \(y_3=m_2\). In either case, we end up with%
\begin{equation*}
x_1\leq x_2\leq x_3\leq y_3\leq y_2\leq y_1;\ \   x_1^2\leq a\leq y_1^2;\ \  x_2^2\leq a\leq y_2^2;\ \   x_3^2\leq a\leq y_3^2
\end{equation*}
and%
\begin{equation*}
y_3-x_3=\frac{1}{2}\left(y_2-x_2\right)=\frac{1}{2^2}\left(y_1-x_1\right)\text{.}
\end{equation*}
%
\par
Continuing in this manner, we will produce two sequences, \(\left(x_n\right)\)and \(\left(y_n\right)\) satisfying the following conditions:%
\begin{enumerate}
\item{}\(\displaystyle x_1\leq x_2\leq x_3\leq\ldots\)%
\item{}\(\displaystyle y_1\geq y_2\geq y_3\geq\ldots\)%
\item{}\(\forall\) \(n\), \(x_n\leq y_n\)%
\item{}\(\displaystyle \lim_{n\rightarrow\infty}\left(y_n-x_n\right)=\,\lim_{n\rightarrow\infty}\frac{1}{2^{n-1}}\left(y_1-x_1\right)=0\)%
\item{}These sequences also satisfy the following property:%
\begin{equation*}
\forall n,\,x_n^2\leq a\leq y_n^2
\end{equation*}
%
\end{enumerate}
%
\par
Properties 1-4 tell us that \(\left(x_n\right)\)and \(\left(y_n\right)\) satisfy all of the conditions of the NIP, so we can conclude that there must exist a real number \(c\) such that \(x_n\leq c\leq y_n\) for all \(n\). At this point, you should be able to use property 5. to show that \(c^2=a\) as desired.%
\end{proof}
\begin{problem}{}{g:problem:idp217}%
\index{Nested Interval Property (NIP)!square roots of integers, and}\index{Nested Interval Property (NIP)!implies the existence of square roots of integers} Turn the above outline into a formal proof of \hyperref[x:theorem:thm_SqrtsExist]{Theorem~{\xreffont\ref{x:theorem:thm_SqrtsExist}}}.%
\end{problem}
The bisection method we employed in the proof of \hyperref[x:theorem:thm_SqrtsExist]{Theorem~{\xreffont\ref{x:theorem:thm_SqrtsExist}}} is pretty typical of how we will use the NIP, as taking midpoints ensures that we will create a sequence of ``nested intervals.'' We will employ this strategy in the proofs of the IVT and EVT. Deciding how to relabel the endpoints of our intervals will be determined by what we want to do with these two sequences of real numbers. This will typically lead to a fifth property, which will be crucial in proving that the \(c\) guaranteed by the NIP does what we want it to do. Specifically, in the above example, we always wanted our candidate for \(\sqrt{a}\) to be in the interval \([\,x_n,y_n]\). This judicious choice led to the extra Property~5: \(\forall\) \(n,\,x_n^2\leq a\leq y_n^2\). In applying the NIP to prove the IVT and EVT, we will find that properties 1-4 will stay the same. Property~5 is what will change based on the property we want \(c\) to have.%
\par
Before we tackle the IVT and EVT, let's use the NIP to address an interesting question about the Harmonic Series. \index{series!slow divergence of the Harmonic Series} Recall that the Harmonic Series, \(1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots\), grows without bound, that is, \(\sum_{n=1}^\infty\frac{1}{n}=\infty\).  The question is how slowly does this series grow?  For example, how many terms would it take before the series surpasses 100? 1000? 10000?  Leonhard Euler \index{Euler, Leonhard} decided to tackle this problem in the following way.  Euler decided to consider the \(\lim_{n\rightarrow\infty}\left(\left(1+\frac{1}{2}+\frac{1}{3}+\cdots+
\frac{1}{n}\right)-\text{ ln } \left(n+1\right)\right)\).  This limit is called Euler's constant and is denoted by \(\gamma\). This says that for \(n\) large, we have \(1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{n}\approx\)ln\(\left(n+1\right)+\gamma\). If we could approximate \(\gamma\), then we could replace the inequality \(1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{n}\geq
100\) with the more tractable inequality ln\(\left(n+1\right)+\gamma\) \(\geq 0\) and solve for \(n\) in this.  This should tell us roughly how many terms would need to be added in the Harmonic Series to surpass 100. Approximating \(\gamma\) with a computer is not too bad.  We could make \(n\) as large as we wish in \(\left(1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{n}\right)-\)ln\(\left(1+n\right)\) to make closer approximations for \(\gamma\).  The real issue is, \alert{HOW DO WE KNOW THAT}%
\begin{equation*}
\lim_{n\rightarrow\infty}\left(\left(1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{n}\right)-\text{ ln } \left(\text{n} +1\right)\right)
\end{equation*}
\alert{ACTUALLY EXISTS?}%
\par
You might want to say that obviously it should, but let us point out that as of the printing of this book (2013), it is not even known if \(\gamma\) is rational or irrational. So, in our opinion the existence of this limit is not so obvious. This is where the NIP will come into play; we will use it to show that this limit, in fact, exists. The details are in the following problem.%
\begin{problem}{}{g:problem:idp218}%
The purpose of this problem is to show that%
\begin{equation*}
\lim_{n\rightarrow\infty}\left(\left(1+\frac{1}{2}+\frac{1}{3}+\cdots+ \frac{1}{n}\right)-\ln\left(n+1\right)\right)
\end{equation*}
exists.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Let \(x_n=\left(1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{n}\right)-\ln\left(n+1 \right)\). Use the following diagram to show%
\begin{equation*}
x_1\leq x_2\leq x_3\leq\cdots
\end{equation*}
%
\begin{image}{0.08}{0.84}{0.08}%
\includegraphics[width=\linewidth]{images/Ch6fig7.png}
\end{image}%
\item[(b)]Let \(z_n=\ln\left(n+1\right)-\left(\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{n+1} \right)\). Use a similar diagram to show that \(z_1\leq z_2\leq z_3\leq\cdots\).%
\item[(c)]Let \(y_n=1-z_n\).  Show that \(\left(x_n\right)\) and \(\left(y_n\right)\) satisfy the hypotheses of the nested interval property and use the NIP to conclude that there is a real number \(\gamma\) such that \(x_n\leq\gamma\leq
y_n\) for all \(n\).%
\item[(d)]Conclude that \(\lim_{n\rightarrow\infty}\left(\left(1+\frac{1}{2}+\frac{1}{3}+\cdots+
\frac{1}{n}\right)-\ln\left(n+1\right)\right)=\gamma\).%
\end{enumerate}
\end{problem}
\begin{problem}{}{g:problem:idp219}%
Use the fact that \(x_n\leq\gamma\leq y_n\) for all \(n\) to approximate \(\gamma\) to three decimal places.%
\end{problem}
\begin{problem}{}{g:problem:idp220}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Use the fact that for large \(n\), \(1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{n}\approx
\ln\left(n+1\right)+ \gamma\) to determine approximately how large \(n\) must be to make%
\begin{equation*}
1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{n}\geq 100\text{.}
\end{equation*}
%
\item[(b)]Suppose we have a supercomputer which can add 10 trillion terms of the Harmonic Series per second. Approximately how many earth lifetimes would it take for this computer to sum the Harmonic Series until it surpasses 100?%
\end{enumerate}
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 10.2 Proof of the Intermediate Value Theorem}
\typeout{************************************************}
%
\begin{sectionptx}{Proof of the Intermediate Value Theorem}{}{Proof of the Intermediate Value Theorem}{}{}{x:section:IVTandEVT-ProofOfIVT}
We now have all of the tools to prove the Intermediate Value Theorem (\terminology{IVT}).%
\begin{theorem}{}{}{x:theorem:IntermediateValueTheorem}%
\terminology{Intermediate Value Theorem}%
\par
\index{Intermediate Value Theorem (IVT)} Suppose \(f(x)\) is continuous on \([a,b]\) and \(v\) is any real number between \(f(a)\) and \(f(b)\). Then there exists a real number \(c\in[\,a,b]\) such that \(f(c)=v\).%
\end{theorem}
\begin{proof}{Sketch of Proof.}{g:proof:idp221}
We have two cases to consider: \(f(a)\leq v\leq f(b)\) and \(f(a)\geq v\geq f(b)\).%
\par
We will look at the case \(f(a)\leq v\leq f(b)\). Let \(x_1=a\) and \(y_1=b\), so we have \(x_1\leq y_1\) and \(f(x_1)\leq v\leq f(y_1)\). Let \(\,m_1\) be the midpoint of \([\,x_1,y_1]\) and notice that we have either \(f(m_1)\leq v\) or \(f(m_1)\geq v\). If \(f(m_1)\leq v\) , then we relabel \(x_2=m_1\) and \(y_2=y_1\). If \(f(m_1)\geq v\) , then we relabel \(x_2=x_1\) and \(y_2=m_1\). In either case, we end up with \(x_1\leq x_2\leq y_2\leq y_1\), \(y_2-x_2=\frac{1}{2}\left(y_1-x_1\right)\), \(f(x_1)\leq v\leq f(y_1)\), and \(f(x_2)\leq v\leq f(y_2)\).%
\par
Now play the same game with the interval \([\,x_2,y_2]\). If we keep playing this game, we will generate two sequences \(\left(x_n\right)\) and \(\left(y_n\right)\), satisfying all of the conditions of the nested interval property. These sequences will also satisfy the following extra property: \(\forall\) \(n,\,f(x_n)\leq v\leq f(y_n)\). By the NIP, there exists a \(c\) such that \(\,x_n\leq c\leq y_n\), \(\forall\) \(n\). This should be the \(c\) that we seek though this is not obvious. Specifically, we need to show that \(f(c)=v\). This should be where the continuity of \(f\) at \(c\) and the extra property on \(\left(x_n\right)\)and \(\left(y_n\right)\) come into play.%
\end{proof}
\begin{problem}{}{g:problem:idp222}%
\index{Intermediate Value Theorem (IVT)!the case \(f(a)\leq v\leq f(b)\)} Turn the ideas of the previous paragraphs into a formal proof of the IVT for the case \(f(a)\leq v\leq f(b)\).%
\end{problem}
\begin{problem}{}{g:problem:idp223}%
\index{Intermediate Value Theorem (IVT)!the case \(f(a)\geq v\geq f(b)\)} We can modify the proof of the case \(f(a)\leq v\leq f(b)\) into a proof of the IVT for the case \(f(a)\geq v\geq f(b)\). However, there is a sneakier way to prove this case by applying the IVT to the function \(-f\). Do this to prove the IVT for the case \(f(a)\geq v\geq f(b)\).%
\end{problem}
\begin{problem}{}{g:problem:idp224}%
\index{polynomials!with odd degree must have a root}\index{a polynomial with odd degree must have a root} Use the IVT to prove that any polynomial of odd degree must have a real root.%
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 10.3 The Bolzano-Weierstrass Theorem}
\typeout{************************************************}
%
\begin{sectionptx}{The Bolzano-Weierstrass Theorem}{}{The Bolzano-Weierstrass Theorem}{}{}{x:section:IVTandEVT-BWT}
Once we introduced the Nested Interval Property, the Intermediate Value Theorem followed pretty readily. The proof of Extreme Value (which says that any continuous function \(f\)defined on a closed interval \([\,a,b]\) must have a maximum and a minimum) takes a bit more work. First we need to show that such a function is bounded.%
\begin{theorem}{}{}{x:theorem:thm_CompactBounded}%
\index{continuous functions!continuous function on a closed, bounded interval is bounded} A continuous function defined on a closed, bounded interval must be bounded.  That is, let \(f\) be a continuous function defined on \([\,a,b]\).  Then there exists a positive real number \(B\) such that \(|f(x)|\leq B\) for all \(x\in[\,a,b]\).%
\end{theorem}
\alert{Sketch of Alleged Proof:} Let's assume, for contradiction, that there is no such bound \(B\). This says that for any positive integer \(n\), there must exist \(x_n\in[a,b]\) such that \(\abs{f(x_n)}>n\). (Otherwise \(n\) would be a bound for \(f\).) \alert{IF} the sequence \(\left(x_n\right)\) converged to something in \([\,a,b]\), say \(c\), then we would have our contradiction. Indeed, we would have \(\lim_{n\rightarrow\infty}x_n=c\). By the continuity of \(f\) at \(c\) and \hyperref[x:theorem:thm_LimDefOfContinuity]{Theorem~{\xreffont\ref{x:theorem:thm_LimDefOfContinuity}}} of \hyperref[x:chapter:Continuity]{Chapter~{\xreffont\ref{x:chapter:Continuity}}}, we would have \(\lim_{n\rightarrow\infty}f(x_n)=f(c)\). This would say that the sequence \(\left(f(x_n)\right)\) converges, so by \hyperref[x:lemma:lemma_BoundedConvergent]{Lemma~{\xreffont\ref{x:lemma:lemma_BoundedConvergent}}} of \hyperref[x:chapter:Convergence]{Chapter~{\xreffont\ref{x:chapter:Convergence}}}, it must be bounded. This would provide our contradiction, as we had \(|f(x_n)|>n\), for all positive integers \(n\). \alert{QED?}%
\par
This would all work well except for one little problem. The way it was constructed, there is no reason to expect the sequence \(\left(x_n\right)\) to converge to anything and we can't make such an assumption. That is why we emphasized the \alert{IF} above. Fortunately, this idea can be salvaged. While it is true that the sequence \(\left(x_n\right)\) may not converge, part of it will. We will need the following definition.%
\begin{definition}{}{x:definition:def_subsequences}%
\index{sequences!subsequences} Let \(\left(n_k\right)_{k=1}^\infty\) be a strictly increasing sequence of positive integers; that is, \(n_1\lt n_2\lt n_3\lt \cdots\) . If \(\left(x_n\right)_{n=1}^\infty\)is a sequence, then \(\left(x_{n_k}\right)_{k=1}^\infty=\left(x_{n_1},x_{n_2},x_{n_3},\ldots \right)\) is called a \textbraceleft{}subsequence\textbraceright{} of \(\left(x_n\right)\).%
\end{definition}
The idea is that a subsequence of a sequence is a part of the sequence, \((x_n)\), which is itself a sequence. However, it is a little more restrictive. We can choose any term in our sequence to be part of the subsequence, but once we choose that term, we can't go backwards. This is where the condition \(n_1\lt n_2\lt n_3\lt \cdots\) comes in. For example, suppose we started our subsequence with the term \(x_{100}\). We could not choose our next term to be \(x_{99}\). The subscript of the next term would have to be greater than 100. In fact, the thing about a subsequence is that it is all in the subscripts; we are really choosing a subsequence \(\left(n_k\right)\) of the sequence of subscripts \(\left(n\right)\) in \(\left(x_n\right)\).%
\begin{example}{}{g:example:idp225}%
Given the sequence \(\left(x_n\right)\), the following are subsequences.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\(\left(x_2,\,x_4,\,x_6,\,\ldots\right)=\left(x_{2k}\right)_{k=1}^\infty\)%
\item[(b)]\(\left(x_1,\,x_4,\,x_9,\,\ldots\right)=\left(x_{k^2}\right)_{k=1}^ \infty\)%
\item[(c)]\(\left(x_n\right)\) itself.%
\end{enumerate}
\end{example}
\begin{example}{}{g:example:idp226}%
The following are NOT subsequences.%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]\(\left(x_1,\,x_1,\,x_1,\,\ldots\right)\)%
\item[(b)]\(\left(x_{99},\,x_{100},\,x_{99},\,\ldots\right)\)%
\item[(c)]\(\left(x_1,\,x_2,\,x_3\right)\)%
\end{enumerate}
\end{example}
The subscripts in the examples we have seen so far have a discernable pattern, but this need not be the case. For example,%
\begin{equation*}
\left(x_2,\,x_5,\,x_{12},\,x_{14},\,x_{23},\,\ldots\right)
\end{equation*}
would be a subsequence as long as the subscripts form an increasing sequence themselves.%
\begin{problem}{}{g:problem:idp227}%
\index{sequences!all subsequences of a convergent sequence converge} Suppose \(\lim_{n\rightarrow\infty}x_n=c\). Prove that \(\lim_{k\rightarrow\infty}x_{n_k}=c\) for any subsequence \(\left(x_{n_k}\right)\) of \(\left(x_n\right)\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp228}{}\quad{}\(n_k\geq k\).%
\end{problem}
A very important theorem about subsequences was introduced by Bernhard Bolzano \index{Bolzano, Bernhard} and, later, independently proven \index{Weierstrass, Karl} by Karl Weierstrass. Basically, this theorem says that any bounded sequence of real numbers has a convergent subsequence.%
\begin{theorem}{}{}{x:theorem:BolzanoWeierstrass}%
\alert{The Bolzano-Weierstrass Theorem}%
\par
\index{Bolzano-Weierstrass Theorem} Let \(\left(x_n\right)\) be a sequence of real numbers such that \(x_n\in[\,a,b]\), \(\forall\) \(n\).  Then there exists \(c\in[\,a,b]\) and a subsequence \(\left(x_{n_k}\right)\), such that \(\lim_{k\rightarrow\infty}x_{n_k}=c\).%
\end{theorem}
As an example of this theorem, consider the sequence%
\begin{equation*}
\left((-1)^n\right)=\left(-1,1,-1,1,\ldots\right) \text{.}
\end{equation*}
%
\par
This sequence does not converge, but the subsequence%
\begin{equation*}
\left((-1)^{2k}\right)=\left(1,1,1,\ldots\right)
\end{equation*}
converges to \(1\). This is not the only convergent subsequence, as \(\left((-1)^{2k+1}\right)=(-1,-1,-1,\,\ldots)\) converges to \(-1\). Notice that if the sequence is unbounded, then all bets are off; the sequence may have a convergent subsequence or it may not. The sequences \(\left(\left((-1)^n+1\right)n\right)\) and \(\left(n\right)\) represent these possibilities as the first has, for example, \(\left(\left((-1)^{2k+1}+1\right)(2k+1)\right)=(0,0,0,\,\ldots)\) and the second one has none.%
\par
The Bolzano-Weierstrass Theorem says that no matter how ``random'' the sequence \(\left(x_n\right)\) may be, as long as it is bounded then some part of it must converge. This is very useful when one has some process which produces a ``random'' sequence such as what we had in the idea of the alleged proof in \hyperref[x:theorem:thm_CompactBounded]{Theorem~{\xreffont\ref{x:theorem:thm_CompactBounded}}}.%
\begin{proof}{Sketch of a Proof of the Bolzano-Weierstrass Theorem.}{g:proof:idp229}
Suppose we have our sequence \(\left(x_n\right)\) such that \(x_n\in[a,b]\), \(\forall\) \(n\). To find our \(c\) for the subsequence to converge to we will use the NIP. Since we are already using \(\left(x_n\right)\) as our original sequence, we will need to use different letters in setting ourselves up for the NIP. With this in mind, let \(a_1=a\) and \(b_1=b\), and notice that \(x_n\in[a_1,b_1]\) for infinitely many \(n\). (This is, in fact true for all \(n\), but you'll see why we said it the way we did.) Let \(m_1\) be the midpoint of \([a_1,b_1]\) and notice that either \(x_n\in[a_1,m_1]\) for infinitely many \(n\) or \(x_n\in[m_1,b_1]\) for infinitely many \(n\). If \(x_n\in[a_1,m_1]\) for infinitely many \(n\), then we relabel \(a_2=a_1\) and \(b_2=m_1\). If \(x_n\in[m_1,b_1]\) for infinitely many \(n\), then relabel \(a_2=m_1\) and \(b_2=b_1\). In either case, we get \(a_1\leq a_2\leq b_2\leq b_1\), \(b_2-a_2=\frac{1}{2}\left(b_1-a_1\right)\), and \(x_n\in[a_2,b_2]\) for infinitely many \(n\).%
\par
Now we consider the interval \([a_2,b_2]\) and let \(m_2\) be the midpoint of \([a_2,b_2]\). Since \(x_n\in[a_2,b_2]\) for infinitely many \(n\), then either \(x_n\in[a_2,m_2]\) for infinitely many \(n\) or \(x_n\in[m_2,b_2]\) for infinitely many \(n\). If \(x_n\in[a_2,m_2]\) for infinitely many \(n\), then we relabel \(a_3=a_2\) and \(b_3=m_2\). If \(x_n\in[m_2,b_2]\) for infinitely many \(n\), then we relabel \(a_3=m_2\) and \(b_3=b_2\). In either case, we get \(a_1\leq a_2\leq a_3\leq b_3\leq b_2\leq b_1\), \(b_3-a_3=\frac{1}{2}\left(b_2-a_2\right)=\frac{1}{2^2}\left(b_1-a_1\right)\), and \(x_n\in[a_3,b_3]\) for infinitely many \(n\).%
\par
If we continue in this manner, we will produce two sequences \(\left(a_k\right)\)and \(\left(b_k\right)\) with the following properties:%
\begin{enumerate}
\item{}\(\displaystyle a_1\leq a_2\leq a_3\leq\cdots\)%
\item{}\(\displaystyle b_1\geq b_2\geq b_3\geq\cdots\)%
\item{}\(\forall\) \(k,\,a_k\leq b_k\)%
\item{}\(\displaystyle \displaystyle\lim_{k\rightarrow\infty}\left(b_k-a_k\right)=\,\lim_{k\rightarrow\infty} \frac{1}{2^{k-1}}\left(b_1-a_1\right)=0\)%
\item{}For each \(k\), \(x_n\in[\,a_k,b_k]\) for infinitely many \(n\)%
\end{enumerate}
%
\par
By properties 1-5 and the NIP, there exists a unique \(c\) such that \(c\in[a_k,b_k]\), for all \(k\). In particular, \(c\in[a_1,b_1]=[a,b]\).%
\par
We have our \(c\). Now we need to construct a subsequence converging to it. Since \(x_n\in[a_1,b_1]\) for infinitely many \(n\), choose an integer \(n_1\) such that \(x_{n_1}\in[a_1,b_1]\). Since \(x_n\in[a_2,b_2]\) for infinitely many \(n\), choose an integer \(n_2>n_1\) such that \(x_{n_2}\in[a_2,b_2]\). (Notice that to make a subsequence it is crucial that \(n_2>n_1\), and this is why we needed to insist that \(x_n\in[a_2,b_2]\) for infinitely many \(n\).) Continuing in this manner, we should be able to build a subsequence \(\left(x_{n_k}\right)\) that will converge to \(c\). You can supply the details in the following problem.%
\end{proof}
\begin{problem}{}{g:problem:idp230}%
\index{Bolzano-Weierstrass Theorem (BWT)} Turn the ideas of the above outline into a formal proof of the Bolzano-Weierstrass Theorem.%
\end{problem}
\begin{problem}{}{g:problem:idp231}%
\index{Bolzano-Weierstrass Theorem (BWT)!implies that a  continuous functions on a closed set is bounded}\index{continuous functions!on a closed set, and the Bolzano-Weierstrass Theorem}\index{continuity!Bolzano-Weierstrass Theorem implies a continuous function on a closed set is bounded} Use the Bolzano-Weierstrass Theorem to complete the proof of \hyperref[x:theorem:thm_CompactBounded]{Theorem~{\xreffont\ref{x:theorem:thm_CompactBounded}}}.%
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 10.4 The Supremum and the Extreme Value Theorem}
\typeout{************************************************}
%
\begin{sectionptx}{The Supremum and the Extreme Value Theorem}{}{The Supremum and the Extreme Value Theorem}{}{}{x:section:IVTandEVT-SupremumAndEVT}
\hyperref[x:theorem:thm_CompactBounded]{Theorem~{\xreffont\ref{x:theorem:thm_CompactBounded}}} says that a continuous function on a closed, bounded interval must be bounded. Boundedness, in and of itself, does not ensure the existence of a maximum or minimum. We must also have a closed, bounded interval. To illustrate this, consider the continuous function \(f(x)=\)tan\(^{-1}x\) defined on the (unbounded) interval \(\left(-\infty,\infty\right)\).%
\begin{image}{0.125}{0.75}{0.125}%
\includegraphics[width=\linewidth]{images/Ch6fig8.png}
\end{image}%
This function is bounded between \(-\frac{\pi}{2}\)and \(\frac{\pi}{2}\), but it does not attain a maximum or minimum as the lines \(y=\pm\frac{\pi}{2}\) are horizontal asymptotes. Notice that if we restricted the domain to a closed, bounded interval then it would attain its extreme values on that interval (as guaranteed by the \terminology{EVT}).%
\par
To find a maximum we need to find the smallest possible upper bound for the range of the function. This prompts the following definitions.%
\begin{definition}{}{x:definition:def_UpperBound}%
\index{Upper Bound} Let \(S\subseteq\mathbb{R}\) and let \(b\) be a real number. We say that \(b\) is an upper bound of \(S\) provided \(b\geq x\) for all \(x\in S\).%
\end{definition}
For example, if \(S=(0,1)\), then any \(b\) with \(b\geq 1\) would be an upper bound of \(S\). Furthermore, the fact that \(b\) is not an element of the set \(S\) is immaterial. Indeed, if \(T=[\,0,1]\), then any \(b\) with \(b\geq 1\) would still be an upper bound of \(T\). Notice that, in general, if a set has an upper bound, then it has infinitely many since any number larger than that upper bound would also be an upper bound. However, there is something special about the smallest upper bound.%
\begin{definition}{Least Upper Bound Property (\terminology{LUBP}).}{x:definition:def_LeastUpperBound}%
\index{Least Upper Bound Property (LUBP)}%
Let \(S\subseteq\mathbb{R}\) and let \(b\) be a real number.  We say that \(b\) is the \emph{least upper bound} of \(S\) provided%
\par
%
\begin{enumerate}[label=(\alph*)]
\item{}\(b\geq x\) for all \(x\in S\). (\(b\) is an upper bound of \(S\))%
\item{}If \(c\geq x\) for all \(x\in S\), then \(c\geq b\). (Any upper bound of \(S\) is at least as big as \(b\).)%
\end{enumerate}
%
\par
In this case, we also say that \(b\) \emph{is the supremum} of \(S\) and we write%
\begin{equation*}
b=\sup\left(S\right)\text{.}
\end{equation*}
%
\end{definition}
Notice that the definition really says that \(b\) is the smallest upper bound of \(S\). Also notice that the second condition can be replaced by its contrapositive so we can say that \(b=\sup S\) if and only if%
\begin{enumerate}[label=(\alph*)]
\item{}\lititle{(i).}\par%
\(\displaystyle b\,\geq\,x\text{ for all } x\,\in S\)%
\item{}\lititle{(ii).}\par%
If \(c\,\lt \,b\) then there exists \(x\,\in S\) such that \(c\,\lt \,x\).%
\end{enumerate}
%
\par
The second condition says that if a number \(c\) is less than \(b\), then it can't be an upper bound, so that \(b\) really is the smallest upper bound.%
\par
Also notice that the supremum of the set may or may not be in the set itself. This is illustrated by the examples above as in both cases, \(1=\sup(0,1)\) and \(1=\sup [0,1]\). Obviously, a set which is not bounded above such as \(\mathbb{N}=\{1,\,2,\,3,\,\ldots\}\) cannot have a supremum. However, for non-empty sets which are bounded above, we have the following.%
\begin{theorem}{The Least Upper Bound Property (\terminology{LUBP}).}{}{x:theorem:thm_LUB}%
\index{Least Upper Bound Property (LUBP)}%
Let \(S\) be a non-empty subset of \(\mathbb{R}\) which is bounded above. Then \(S\) has a supremum.%
\end{theorem}
\begin{proof}{Sketch of Proof.}{g:proof:idp232}
Since \(S\neq\emptyset\), then there exists \(s\in S\). Since \(S\) is bounded above then it has an upper bound, say \(b\). We will set ourselves up to use the Nested Interval Property. With this in mind, let \(x_1=s\) and \(y_1=b\) and notice that \(\exists\) \(x\in S\) such that \(x\geq x_1\) (namely, \(x_1\) itself) and \(\forall\,x\in S\), \(y_1\geq x\). You probably guessed what's coming next: let \(m_1\)be the midpoint of \([\,x_1,y_1]\). Notice that either \(m_1\geq x,\,\forall\,x\in S\) or \(\exists\) \(x\in S\) such that \(x\geq m_1\). In the former case, we relabel, letting \(x_2=x_1\) and \(y_2=m_1\). In the latter case, we let \(x_2=m_1\) and \(y_2=y_1\). In either case, we end up with \(x_1\leq x_2\leq y_2\leq y_1\), \(y_2-x_2=\frac{1}{2}\left(y_1-x_1\right)\), and \(\exists\) \(x\in S\) such that \(x\geq x_2\) and \(\forall\,x\in S\), \(y_2\geq x\). If we continue this process, we end up with two sequences, \(\left(x_n\right)\)and \(\left(y_n\right)\), satisfying the following conditions:%
\begin{enumerate}
\item{}\(\displaystyle x_1\leq x_2\leq x_3\leq\ldots\)%
\item{}\(\displaystyle y_1\geq y_2\geq y_3\geq\ldots\)%
\item{}\(\forall\) \(n\), \(x_n\leq y_n\)%
\item{}\(\displaystyle \lim_{n\rightarrow\infty}\left(y_n-x_n\right)=\lim_{n\rightarrow\infty} \frac{1}{2^{n-1}}\left(y_1-x_1\right)=0\)%
\item{}\(\forall\) \(n,\exists\) \(x\in S\) such that \(x\geq x_n\) and \(\forall\,x\in S\), \(y_n\geq x\),%
\end{enumerate}
%
\par
By properties 1-5 and the NIP there exists \(c\) such that \(x_n\leq c\leq y_n,\,\forall\,n\). We will leave it to you to use property 5 to show that \(c=\sup S\).%
\end{proof}
\begin{problem}{}{g:problem:idp233}%
\index{Nested Interval Property (NIP)!implies the LUBP} Complete the above ideas to provide a formal proof of \hyperref[x:theorem:thm_LUB]{Theorem~{\xreffont\ref{x:theorem:thm_LUB}}}.%
\end{problem}
Notice that we really used the fact that \(S\) was non-empty and bounded above in the proof of \hyperref[x:theorem:thm_LUB]{Theorem~{\xreffont\ref{x:theorem:thm_LUB}}}. This makes sense, since a set which is not bounded above cannot possibly have a least upper bound. In fact, any real number is an upper bound of the empty set so that the empty set would not have a least upper bound.%
\par
The following corollary to \hyperref[x:theorem:thm_LUB]{Theorem~{\xreffont\ref{x:theorem:thm_LUB}}} can be very useful.%
\begin{corollary}{}{}{x:corollary:cor_IncBoundedConverge}%
Let \((x_n)\) be a bounded, increasing sequence of real numbers. That is, \(x_1\leq x_2\leq x_3\leq\cdots\). Then \((x_n)\) converges to some real number \(c\).%
\end{corollary}
\begin{problem}{}{g:problem:idp234}%
\index{sequences!bounded and non-decreasing}\index{a bounded, non-decreasing sequence converges} Prove \hyperref[x:corollary:cor_IncBoundedConverge]{Corollary~{\xreffont\ref{x:corollary:cor_IncBoundedConverge}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp235}{}\quad{}Let \(c=\sup\{x_n|\,n=1,2,3,\ldots\}\). To show that \(\limit{n}{\infty}{x_n}=c\), let \(\epsilon>0.\)Note that \(c-\epsilon\) is not an upper bound. You take it from here!%
\end{problem}
\begin{problem}{}{g:problem:idp236}%
\index{\(\sqrt{2+\sqrt{2+\sqrt{2+\sqrt{...}}}}\)!value of} Consider the following curious expression \(\sqrt{2+\sqrt{2+\sqrt{2+\sqrt{...}}}}\). We will use \hyperref[x:corollary:cor_IncBoundedConverge]{Corollary~{\xreffont\ref{x:corollary:cor_IncBoundedConverge}}} to show that this actually converges to some real number. After we know it converges we can actually compute what it is. Of course to do so, we need to define things a bit more precisely. With this in mind consider the following sequence \(\left(x_n\right)\) defined as follows:%
\begin{equation*}
x_1=\sqrt{2}
\end{equation*}
%
\begin{equation*}
x_{n+1}=\sqrt{2+x_n}\text{.}
\end{equation*}
%
\begin{enumerate}[label=(\alph*)]
\item{}Use induction to show that \(x_n\lt 2\) for \(n=1,\,2,\,3,\,\ldots\).%
\item{}Use the result from part (a) to show that \(x_n\lt x_{n+1}\) for \(n=1,\,2,\,3,\,\ldots\) .%
\item{}From \hyperref[x:corollary:cor_IncBoundedConverge]{Corollary~{\xreffont\ref{x:corollary:cor_IncBoundedConverge}}}, we have that \(\left(x_n\right)\) must converge to some number \(c\). Use the fact that \(\left(x_{n+1}\right)\) must converge to \(c\) as well to compute what \(c\) must be.%
\end{enumerate}
%
\end{problem}
We now have all the tools we need to tackle the Extreme Value Theorem.%
\begin{theorem}{}{}{x:theorem:thm_EVT}%
\alert{Extreme Value Theorem (\terminology{EVT})}%
\par
\index{Extreme Value Theorem (EVT)} Suppose \(f\) is continuous on \([\,a,b]\). Then there exists \(c,d\in[\,a,b]\) such that \(f(d)\leq f(x)\leq f(c)\), for all \(x\in[\,a,b]\).%
\end{theorem}
\begin{proof}{Sketch of Proof.}{g:proof:idp237}
We will first show that \(f\) attains its maximum. To this end, recall that \hyperref[x:theorem:thm_CompactBounded]{Theorem~{\xreffont\ref{x:theorem:thm_CompactBounded}}} tells us that \(f[\,a,b]=\{f(x)|\,x\in[\,a,b]\}\) is a bounded set. By the LUBP, \(f[\,a,b]\) must have a least upper bound which we will label \(s\), so that \(s=\sup f[\,a,b]\). This says that \(s\geq f(x)\),for all \(x\in[\,a,b]\). All we need to do now is find a \(c\in[\,a,b]\) with \(f(c)=s\). With this in mind, notice that since \(s=\sup f[\,a,b]\), then for any positive integer \(n\), \(s-\frac{1}{n}\) is not an upper bound of \(f[\,a,b]\). Thus there exists \(x_n\in[\,a,b]\) with\(\,s-\frac{1}{n}\lt f(x_n)\leq s\). Now, by the Bolzano-Weierstrass Theorem, \(\left(x_n\right)\) has a convergent subsequence\(\,\left(x_{n_k}\right)\) converging to some \(c\in[\,a,b]\). Using the continuity of \(f\) at \(c\), you should be able to show that \(f(c)=s\). To find the minimum of \(f\), find the maximum of \(-f\).%
\end{proof}
\begin{problem}{}{g:problem:idp238}%
\index{Extreme Value Theorem (EVT)} Formalize the above ideas into a proof of \hyperref[x:theorem:thm_EVT]{Theorem~{\xreffont\ref{x:theorem:thm_EVT}}}.%
\end{problem}
Notice that we used the NIP to prove both the Bolzano-Weierstrass Theorem and the LUBP. This is really unavoidable, as it turns out that all of those statements are equivalent in the sense that any one of them can be taken as the completeness axiom for the real number system and the others proved as theorems. This is not uncommon in mathematics, as people tend to gravitate toward ideas that suit the particular problem they are working on. In this case, people realized at some point that they needed some sort of completeness property for the real number system to prove various theorems. Each individual's formulation of completeness fit in with his understanding of the problem at hand. Only in hindsight do we see that they were really talking about the same concept: the completeness of the real number system. In point of fact, most modern textbooks use the LUBP as the axiom of completeness and prove all other formulations as theorems. We will finish this section by showing that either the Bolzano-Weierstrass Theorem or the LUBP can be used to prove the NIP. This says that they are all equivalent and that any one of them could be taken as the completeness axiom.%
\begin{problem}{}{g:problem:idp239}%
\index{Bolzano-Weierstrass Theorem (BWT)!implies the NIP} Use the Bolzano-Weierstrass Theorem to prove the NIP. That is, assume that the Bolzano-Weierstrass Theorem holds and suppose we have two sequences of real numbers, \(\left(x_n\right)\) and \(\left(y_n\right)\), satisfying:%
\begin{enumerate}
\item{}\(\displaystyle x_1\le x_2 \le x_3 \le \ldots\)%
\item{}\(\displaystyle y_1\ge y_2 \ge y_3 \ge \ldots\)%
\item{}\(\displaystyle \forall\ n,\ x_n\le y_n\)%
\item{}\(\displaystyle\lim_{n\rightarrow\infty}\left(y_n-x_n\right) = 0\).%
\end{enumerate}
%
\par
Prove that there is a real number \(c\) such that \(x_n\le c\le y_n\), for all \(n\).%
\end{problem}
Since the Bolzano-Weierstrass Theorem and the Nested Interval Property are equivalent, it follows that the Bolzano-Weierstrass Theorem will not work for the rational number system.%
\begin{problem}{}{g:problem:idp240}%
\index{find a bounded sequence of rational numbers such that no subsequence converges to a rational number} Find a bounded sequence of rational numbers such that no subsequence of it converges to a rational number.%
\end{problem}
\begin{problem}{}{g:problem:idp241}%
\index{Least Upper Bound Property (LUBP)!implies the Nested Interval Property (NIP)} Use the Least Upper Bound Property to prove the Nested Interval Property. That is, assume that every non-empty subset of the real numbers which is bounded above has a least upper bound; and suppose that we have two sequences of real numbers \(\left(x_n\right)\) and \(\left(y_n\right)\), satisfying:%
\begin{enumerate}
\item{}\(\displaystyle x_1\le x_2 \le x_3 \le \ldots\)%
\item{}\(\displaystyle y_1\ge y_2 \ge y_3 \ge \ldots\)%
\item{}\(\displaystyle \forall\ n, x_n\le y_n\)%
\item{}\(\displaystyle\lim_{n\rightarrow\infty}\left(y_n-x_n\right) = 0\).%
\end{enumerate}
%
\par
Prove that there exists a real number \(c\) such that \(x_n\le c\le y_n\), for all n. (Again, the \(c\) will, of necessity, be unique, but don't worry about that.)%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp242}{}\quad{}\hyperref[x:corollary:cor_IncBoundedConverge]{Corollary~{\xreffont\ref{x:corollary:cor_IncBoundedConverge}}} might work well here.%
\end{problem}
\begin{problem}{}{g:problem:idp243}%
\index{Least Upper Bound Property (LUBP)!doesn't hold in \(\QQ\)} Since the LUBP is equivalent to the NIP it does not hold for the rational number system. Demonstrate this by finding a non-empty set of rational numbers which is bounded above, but whose supremum is an irrational number.%
\end{problem}
We have the machinery in place to clean up a matter that was introduced in \hyperref[x:chapter:NumbersRealRational]{Chapter~{\xreffont\ref{x:chapter:NumbersRealRational}}}. If you recall (or look back) we introduced the Archimedean Property of the real number system. This property says that given any two positive real numbers \(a, b\), there exists a positive integer \(n\) with \(na>b\). As we mentioned in \hyperref[x:chapter:NumbersRealRational]{Chapter~{\xreffont\ref{x:chapter:NumbersRealRational}}}, this was taken to be intuitively obvious. The analogy we used there was to emptying an ocean \(b\) with a teaspoon \(a\) provided we are willing to use it enough times \(n\). The completeness of the real number system allows us to prove it as a formal theorem.%
\begin{theorem}{}{}{x:theorem:thm_ArchmedeanProperty}%
\index{Archimedean Property}%
\alert{Archimedean Property of \(\RR\)}%
\par
Given any positive real numbers \(a\) and \(b\), there exists a positive integer \(n\), such that \(na>b\).%
\end{theorem}
\begin{problem}{}{g:problem:idp244}%
Prove \hyperref[x:theorem:thm_ArchmedeanProperty]{Theorem~{\xreffont\ref{x:theorem:thm_ArchmedeanProperty}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp245}{}\quad{}Assume that there are positive real numbers \(a\) and \(b\), such that \(na\le b\) \(\forall n\in \NN\). Then \(\NN\) would be bounded above by \(b/a\). Let \(s=\sup(\NN)\) and consider \(s-1\).%
\end{problem}
Given what we've been doing, one might ask if the Archimedean Property is equivalent to the LUBP (and thus could be taken as an axiom). The answer lies in the following problem.%
\begin{problem}{}{g:problem:idp246}%
\index{Archimedean Property!and \(\QQ\)}  Does \(\QQ\) satisfy the Archimedean Property and what does this have to do with the question of taking the Archimedean Property as an axiom of completeness?%
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 10.5 Additional Problems}
\typeout{************************************************}
%
\begin{sectionptx}{Additional Problems}{}{Additional Problems}{}{}{x:section:IVTandEVT-AddProb}
\begin{problem}{}{g:problem:idp247}%
\index{Least Upper Bound Property (LUBP)!definition of the Greatest Lower Bound (GLB)} Mimic the definitions of an upper bound of a set and the least upper bound (supremum) of a set to give definitions for a lower bound of a set and the greatest lower bound (infimum) of a set.%
\par
\alert{Note}: The infimum of a set \(S\) is denoted by \(\inf(S)\).%
\end{problem}
\begin{problem}{}{g:problem:idp248}%
\index{Least Upper Bound Property (LUBP)!identifying suprema and infima} Find the least upper bound (supremum) and greatest lower bound (infimum) of the following sets of real numbers, if they exist. (If one does not exist then say so.)%
\begin{enumerate}[label=(\alph*)]
\item{}\(\displaystyle S=\{\frac{1}{n}\,|\,n=1,2,3,\ldots\}\)%
\item{}\(T=\{r\,|\,r\) is rational and \(r^2\lt 2\}\)%
\item{}\(\displaystyle (-\infty,0)\cup(1,\infty)\)%
\item{}\(\displaystyle R=\{\frac{(-1)^n}{n}\,|\,n=1,2,3,\ldots\}\)%
\item{}\(\displaystyle (2,3\pi]\cap\QQ\)%
\item{}The empty set \(\emptyset\)%
\end{enumerate}
%
\end{problem}
\begin{problem}{}{g:problem:idp249}%
\index{Least Upper Bound Property (LUBP)}\index{\(b\) is an upper bound of \(S\subseteq \RR\) if and only if \(-b\) is a lower bound of \(-S\)} Let \(S\subseteq\RR\) and let \(T=\{-x|\,x\in S\}\).%
\begin{enumerate}[label=(\alph*)]
\item{}Prove that \(b\) is an upper bound of \(S\) if and only if \(-b\) is a lower bound of \(T\).%
\item{}Prove that \(b=\sup S\) if and only if \(-b=\inf T\).%
\end{enumerate}
%
\end{problem}
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Chapter 11 Back to Power Series}
\typeout{************************************************}
%
\begin{chapterptx}{Back to Power Series}{}{Back to Power Series}{}{}{x:chapter:PowerSeriesRedux}
%
%
\typeout{************************************************}
\typeout{Section 11.1 Uniform Convergence}
\typeout{************************************************}
%
\begin{sectionptx}{Uniform Convergence}{}{Uniform Convergence}{}{}{x:section:PowerSeriesRedux-UnifConv}
We have developed precise analytic definitions of the convergence of a sequence and continuity of a function and we have used these to prove the EVT and IVT for a continuous function. We will now draw our attention back to the question that originally motivated these definitions, ``Why are Taylor series well behaved, but Fourier series are not necessarily?'' More precisely, we mentioned that whenever a power series converges then whatever it converged to was continuous. Moreover, if we differentiate or integrate these series term by term then the resulting series will converge to the derivative or integral of the original series. This was not always the case for Fourier series. For example consider the function%
\begin{align*}
f(x) \amp = \frac{4}{\pi}\left(\sum_{k=0}^\infty\frac{(-1)^k}{2k+1}\cos\left((2k+1)\pi x\right)\right)\\
\amp = \frac{4}{\pi}\left(\cos(\pi x)-\frac13\cos(3\pi x)+\frac15 (5\pi x)-\ldots\right)\text{.}
\end{align*}
%
\par
We have seen that the graph of \(f\) is given by%
\begin{image}{0.075}{0.85}{0.075}%
\includegraphics[width=\linewidth]{images/Ch7fig1.png}
\end{image}%
If we consider the following sequence of functions%
\begin{align*}
f_1(x)=\amp \frac{4}{\pi}\cos\left(\pi x\right)\\
f_2(x)=\amp \frac{4}{\pi}\left(\cos \left(\pi x\right)-\frac{1}{3}\cos\left( 3\pi x\right)\right)\\
f_3(x)=\amp \frac{4}{\pi}\left(\cos\left(\pi x\right)-\frac{1}{3}\cos\left(3\pi x\right)+\frac{1}{5}\cos\left(5\pi x\right)\right)\\
\amp \vdots
\end{align*}
we see the sequence of continuous functions \(\left(f_n\right)\) converges to the non-continuous function \(f\) for each real number \(x\). This didn't happen with Taylor series. The partial sums for a Taylor series were polynomials and hence continuous but what they converged to was continuous as well.%
\par
The difficulty is quite delicate and it took mathematicians a while to determine the problem. There are two very subtly different ways that a sequence of functions can converge: pointwise or uniformly. This \index{Abel, Niels Henrik} distinction was touched upon by Niels Henrik Abel (1802-1829) in 1826 while studying the domain of convergence of a power series. However, the necessary formal definitions were not made explicit until Weierstrass \index{Weierstrass, Karl} did it in his 1841 paper \textit{Zur Theorie der Potenzreihen} \emph{(On the Theory of Power Series)}. This was published in his collected works in 1894.%
\par
It will be instructive to take a look at an argument that doesn't quite work before looking at the formal definitions we will need. In 1821 Augustin Cauchy \index{Cauchy, Augustin} ``proved'' that the infinite sum of continuous functions is continuous. Of course, it is obvious (to us) that this is not true because we've seen several counterexamples. But Cauchy, who was a first rate mathematician was so sure of the correctness of his argument that he included it in his textbook on analysis, \textit{Cours d'analyse} (1821).%
\begin{problem}{}{x:problem:prob_Cauchy_s_incorrect_proof}%
\index{Cauchy, Augustin!Cauchy's flawed proof that the limit of continuous functions is continuous}\index{continuity!Cauchy's flawed proof that the limit of continuous functions is continuous} Find the flaw in the following ``proof'' that \(f\) is also continuous at \(a\).%
\par
Suppose \(f_1, f_2, f_3, f_4 \ldots\) are all continuous at \(a\) and that \(\sum_{n=1}^\infty f_n=f\). Let \(\eps>0\). Since \(f_n\) is continuous at \(a\), we can choose \(\delta_n>0\) such that if \(\abs{x-a}\lt \delta_n\), then \(\abs{f_n(x)-f_n(a)}\lt \frac{\eps}{2^n}\). Let \(\delta=\inf(\delta_1,\delta_2,\delta_3,\ldots)\). If \(\abs{x-a}\lt \delta\) then%
\begin{align*}
\abs{f(x)-f(a)} \amp =  \abs{\sum_{n=1}^\infty f_n(x)  -  \sum_{n=1}^\infty f_n(a) }\\
\amp = \abs{\sum_{n=1}^\infty \left(f_n(x)-f_n(a)\right) }\\
\amp \le \sum_{n=1}^\infty \abs{f_n(x)-f_n(a) }\\
\amp \le \sum_{n=1}^\infty \frac{\eps}{2^n}\\
\amp \le \eps\sum_{n=1}^\infty \frac{1}{2^n}\\
\amp =   \eps\text{.}
\end{align*}
%
\par
Thus \(f\) is continuous at \(a\).%
\end{problem}
\begin{definition}{}{x:definition:def_PointwiseConvergence}%
\index{convergence!of a series!pointwise} Let \(S\) be a subset of the real number system and let \(\left(f_n\right)=\left(f_1,f_2,f_3,\,\ldots\right)\) be a sequence of functions defined on \(S\). Let \(f\) be a function defined on \(S\) as well. We say that  \(\left(f_n\right)\) \alert{converges to \(f\) pointwise on \(S\)} provided that for all \(x\in S\), the sequence of real numbers \(\left(f_n(x)\right)\) converges to the number \(f(x)\). In this case we write\(\,f_n\ptwise f\) on \(S\).%
\end{definition}
Symbolically, we have \(f_n\ptwise f\text{ on } S\Leftrightarrow \forall\,x\in S,\forall\ \eps>0,\,\exists\ N\) such that \(\left(n>N \Rightarrow|f_n(x)-f(x)|\lt \eps\right)\).%
\par
This is the type of convergence we have been observing to this point. By contrast we have the following new definition.%
\begin{definition}{}{x:definition:def_UniformConvergence}%
\index{uniform convergence} Let \(S\) be a subset of the real number system and let \(\left(f_n\right)=\left(f_1,f_2,f_3,\,\ldots\right)\) be a sequence of functions defined on \(S\). Let \(f\) be a function defined on \(S\) as well. We say that \(\left(f_n\right)\) \alert{converges to \(f\) uniformly on \(S\)} provided \(\forall\ \eps>0,\,\exists\ N\) such that \(n>N\Rightarrow|f_n(x)-f(x)|\lt \eps\text{ , } \forall\
x\in S\).%
\par
In this case we write \(f_n\unif f\) on \(S\).%
\end{definition}
The difference between these two definitions is subtle. In pointwise convergence, we are given a fixed \(x\in S\) and an \(\eps>0\). Then the task is to find an \(N\) that works for that particular \(x\) and \(\eps\). In uniform convergence, one is given \(\eps>0\) and must find a single \(N\) that works for that particular \(\eps\) but also simultaneously (uniformly) for all \(x\in S\). Clearly uniform convergence implies pointwise convergence as an \(N\) which works uniformly for all \(x\), works for each individual \(x\) also. However the reverse is not true. This will become evident, but first consider the following example.%
\begin{problem}{}{x:problem:prob_uniform_convergence}%
Let \(0\lt b\lt 1\) and consider the sequence of functions \(\left(f_n\right)\) defined on \([0,b]\) by \(f_n(x)=x^n\). Use the definition to show that \(f_n\unif 0\) on \([0,b]\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp250}{}\quad{}\(|x^n-0|=x^n\leq b^n\).%
\end{problem}
Uniform convergence is not only dependent on the sequence of functions but also on the set \(S\). For example, the sequence \(\left(f_n(x)\right)=\left(x^n\right)_{n=0}^\infty\) of \hyperref[x:problem:prob_uniform_convergence]{Problem~{\xreffont\ref{x:problem:prob_uniform_convergence}}} does not converge uniformly on \([0,1]\). We could use the negation of the definition to prove this, but instead, it will be a consequence of the following theorem.%
\begin{theorem}{}{}{x:theorem:thm_UnifConv-_Continuity}%
\index{uniform convergence!continuous functions and}\index{continuous functions!uniform convergence and}\index{continuous functions!uniform limit of continuous functions is continuous} Consider a sequence of functions \(\left(f_n\right)\) which are all continuous on an interval \(I\). Suppose \(f_n\unif f\) on \(I\). Then \(f\) must be continuous on \(I\).%
\end{theorem}
\begin{proof}{Sketch of Proof.}{g:proof:idp251}
Let \(a\in I\) and let \(\eps>0\). The idea is to use uniform convergence to replace \(f\) with one of the known continuous functions \(f_n\). Specifically, by uncancelling, we can write%
\begin{align*}
\left|f(x)-f(a)\right|\amp =\left|f(x)-f_n(x)+f_n(x)-f_n(a)+f_n(a)-f(a)\right|\\
\amp \leq \left|f(x)-f_n(x)\right|+\left|f_n(x)-f_n(a)\right|+\left|f_n(a)-f(a)\right|
\end{align*}
%
\par
If we choose \(n\) large enough, then we can make the first and last terms as small as we wish, noting that the uniform convergence makes the first term uniformly small for all \(x\). Once we have a specific \(n\), then we can use the continuity of \(f_n\) to find a \(\delta>0\) such that the middle term is small whenever \(x\) is within \(\delta\) of \(a\).%
\end{proof}
\begin{problem}{}{g:problem:idp252}%
\index{uniform convergence!continuous functions and}\index{uniform convergence implies continuity of limit function} Provide a formal proof of \hyperref[x:theorem:thm_UnifConv-_Continuity]{Theorem~{\xreffont\ref{x:theorem:thm_UnifConv-_Continuity}}} based on the above ideas.%
\end{problem}
\begin{problem}{}{g:problem:idp253}%
\index{\(x^n\)!converges pointwise on \([0,1]\)}\index{pointwise convergence}\index{\(x^n\) converges pointwise on \([0,1]\)} Consider the sequence of functions \(\left(f_n\right)\) defined on \([0,1]\) by \(f_n(x)=x^n\). Show that the sequence converges to the function%
\begin{equation*}
f(x)= \begin{cases}0\amp \text{ if  } x\in[0,1)\\ 1\amp \text{ if } x=1 \end{cases}
\end{equation*}
pointwise on \(\,[0,1]\), but not uniformly on \([0,1]\).%
\end{problem}
Notice that for the Fourier series at the beginning of this chapter,%
\begin{equation*}
f(x)=\frac{4}{\pi}\left(\cos\left(\frac{\pi}{2}x\right)-\frac{1}{3}\cos\left( 3\pi x\right)+\frac{1}{5}\cos\left(5\pi x\right)-\frac{1}{7}\cos\left(7\pi x\right)+\cdots\right)
\end{equation*}
the convergence cannot be uniform on \((-\infty,\infty)\), as the function \(f\) is not continuous. This never happens with a power series, since they converge to continuous functions whenever they converge. We will also see that uniform convergence is what allows us to integrate and differentiate a power series term by term.%
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 11.2 Uniform Convergence: Integrals and Derivatives}
\typeout{************************************************}
%
\begin{sectionptx}{Uniform Convergence: Integrals and Derivatives}{}{Uniform Convergence: Integrals and Derivatives}{}{}{x:section:PowerSeriesRedux-UnifConv-IntsAndDerivs}
\begin{introduction}{}%
We saw in the previous section that if \(\left(f_n\right)\) is a sequence of continuous functions which converges uniformly to \(f\) on an interval, then \(f\) must be continuous on the interval as well. This was not necessarily true if the convergence was only pointwise, as we saw a sequence of continuous functions defined on \((-\infty,\infty)\) converging pointwise to a Fourier series that was not continuous on the real line. Uniform convergence guarantees some other nice properties as well.%
\begin{theorem}{}{}{x:theorem:th_UniformIntegralConvergence}%
\index{uniform convergence!integration and}\index{integral of the uniform limit of functions} Suppose \(f_n\) and \(f\) are integrable and \(f_n\unif f\) on \([a,b]\). Then%
\begin{equation*}
\lim_{n\rightarrow\infty}\int_{x=a}^b f_n(x)\dx{ x}=\int_{x=a}^bf(x)\dx{ x}. {}
\end{equation*}
%
\end{theorem}
\begin{problem}{}{g:problem:idp254}%
\index{uniform convergence!integration and} Prove \hyperref[x:theorem:th_UniformIntegralConvergence]{Theorem~{\xreffont\ref{x:theorem:th_UniformIntegralConvergence}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp255}{}\quad{}For \(\eps>0\), we need to make \(|f_n(x)-f(x)|\lt \frac{\eps}{b-a}\), for all \(x\in[a,b]\).%
\end{problem}
Notice that this theorem is not true if the convergence is only pointwise, as illustrated by the following.%
\begin{problem}{}{g:problem:idp256}%
\index{convergence!pointwise convergence}\index{convergence!uniform convergence}\index{convergence!pointwise vs. uniform convergence} Consider the sequence of functions \(\left(f_n\right)\) given by%
\begin{equation*}
f_n(x)= \begin{cases}n\amp \text{ if } x\in\left(0,\frac{1}{n}\right)\\ 0\amp \text{ otherwise } \end{cases} \text{.}
\end{equation*}
%
\begin{enumerate}[label=(\alph*)]
\item{}Show that \(f_n\ptwise 0\) on \([0,1]\), but \(\limit{n}{\infty}{\int_{x=0}^1f_n(x)\dx{ x}\neq\int_{x=0}^10\dx{ x}.}\)%
\item{}Can the convergence be uniform? Explain.%
\end{enumerate}
%
\end{problem}
Applying this result to power series we have the following.%
\begin{corollary}{}{}{x:corollary:cor_IntConvUni}%
If \(\sum_{n=0}^\infty a_nx^n\) converges uniformly\footnotemark{} to \(f\) on an interval containing \(0\) and \(x\) then \(\int_{t=0}^xf(t)\dx{ t}=\sum_{n=0}^\infty\left(\frac{a_n}{n+1}x^{n+1}\right)\).%
\end{corollary}
\footnotetext[1]{Notice that we must explicitly assume uniform convergence. This is because we have not \emph{yet} proved that power series actually do converge uniformly.\label{g:fn:idp257}}%
\begin{problem}{}{g:problem:idp258}%
\index{power series!term by term integral of}\index{integration!term by term integration of power series} Prove \hyperref[x:corollary:cor_IntConvUni]{Corollary~{\xreffont\ref{x:corollary:cor_IntConvUni}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp259}{}\quad{}Remember that%
\begin{equation*}
\displaystyle \sum_{n=0}^\infty f_n(x) = \lim_{N\rightarrow\infty}\sum_{n=0}^N f_n(x). 
\end{equation*}
%
\end{problem}
Surprisingly, the issue of term-by-term differentiation depends not on the uniform convergence of \(\left(f_n\right)\), but on the uniform convergence of \(\left(f^\prime_n\right)\). More precisely, we have the following result.%
\begin{theorem}{}{}{x:theorem:thm_UniformDerivativeConvergence}%
\index{pointwise convergence!derivative and}%
\index{differentiation!of the pointwise limit of functions}%
Suppose for every \(n\in\NN\) \(f_n\) is differentiable, \(f_n^\prime\) is continuous, \(f_n\ptwise f\), and \(f_n^\prime\unif g\) on an interval, \(I\).  Then \(f\) is differentiable and \(f^\prime = g\) on \(I\).%
\end{theorem}
\begin{problem}{}{g:problem:idp260}%
Prove \hyperref[x:theorem:thm_UniformDerivativeConvergence]{Theorem~{\xreffont\ref{x:theorem:thm_UniformDerivativeConvergence}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp261}{}\quad{}Let \(a\) be an arbitrary fixed point in \(I\) and let \(x\in
I\).  By the Fundamental Theorem of Calculus, we have%
\begin{equation*}
\int_{t=a}^x f^\prime_n(t)\dx{ t}=f_n(x)-f_n(a)\text{.}
\end{equation*}
Take the limit of both sides and differentiate with respect to \(x\).%
\end{problem}
As before, applying this to power series gives the following result.%
\begin{corollary}{}{}{x:corollary:cor_UniformConvergenceDerivative}%
If \(\sum_{n=0}^\infty a_nx^n\) converges pointwise to \(f\) on an interval containing \(0\) and \(x\) and \(\sum_{n=1}^\infty a_nnx^{n-1}\) converges uniformly on an interval containing \(0\) and \(x\), then \(f^\prime(x)=\sum_{n=1}^\infty a_nnx^{n-1}\).%
\end{corollary}
\begin{problem}{}{g:problem:idp262}%
\index{power series!term by term derivative of}\index{differentiation!term by term differentiation of power series} Prove \hyperref[x:corollary:cor_UniformConvergenceDerivative]{Corollary~{\xreffont\ref{x:corollary:cor_UniformConvergenceDerivative}}}.%
\end{problem}
The above results say that a power series can be differentiated and integrated term-by-term as long as the convergence is uniform. Fortunately it is, in general, true that when a power series converges the convergence of it and its integrated and differentiated series is also uniform (almost).%
\par
However we do not yet have all of the tools necessary to see this. To build these tools requires that we return briefly to our study, begun in \hyperref[x:chapter:Convergence]{Chapter~{\xreffont\ref{x:chapter:Convergence}}}, of the convergence of sequences.%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection 11.2.1 Cauchy Sequences}
\typeout{************************************************}
%
\begin{subsectionptx}{Cauchy Sequences}{}{Cauchy Sequences}{}{}{g:subsection:idp263}
Knowing that a sequence or a series converges and knowing what it converges to are typically two different matters. For example, we know that \(\sum_{n=0}^\infty\frac{1}{n!}\)and \(\sum_{n=0}^\infty\frac{1}{n!\,n!}\) both converge. The first converges to \(e\), which has meaning in other contexts. We don't know what the second one converges to, other than to say it converges to \(\sum_{n=0}^\infty\frac{1}{n!\,n!}\). In fact, that question might not have much meaning without some other context in which \(\sum_{n=0}^\infty\frac{1}{n!\,n!}\) arises naturally. Be that as it may, we need to look at the convergence of a series (or a sequence for that matter) without necessarily knowing what it might converge to. We make the following definition.%
\begin{definition}{}{x:definition:def_CauchySequence}%
\index{sequences!Cauchy sequences} Let \(\left(s_n\right)\) be a sequence of real numbers. We say that \(\left(s_n\right)\)is a \terminology{Cauchy sequence} if for any \(\eps>0\), there exists a real number \(N\) such that if \(m,n>N\), then \(|s_m-s_n|\lt \eps\).%
\end{definition}
Notice that this definition says that the terms in a Cauchy sequence get arbitrarily close to each other and that there is no reference to getting close to any particular fixed real number. Furthermore, you have already seen lots of examples of Cauchy sequences as illustrated by the following result.%
\begin{theorem}{}{}{x:theorem:thm_Converge-_Cauchy}%
\index{sequences!convergence}\index{convergence!of a sequence!implies Cauchy sequence} Suppose \(\left(s_n\right)\) is a sequence of real numbers which converges to \(s\). Then \(\left(s_n\right)\) is a Cauchy sequence.%
\end{theorem}
Intuitively, this result makes sense. If the terms in a sequence are getting arbitrarily close to \(s\), then they should be getting arbitrarily close to each other.\footnote{But the converse isn't nearly as clear. In fact, it isn't true in the rational numbers.\label{g:fn:idp264}} This is the basis of the proof.%
\begin{problem}{}{g:problem:idp265}%
Prove \hyperref[x:theorem:thm_Converge-_Cauchy]{Theorem~{\xreffont\ref{x:theorem:thm_Converge-_Cauchy}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp266}{}\quad{}\(|s_m-s_n|=|s_m-s+s-s_n|\leq|s_m-s\mathopen|+|s-s_n|\).%
\end{problem}
So any convergent sequence is automatically Cauchy. For the real number system, the converse is also true and, in fact, is equivalent to any of our completeness axioms: the NIP, the Bolzano-Weierstrass Theorem, or the LUB Property. Thus, this could have been taken as our completeness axiom and we could have used it to prove the others. One of the most convenient ways to prove this converse is to use the Bolzano-Weierstrass Theorem. To do that, we must first show that a Cauchy sequence must be bounded. This result is reminiscent of the fact that a convergent sequence is bounded (\hyperref[x:lemma:lemma_BoundedConvergent]{Lemma~{\xreffont\ref{x:lemma:lemma_BoundedConvergent}}} of \hyperref[x:chapter:Convergence]{Chapter~{\xreffont\ref{x:chapter:Convergence}}}) and the proof is very similar.%
\begin{lemma}{}{}{x:lemma:lemma_Cauchy-_Bounded}%
Suppose \(\left(s_n\right)\) \(\)is a Cauchy sequence. Then there exists \(B>0\) such that \(|s_n|\leq B\) for all \(n\).%
\end{lemma}
\begin{problem}{}{g:problem:idp267}%
Prove \hyperref[x:lemma:lemma_Cauchy-_Bounded]{Lemma~{\xreffont\ref{x:lemma:lemma_Cauchy-_Bounded}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp268}{}\quad{}This is similar to \hyperref[x:problem:prob_BoundedConvergent]{problem~{\xreffont\ref{x:problem:prob_BoundedConvergent}}} of \hyperref[x:chapter:Convergence]{Chapter~{\xreffont\ref{x:chapter:Convergence}}}. There exists \(N\) such that if \(m,n>N\)then \(|s_n-s_m|\lt 1.\\), Choose a fixed \(m>N\) and let \(B=\max\left(\abs{s_1}, \abs{s_2}, \ldots, \abs{s_{\lceil N\rceil}}, \abs{s_m}+1\right)\).%
\end{problem}
\begin{theorem}{}{}{x:theorem:thm_Cauchy-_Converge}%
\alert{Cauchy sequences converge}%
\par
\index{sequences!Cauchy sequences!convergence of} Suppose \(\left(s_n\right)\)is a Cauchy sequence of real numbers. There exists a real number \(s\) such that \(\lim_{n\rightarrow\infty}s_n=s\).%
\end{theorem}
\begin{proof}{Sketch of Proof.}{g:proof:idp269}
We know that \(\left(s_n\right)\) \(\)is bounded, so by the Bolzano-Weierstrass Theorem, it has a convergent subsequence \(\left(s_{n_k}\right)\) converging to some real number \(s\). We have \(|s_n-s|=|s_n-s_{n_k}+s_{n_k}-s|\leq|s_n-s_{n_k}\mathopen|+|s_{n_k}-s|\). If we choose \(n\) and \(n_k\) large enough, we should be able to make each term arbitrarily small.%
\end{proof}
\begin{problem}{}{x:problem:prob_Cauchy_sequences_Cauchy_implies_convergence}%
\index{sequences!Cauchy sequences!convergence of} Provide a formal proof of \hyperref[x:theorem:thm_Cauchy-_Converge]{Theorem~{\xreffont\ref{x:theorem:thm_Cauchy-_Converge}}}.%
\end{problem}
From \hyperref[x:theorem:thm_Converge-_Cauchy]{Theorem~{\xreffont\ref{x:theorem:thm_Converge-_Cauchy}}} we see that every Cauchy sequence converges in \(\RR\). Moreover the proof of this fact depends on the Bolzano-Weierstrass Theorem which, as we have seen, is equivalent to our completeness axiom, the Nested Interval Property. What this means is that if there is a Cauchy sequence which does not converge then the NIP is not true. A natural question to ask is if every Cauchy sequence converges does the NIP follow? That is, is the convergence of Cauchy sequences also equivalent to our completeness axiom? The following theorem shows that the answer is yes.%
\begin{theorem}{}{}{x:theorem:thm_ConvCauchyEquivNIP}%
\index{sequences!Cauchy sequences!convergence of is equivalent to the NIP} Suppose every Cauchy sequence converges. Then the Nested Interval Property is true.%
\end{theorem}
\begin{problem}{}{x:problem:prob_Cauchy_sequences_Cauchy_implies_NIP}%
Prove \hyperref[x:theorem:thm_ConvCauchyEquivNIP]{Theorem~{\xreffont\ref{x:theorem:thm_ConvCauchyEquivNIP}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp270}{}\quad{}If we start with two sequences \(\left(x_n\right)\) and \(\left(y_n\right)\), satisfying all of the conditions of the NIP, you should be able to show that these are both Cauchy sequences.%
\end{problem}
\hyperref[x:problem:prob_Cauchy_sequences_Cauchy_implies_convergence]{Problems~{\xreffont\ref{x:problem:prob_Cauchy_sequences_Cauchy_implies_convergence}}} and \hyperref[x:problem:prob_Cauchy_sequences_Cauchy_implies_NIP]{Problem~{\xreffont\ref{x:problem:prob_Cauchy_sequences_Cauchy_implies_NIP}}} tell us that the following are equivalent: the Nested Interval Property, the Bolzano-Weierstrass Theorem, the Least Upper Bound Property, and the convergence of Cauchy sequences. Thus any one of these could have been taken as the completeness axiom of the real number system and then used to prove the each of the others as a theorem according to the following dependency graph:%
\begin{image}{0}{1}{0}%
\includegraphics[width=\linewidth]{images/CompletenessAxioms.png}
\end{image}%
Since we can get from any node on the graph to any other, simply by following the implications (indicated with arrows), any one of these statements is logically equivalent to each of the others.%
\begin{problem}{}{g:problem:idp271}%
\index{sequences!Cauchy sequences!don't always converge in \(\QQ\)} Since the convergence of Cauchy sequences can be taken as the completeness axiom for the real number system, it does not hold for the rational number system. Give an example of a Cauchy sequence of rational numbers which does not converge to a rational number.%
\end{problem}
If we apply the above ideas to series we obtain the following important result, which will provide the basis for our investigation of power series.%
\begin{theorem}{}{}{x:theorem:thm_CauchyCriterion}%
\alert{Cauchy Criterion}%
\par
\index{series!Cauchy Criterion}\index{Cauchy, Augustin!Cauchy Criterion} The series \(\sum_{k=0}^\infty a_k\) converges if and only if \(\forall \eps>0\), \(\exists N\) such that if \(m>n>N\) then \(|\sum_{k=n+1}^ma_k|\lt \eps\).%
\end{theorem}
\begin{problem}{}{g:problem:idp272}%
\index{series!Cauchy criterion}\index{Cauchy, Augustin!Cauchy Criterion} Prove the Cauchy criterion.%
\end{problem}
At this point several of the tests for convergence that you probably learned in calculus are easily proved. For example:%
\begin{problem}{}{x:problem:prob_NthTermTest}%
\alert{The \(n\)th Term Test}%
\par
\index{\(n\)th term test}\index{divergence!of a series!\(n\)th term test} Show that if \(\sum_{n=1}^\infty a_n\) converges then \(\limit{n}{\infty}{a_n}=0\).%
\end{problem}
\begin{problem}{The Strong Cauchy Criterion.}{g:problem:idp273}%
\index{series!Cauchy Criterion!Strong Cauchy Criterion} Show that \(\displaystyle\sum_{k=1}^\infty a_k\) converges if and only if \(\limit{n}{\infty}{\sum_{k=n+1}^\infty a_k}=0\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp274}{}\quad{}The hardest part of this problem is recognizing that it is really about the limit of a sequence as in \hyperref[x:chapter:Convergence]{Chapter~{\xreffont\ref{x:chapter:Convergence}}}.%
\end{problem}
You may also recall the Comparison Test from studying series in calculus: suppose \(0\leq a_n\leq b_n\), if \(\sum b_n\) converges then \(\sum a_n\) converges. This result follows from the fact that the partial sums of \(\sum a_n\) form an increasing sequence which is bounded above by \(\sum b_n\). (See \hyperref[x:corollary:cor_IncBoundedConverge]{Corollary~{\xreffont\ref{x:corollary:cor_IncBoundedConverge}}} of \hyperref[x:chapter:IVTandEVT]{Chapter~{\xreffont\ref{x:chapter:IVTandEVT}}}.) The Cauchy Criterion allows us to extend this to the case where the terms \(a_n\) could be negative as well. This can be seen in the following theorem.%
\begin{theorem}{}{}{x:theorem:thm_ComparisonTest}%
\alert{Comparison Test}%
\par
\index{series!Comparison Test} Suppose \(|a_n|\leq b_n\) for all \(n\). If \(\sum b_n\) converges then \(\sum a_n\) also converges.%
\end{theorem}
\begin{problem}{}{g:problem:idp275}%
Prove \hyperref[x:theorem:thm_ComparisonTest]{Theorem~{\xreffont\ref{x:theorem:thm_ComparisonTest}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp276}{}\quad{}Use the Cauchy criterion with the fact that \(\abs{\sum_{k=n+1}^ma_k}\leq\sum_{k=n+1}^m\abs{a_k}\).%
\end{problem}
The following definition is of marked importance in the study of series.%
\begin{definition}{}{x:definition:AbsoluteConvergence}%
\alert{Absolute Convergence}%
\par
\index{convergence!of a series!absolute} Given a series \(\sum a_n\), the series \(\sum|a_n|\) is called the \textbraceleft{}absolute series\textbraceright{} of \(\sum\boldsymbol{a}_{\boldsymbol{n}}\) and if \(\sum|a_n|\)converges then we say that \(\sum\boldsymbol{a}_{\boldsymbol{n}}\) converges absolutely.%
\end{definition}
The significance of this definition comes from the following result.%
\begin{corollary}{}{}{x:corollary:cor_AbsConv-_Conv}%
If \(\sum a_n\) converges absolutely, then \(\sum a_n\) converges.%
\end{corollary}
\begin{problem}{}{g:problem:idp277}%
\index{convergence!of a series!absolute convergence implies convergence} Show that \hyperref[x:corollary:cor_AbsConv-_Conv]{Corollary~{\xreffont\ref{x:corollary:cor_AbsConv-_Conv}}} is a direct consequence of \hyperref[x:theorem:thm_ComparisonTest]{Theorem~{\xreffont\ref{x:theorem:thm_ComparisonTest}}}.%
\end{problem}
\begin{problem}{}{g:problem:idp278}%
\index{absolute convergence!vs. the absolute value of a series} If \(\sum_{n=0}^\infty|a_n|=s\), then does it follow that \(s=|\sum_{n=0}^\infty a_n|\)? Justify your answer. What can be said?%
\end{problem}
The converse of \hyperref[x:corollary:cor_AbsConv-_Conv]{Corollary~{\xreffont\ref{x:corollary:cor_AbsConv-_Conv}}} is not true as evidenced by the series \(\sum_{n=0}^\infty\frac{(-1)^n}{n+1}\). As we noted in \hyperref[x:chapter:PowerSeriesQuestions]{Chapter~{\xreffont\ref{x:chapter:PowerSeriesQuestions}}}, this series converges to ln \(2\). However, its absolute series is the Harmonic \index{Harmonic Series} Series which diverges. Any such series which converges, but not absolutely, is said to \alert{converge conditionally}. Recall also that in \hyperref[x:chapter:PowerSeriesQuestions]{Chapter~{\xreffont\ref{x:chapter:PowerSeriesQuestions}}}, we showed that we could rearrange the terms of the series \(\sum_{n=0}^\infty\frac{(-1)^n}{n+1}\) to make it converge to any number we wished. We noted further that all rearrangements of the series \(\sum_{n=0}^\infty\frac{(-1)^n}{\left(n+1\right)^2}\) converged to the same value. The difference between the two series is that the latter converges absolutely whereas the former does not. Specifically, we have the following result.%
\begin{theorem}{}{}{x:theorem:thm_RearrageAbsConv}%
\index{series!rearrangements}\index{absolute convergence!all rearrangements of an absolutely convergent series converge to the same limit} Suppose \(\sum a_n\) converges absolutely and let \(s=\sum_{n=0}^\infty a_n\). Then any rearrangement of \(\sum a_n\) must converge to \(s\).%
\end{theorem}
\begin{proof}{Sketch of Proof.}{g:proof:idp279}
We will first show that this result is true in the case where \(a_n\geq 0\). If \(\sum b_n\) represents a rearrangement of \(\sum a_n\), then notice that the sequence of partial sums \(\left(\sum_{k=0}^nb_k\right)_{n=0}^\infty\)is an increasing sequence which is bounded by \(s\). By \hyperref[x:corollary:cor_IncBoundedConverge]{Corollary~{\xreffont\ref{x:corollary:cor_IncBoundedConverge}}} of \hyperref[x:chapter:IVTandEVT]{Chapter~{\xreffont\ref{x:chapter:IVTandEVT}}}, this sequence must converge to some number \(t\) and \(t\leq s\). Furthermore \(\sum a_n\) is also a rearrangement of \(\sum b_n\). Thus the result holds for this special case. (Why?) For the general case, notice that \(a_n=\frac{|a_n\mathopen|+a_n}{2}-\frac{|a_n\mathopen|-a_n}{2}\) and that \(\sum\frac{|a_n\mathopen|+a_n}{2}\) and \(\sum\frac{|a_n\mathopen|-a_n}{2}\) are both convergent series with nonnegative terms. By the special case \(\sum\frac{|b_n\mathopen|+b_n}{2}=\) \(\sum\frac{|a_n\mathopen|+a_n}{2}\) and \(\sum\frac{|b_n\mathopen|-b_n}{2}=\) \(\sum\frac{|a_n\mathopen|-a_n}{2}\).%
\end{proof}
\begin{problem}{}{g:problem:idp280}%
\index{series!rearrangements}\index{rearrangements of absolutely convergent series} Fill in the details and provide a formal proof of \hyperref[x:theorem:thm_RearrageAbsConv]{Theorem~{\xreffont\ref{x:theorem:thm_RearrageAbsConv}}}.%
\end{problem}
\end{subsectionptx}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 11.3 Radius of Convergence of a Power Series}
\typeout{************************************************}
%
\begin{sectionptx}{Radius of Convergence of a Power Series}{}{Radius of Convergence of a Power Series}{}{}{x:section:PowerSeriesRedux-RadiusOfConv}
We've developed enough machinery to look at the convergence of power series. The fundamental result is the following theorem due to Abel.%
\begin{theorem}{}{}{x:theorem:thm_RadiusOfConvergence}%
\index{series!radius of convergence}\index{converge inside radius of convergence} Suppose \(\sum_{n=0}^\infty a_nc^n\) converges for some nonzero real number \(c\). Then \(\sum_{n=0}^\infty a_nx^n\) converges absolutely for all \(x\) such that \(|x|\lt |c|\).%
\end{theorem}
To prove \hyperref[x:theorem:thm_RadiusOfConvergence]{Theorem~{\xreffont\ref{x:theorem:thm_RadiusOfConvergence}}} first note that by \hyperref[x:problem:prob_NthTermTest]{Problem~{\xreffont\ref{x:problem:prob_NthTermTest}}}, \(\limit{n}{\infty}{a_nc^n}=0\). Thus \(\left(a_nc^n\right)\) is a bounded sequence. Let \(B\) be a bound: \(\abs{ac^n}\le B\). Then%
\begin{equation*}
\abs{a_nx^n}=\abs{a_nc^n\cdot\left(\frac{x}{c}\right)^n}\leq B\abs{\frac{x}{c}}^n\text{.}
\end{equation*}
%
\par
We can now use the comparison test.%
\begin{problem}{}{g:problem:idp281}%
\index{power series!the radius of convergence}\index{convergence!the radius of convergence of a power series} Prove \hyperref[x:theorem:thm_RadiusOfConvergence]{Theorem~{\xreffont\ref{x:theorem:thm_RadiusOfConvergence}}}.%
\end{problem}
\begin{corollary}{}{}{x:corollary:cor_RadiusOfDivergence}%
Suppose \(\sum_{n=0}^\infty a_nc^n\) diverges for some real number \(c\). Then \(\sum_{n=0}^\infty a_nx^n\) diverges for all \(x\) such that \(|x|>|c|\).%
\end{corollary}
\begin{problem}{}{g:problem:idp282}%
\index{power series!the radius of convergence}\index{power series!a power series diverges outside it's radius of convergence} Prove \hyperref[x:corollary:cor_RadiusOfDivergence]{Corollary~{\xreffont\ref{x:corollary:cor_RadiusOfDivergence}}}.%
\end{problem}
As a result of \hyperref[x:theorem:thm_RadiusOfConvergence]{Theorem~{\xreffont\ref{x:theorem:thm_RadiusOfConvergence}}} and \hyperref[x:corollary:cor_RadiusOfDivergence]{Corollary~{\xreffont\ref{x:corollary:cor_RadiusOfDivergence}}}, we have the following: either \(\sum_{n=0}^\infty a_nx^n\) converges absolutely for all \(x\) or there exists some nonnegative real number \(r\) such that \(\sum_{n=0}^\infty a_nx^n\) converges absolutely when \(|x|\lt r\) and diverges when \(|x|>r\). In the latter case, we call \(r\) the \emph{radius of convergence} of the power series \(\sum_{n=0}^{\infty}a_{n} x^{n}\). In the former case, we say that the radius of convergence of \(\sum_{n=0}^\infty a_nx^n\) is \(\infty\). Though we can say that \(\sum_{n=0}^\infty a_nx^n\) converges absolutely when \(|x|\lt r\), we cannot say that the convergence is uniform. However, we can come close. We can show that the convergence is uniform for \(|x|\leq b\lt r\). To see this we will use the following result%
\begin{theorem}{}{}{x:theorem:thm_WeierstrassM}%
\alert{The Weierstrass-\(M\) Test}%
\par
\index{Weierstrass-\(M\) Test}\index{Weierstrass-\(M\) Test} Let \(\left(f_n\right)_{n=1}^\infty\) be a sequence of functions defined on \(S\subseteq\RR\) and suppose that \(\left(M_n\right)_{n=1}^\infty\) is a sequence of nonnegative real numbers such that%
\begin{equation*}
\abs{f_n(x)}\leq M_n,\,\, \forall x\in S,\,\, n=1, 2, 3, \ldots\text{.}
\end{equation*}
%
\par
If \(\sum_{n=1}^\infty M_n\) converges then \(\sum_{n=1}^\infty f_n(x)\) converges uniformly on \(S\) to some function (which we will denote by \(f(x)\)).%
\end{theorem}
\begin{proof}{Sketch of Proof.}{g:proof:idp283}
Since the crucial feature of the theorem is the function \(f(x)\) that our series converges to, our plan of attack is to first define \(f(x)\) and then show that our series, \(\sum_{n=1}^\infty f_n(x)\), converges to it uniformly.%
\par
First observe that for any \(x\in S\), \(\sum_{n=1}^\infty f_n(x)\) converges by the Comparison Test (in fact it converges absolutely) to some number we will denote by \(f(x)\). This actually defines the function \(f(x)\) for all \(x\in S\). It follows that \(\sum_{n=1}^\infty f_n(x)\) converges pointwise to \(f(x)\).%
\par
Next, let \(\eps>0\) be given. Notice that since \(\sum_{n=1}^\infty M_n\) converges, say to \(M\), then there is a real number, \(N\), such that if \(n>N\), then%
\begin{equation*}
\sum_{k=n+1}^\infty M_k = \abs{\sum_{k=n+1}^\infty M_k} = \abs{M-\sum_{k=1}^n M_k}\lt \eps\text{.}
\end{equation*}
%
\par
You should be able to use this to show that if \(n>N\), then%
\begin{equation*}
\abs{f(x) - \sum_{k=1}^n f_k(x)}\lt  \eps, \, \, \forall x\in S\text{.}\qedhere
\end{equation*}
%
\end{proof}
\begin{problem}{}{g:problem:idp284}%
\index{Weierstrass-\(M\) Test}\index{Weierstrass-\(M\) Test} Use the ideas above to provide a formal proof of \hyperref[x:theorem:thm_WeierstrassM]{Theorem~{\xreffont\ref{x:theorem:thm_WeierstrassM}}}.%
\end{problem}
\begin{problem}{}{g:problem:idp285}%
\index{uniform convergence!Fourier Series and}\index{Fourier Series!uniform convergence and}%
\begin{enumerate}[label=(\alph*)]
\item{}Referring back to \hyperref[x:men:PDE_sol]{equation~({\xreffont\ref{x:men:PDE_sol}})}, show that the Fourier series%
\begin{equation*}
\sum_{k=0}^\infty\frac{(-1)^k}{(2k+1)^2}\sin\left((2k+1)\pi x\right)
\end{equation*}
converges uniformly on \(\RR\).%
\item{}Does its differentiated series converge uniformly on \(\RR?\) Explain.%
\end{enumerate}
%
\end{problem}
\begin{problem}{}{g:problem:idp286}%
\index{Weierstrass-\(M\) Test}\index{Weierstrass-\(M\) Test!drill problems} Observe that for all \(x \in [-1,1]\) \(\abs{x}\le 1\). Identify which of the following series converges pointwise and which converges uniformly on the interval \([-1,1]\). In every case identify the limit function. \(\displaystyle \text{ (a) } \ \sum_{n=1}^\infty\left(x^n-x^{n-1}\right) \ \ \ \ \ \text{ (b) } \ \sum_{n=1}^\infty\frac{\left(x^n-x^{n-1}\right)}{n} \ \ \ \ \ \text{ (c) } \ \sum_{n=1}^\infty\frac{\left(x^n-x^{n-1}\right)}{n^2}\)%
\end{problem}
Using the Weierstrass-\(M\) test, we can prove the following result.%
\begin{theorem}{}{}{x:theorem:thm_PowerSeriesConvergeUniformly}%
\index{power series!converge uniformly on their interval of convergence} Suppose \(\sum_{n=0}^\infty a_nx^n\) has radius of convergence \(r\) (where \(r\) could be \(\infty\) as well). Let \(b\) be any nonnegative real number with \(b\lt r\). Then \(\sum_{n=0}^\infty a_nx^n\) converges uniformly on \([-b,b]\).%
\end{theorem}
\begin{problem}{}{g:problem:idp287}%
Prove \hyperref[x:theorem:thm_PowerSeriesConvergeUniformly]{Theorem~{\xreffont\ref{x:theorem:thm_PowerSeriesConvergeUniformly}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp288}{}\quad{}We know that \(\sum_{n=0}^\infty|a_nb^n|\) converges. This should be all set for the Weierstrass-\(M\) test.%
\end{problem}
To finish the story on differentiating and integrating power series, all we need to do is show that the power series, its integrated series, and its differentiated series all have the same radius of convergence. You might not realize it, but we already know that the integrated series has a radius of convergence at least as big as the radius of convergence of the original series. Specifically, suppose \(f(x)=\sum_{n=0}^\infty a_nx^n\)has a radius of convergence \(r\) and let \(|x|\lt r\). We know that \(\,\sum_{n=0}^\infty a_nx^n\) converges uniformly on an interval containing \(0\)and \(x\), and so by \hyperref[x:corollary:cor_UniformConvergenceDerivative]{Corollary~{\xreffont\ref{x:corollary:cor_UniformConvergenceDerivative}}}, \(\int_{t=0}^xf(t)\dx{ t}=\sum_{n=0}^\infty\left(\frac{a_n}{n+1}x^{n+1}\right)\). In other words, the integrated series converges for any \(x\) with\(\,|x|\lt r\). This says that the radius of convergence of the integated series must be at least \(r\).%
\par
To show that the radii of convergence are the same, all we need to show is that the radius of convergence of the differentiated series is at least as big as \(r\) as well. Indeed, since the differentiated series of the integrated series is the original, then this would say that the original series and the integrated series have the same radii of convergence. Putting the differentiated series into the role of the original series, the original series is now the integrated series and so these would have the same radii of convergence as well. With this in mind, we want to show that if \(|x|\lt r\), then \(\sum_{n=0}^\infty a_nnx^{n-1}\) converges. The strategy is to mimic what we did in \hyperref[x:theorem:thm_RadiusOfConvergence]{Theorem~{\xreffont\ref{x:theorem:thm_RadiusOfConvergence}}}, where we essentially compared our series with a converging geometric series. Only this time we need to start with the differentiated geometric series.%
\begin{problem}{}{x:problem:prob_PwrSeriesDiffConv}%
Show that \(\sum_{n=1}^\infty nx^{n-1}\) converges for \(|x|\lt 1\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp289}{}\quad{}We know that \(\sum_{k=0}^nx^k=\frac{x^{n+1}-1}{x-1}\). Differentiate both sides and take the limit as \(n\) approaches infinity.%
\end{problem}
\begin{theorem}{}{}{x:theorem:thm_SeriesConv-_DerivConv}%
\index{power series!term by term derivative of} Suppose \(\sum_{n=0}^\infty a_nx^n\) has a radius of convergence \(r\) and let \(|x|\lt r\). Then \(\sum_{n=1}^\infty a_nnx^{n-1}\) converges.%
\end{theorem}
\begin{problem}{}{g:problem:idp290}%
Prove \hyperref[x:theorem:thm_SeriesConv-_DerivConv]{Theorem~{\xreffont\ref{x:theorem:thm_SeriesConv-_DerivConv}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp291}{}\quad{}Let \(b\) be a number with \(|x|\lt b\lt r\) and consider \(\left|a_nnx^{n-1}|=|a_nb^n\cdot\frac{1}{b}\cdot n\left(\frac{x}{b}\right)^{n-1}\right|\). You should be able to use the Comparison Test and \hyperref[x:problem:prob_PwrSeriesDiffConv]{Problem~{\xreffont\ref{x:problem:prob_PwrSeriesDiffConv}}}.%
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 11.4 Boundary Issues and Abel's Theorem}
\typeout{************************************************}
%
\begin{sectionptx}{Boundary Issues and Abel's Theorem}{}{Boundary Issues and Abel's Theorem}{}{}{x:section:PowerSeriesRedux-AbelsThm}
Summarizing our results, we see that any power series \(\sum a_nx^n\) has a radius of convergence \(r\) such that \(\sum a_nx^n\) converges absolutely when \(|x|\lt r\) and diverges when \(|x|>r\). Furthermore, the convergence is uniform on any closed interval \([-b,b]\subset(-r,r)\) which tells us that whatever the power series converges to must be a continuous function on \((-r,r)\). Lastly, if \(f(x)=\sum_{n=0}^\infty a_nx^n\) for \(x\in(-r,r)\), then \(f^\prime(x)=\sum_{n=1}^\infty a_nnx^{n-1}\) for \(x\in(-r,r)\) and \(\int_{t=0}^xf(t)\dx{ t} = \sum_{n=0}^\infty a_n\frac{x^{n+1}}{n+1}\) for \(x\in(-r,r)\).%
\par
Thus power series are very well behaved within their interval of convergence, and our cavalier approach from \hyperref[x:chapter:CalcIn17th18thCentury]{Chapter~{\xreffont\ref{x:chapter:CalcIn17th18thCentury}}} is justified, \alert{EXCEPT} for one issue. If you go back to \hyperref[x:problem:prob_alternating_harmonic_series]{Problem~{\xreffont\ref{x:problem:prob_alternating_harmonic_series}}} of \hyperref[x:chapter:CalcIn17th18thCentury]{Chapter~{\xreffont\ref{x:chapter:CalcIn17th18thCentury}}}, you see that we used the geometric series to obtain the series, \(\arctan x =\sum_{n=0}^\infty(-1)^n\frac{1}{2n+1}x^{2n+1}\). We substituted \(x=1\) into this to obtain \(\frac{\pi}{4}=\sum_{n=0}^\infty(-1)^n\frac{1}{2n+1}\). Unfortunately, our integration was only guaranteed on a closed subinterval of the interval \((-1,1)\) where the convergence was uniform and we substituted in \(x=1\). We ``danced on the boundary'' in other places as well, including when we said that%
\begin{equation*}
\frac{\pi}{4}=\int_{x=0}^1\sqrt{1-x^2}dx=1+\sum_{n=1}^\infty\left(\frac{\prod_{j=0}^{n-1}\left(\frac{1}{2}-j\right)}{n!}\text{ } \right)\left(\frac{\left(-1\right)^n}{2n+1}\right)\text{.}
\end{equation*}
%
\par
The fact is that for a power series \(\sum a_nx^n\) with radius of convergence \(r\), we know what happens for \(x\) with \(|x|\lt r\)and \(x\) with \(|x|>r\). We never talked about what happens for \(x\) with \(|x|=r\). That is because there is no systematic approach to this boundary problem. For example, consider the three series%
\begin{equation*}
\sum_{n=0}^\infty x^n,\sum_{n=0}^\infty\frac{x^{n+1}}{n+1}, \sum_{n=0}^\infty\frac{x^{n+2}}{(n+1)(n+2)}\text{.}
\end{equation*}
%
\par
They are all related in that we started with the geometric series and integrated twice, thus they all have radius of convergence equal to 1. Their behavior on the boundary, i.e., when \(x=\pm 1\), is another story. The first series diverges when \(x=\pm 1\), the third series converges when \(x=\pm 1\). The second series converges when \(x=-1\) and diverges when \(x=1\).%
\par
Even with the unpredictability of a power series at the endpoints of its interval of convergence, the Weierstrass-\(M\) test does give us some hope of uniform convergence.%
\begin{problem}{}{g:problem:idp292}%
Suppose the power series \(\sum a_nx^n\) has radius of convergence \(r\) and the series \(\sum a_nr^n\) converges absolutely. Then \(\sum a_nx^n\) converges uniformly on \([-r,r]\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp293}{}\quad{}For \(|x|\leq r\), \(|a_nx^n|\leq |a_nr^n|\).%
\end{problem}
Unfortunately, this result doesn't apply to the integrals we mentioned as the convergence at the endpoints is not absolute. Nonetheless, the integrations we performed in \hyperref[x:chapter:CalcIn17th18thCentury]{Chapter~{\xreffont\ref{x:chapter:CalcIn17th18thCentury}}} are still legitimate. This is due to the following theorem by Abel which \index{Abel, Niels Henrik} extends uniform convergence to the endpoints of the interval of convergence even if the convergence at an endpoint is only conditional. Abel did not use the term uniform convergence, as it hadn't been defined yet, but the ideas involved are his.%
\begin{theorem}{}{}{x:theorem:AbelsTheorem}%
\alert{Abel's Theorem}%
\par
\index{Abel, Niels Henrik!Abel's Theorem}\index{Abel's Theorem} Suppose the power series \(\sum a_nx^n\) has radius of convergence \(r\) and the series \(\sum a_nr^n\) converges. Then \(\sum a_nx^n\) converges uniformly on \([0, r]\).%
\end{theorem}
The proof of this is not intuitive, but involves a clever technique known as Abel's Partial Summation Formula.%
\begin{lemma}{}{}{x:lemma:lemma_AbelsPartialSummationFormula}%
Let%
\begin{equation*}
a_1,a_2,\,\ldots,\,a_n,\,b_1,b_2,\,\ldots\,,\,b_n
\end{equation*}
be real numbers and let \(A_m=\sum_{k=1}^ma_k\). Then%
\begin{equation*}
a_1b_1+a_2b_2+\cdots+a_nb_n=\sum_{j=1}^{n-1}A_j\left(b_j-b_{j+1}\text{ } \right)+A_nb_n\text{.}
\end{equation*}
%
\end{lemma}
\begin{problem}{}{g:problem:idp294}%
Prove \hyperref[x:lemma:lemma_AbelsPartialSummationFormula]{Lemma~{\xreffont\ref{x:lemma:lemma_AbelsPartialSummationFormula}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp295}{}\quad{}For \(j>1\), \(a_j=A_j-A_{j-1}\).%
\end{problem}
\begin{lemma}{}{}{x:lemma:lemma_AbelsLemma}%
\alert{Abel's Lemma}%
\par
Let \(a_1,a_2,\,\ldots,\,a_n,\,b_1,b_2,\,\ldots\,,\,b_n\) be real numbers with \(\,b_1\geq b_2\geq\,\ldots\geq\,b_n\geq 0\)and let \(A_m=\sum_{k=1}^ma_k\). Suppose \(|A_m|\leq B\) for all \(m\). Then \(|\sum_{j=1}^na_jb_j|\leq B\cdot b_1\)%
\end{lemma}
\begin{problem}{}{g:problem:idp296}%
\index{Abel, Niels Henrik!Abel's Lemma}\index{Abel's Lemma} Prove \hyperref[x:lemma:lemma_AbelsLemma]{Lemma~{\xreffont\ref{x:lemma:lemma_AbelsLemma}}}.%
\end{problem}
\begin{problem}{}{g:problem:idp297}%
Prove \hyperref[x:theorem:AbelsTheorem]{Theorem~{\xreffont\ref{x:theorem:AbelsTheorem}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp298}{}\quad{}Let \(\epsilon>0\). Since \(\sum_{n=0}^\infty a_nr^n\) converges then by the Cauchy Criterion, there exists \(N\) such that if \(m>n>N\) then \(\abs{\sum_{k=n+1}^ma_kr^k}\lt \frac{\epsilon}{2}\) Let \(0\leq x\leq r\). By \hyperref[x:lemma:lemma_AbelsLemma]{Lemma~{\xreffont\ref{x:lemma:lemma_AbelsLemma}}},%
\begin{equation*}
\abs{\sum_{k=n+1}^ma_kx^k}=\abs{\sum_{k=n+1}^ma_kr^k\left(\frac{x}{r}\right)^k}\leq \left(\frac{\epsilon}{2}\right)\left(\frac{x}{r}\right)^{n+1}\leq\frac{\epsilon}{2}\text{.}
\end{equation*}
%
\par
Thus for \(0\leq x\leq r\), \(n>N\),%
\begin{equation*}
\abs{\sum_{k=n+1}^\infty a_kx^k}=\lim_{n\rightarrow\infty}\abs{\sum_{k=n+1}^ma_kx^k}\leq\frac{\epsilon}{2}\lt \epsilon.\rbrack{}
\end{equation*}
%
\end{problem}
\begin{corollary}{}{}{x:corollary:cor_PowerSeriesConvUnif}%
Suppose the power series \(\sum a_nx^n\) has radius of convergence \(r\) and the series \(\sum a_n\left(-r\right)^n\) converges. Then \(\sum a_nx^n\) converges uniformly on \([-r,0]\).%
\end{corollary}
\begin{problem}{}{g:problem:idp299}%
Prove \hyperref[x:corollary:cor_PowerSeriesConvUnif]{Corollary~{\xreffont\ref{x:corollary:cor_PowerSeriesConvUnif}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp300}{}\quad{}Consider \(\sum a_n\left(-x\right)^n\).%
\end{problem}
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Chapter 12 Back to the Real Numbers}
\typeout{************************************************}
%
\begin{chapterptx}{Back to the Real Numbers}{}{Back to the Real Numbers}{}{}{x:chapter:BackToFourier}
\begin{introduction}{}%
As we have seen, when they converge, power series are very well behaved and Fourier (trigonometric) series are not necessarily. The fact that trigonometric series were so interesting made them a lightning rod for mathematical study in the late nineteenth century.%
\par
For example, consider the question of uniqueness. We saw in \hyperref[x:chapter:TaylorSeries]{Chapter~{\xreffont\ref{x:chapter:TaylorSeries}}} that if a function could be represented by a power series, then that series must be the Taylor series. More precisely, if%
\begin{equation*}
f(x)=\sum_{n=0}^\infty a_n(x-a)^n, \text{  then  }  a_n=\frac{f^{(n)}(a)}{n!}\text{.}
\end{equation*}
%
\par
But what can be said about the uniqueness of a trigonometric series? If we can represent a function \(f\) as a general trigonometric series%
\begin{equation*}
f(x)=\sum_{n=0}^\infty(a_n\cos n\pi x\,+b_n\sin n\pi x)\text{,}
\end{equation*}
then must this be the Fourier series with the coefficients as determined by Fourier? \index{Fourier, Jean Baptiste Joseph}%
\par
For example, if \(\sum_{n=0}^\infty(a_n\cos n\pi x\,+b_n\sin n\pi x)\) converges to \(f\) uniformly on the interval \((0,1)\), then because of the uniform convergence, Fourier's term-by-term integration which we saw in \hyperref[x:part:Interregnum]{Part~{\xreffont\ref{x:part:Interregnum}}} is perfectly legitimate and the coefficients are, of necessity, the coefficients he computed. However we have seen that the convergence of a Fourier series need not be uniform. This does not mean that we cannot integrate term-by-term, but it does say that we can't be sure that term-by-term integration of a Fourier series will yield the integral of the associated function.%
\par
\index{Lebesgue, Henri}\index{Cantor, Georg} This led to a generalization of the integral by Henri Lebesgue in 1905.  Lebesgue's profound work settled the issue of whether or not a bounded pointwise converging trigonometric series is the Fourier series of a function, but we will not go in this direction.  We will instead focus on work that Georg Cantor did in the years just prior. Cantor's work was also profound and had far reaching implications in modern mathematics.  It also leads to some very weird conclusions.\footnote{'Weird' does not mean false.  It simply means that some of Cantor's results can be hard to accept, even after you have seen the proof and verified its validity.\label{g:fn:idp301}}%
\par
To begin, let's suppress the underlying function and suppose we have%
\begin{equation*}
\sum_{n=0}^\infty(a_n\cos n\pi x\,+b_n\sin n\pi x) = \sum_{n=0}^\infty(a^\prime_n\cos n\pi x\,+b^\prime_n\sin n\pi x)\text{.}
\end{equation*}
%
\par
We ask: If these two series are equal must it be true that \(a_n=a^\prime_n\) and \(b_n=b^\prime_n?\) We can reformulate this uniqueness question as follows: Suppose%
\begin{equation*}
\sum_{n=0}^\infty\left((a_n-a^\prime_n)\cos n\pi x\,+(b_n-b^\prime_n)\sin n\pi x\right) = 0\text{.}
\end{equation*}
%
\par
If we let \(c_n = a_n-a^\prime_n\) and \(d_n = b_n=b^\prime_n\), then the question becomes: If \(\sum_{n=0}^\infty\left(c_n\cos n\pi x\,+d_n\sin n\pi x\right) = 0\), then will \(c_n=d_n=0?\) It certainly seems reasonable to suppose so, but at this point we have enough experience with infinite sums to know that we need to be \emph{very} careful about relying on the intuition we have honed on finite sums.%
\par
\index{Cantor, Georg!and the modern view of mathematics} The answer to this seemingly basic question leads to some very profound results.  In particular, answering this question led the mathematician Georg Cantor (1845-1918) to study the makeup of the real number system.  This in turn opened the door to the modern view of mathematics of the twentieth century.  In particular, Cantor proved the following result in 1871~(\hyperlink{x:biblio:jahnke03__histor_analy}{[{\xreffont 6}]}, p. 305).%
\begin{theorem}{}{}{x:theorem:thm_Cantor-uniqueness-of-trig-series}%
\alert{Cantor}%
\par
\index{Cantor, Georg}\index{Cantor, Georg!uniqueness of Fourier series!first theorem on}\index{Fourier Series!Cantor's first theorem on uniqueness} If the trigonometric series%
\begin{equation*}
\sum_{n=0}^\infty\left(c_n\cos n\pi  x\,+d_n\sin n\pi x\right) = 0\text{,}
\end{equation*}
%
\par
``with the exception of certain values of \(x\),'' then all of its coefficients vanish.%
\end{theorem}
In his attempts to nail down precisely which ``certain values'' could be exceptional, Cantor was led to examine the nature of subsets of real numbers and ultimately to give a precise definition of the concept of infinite sets and to define an arithmetic of ``infinite numbers.'' (Actually, he called them \textbraceleft{}transfinite numbers\textbraceright{} because, by definition, numbers are finite.)%
\par
As a first step toward identifying those ``certain values,'' Cantor proved the following theorem, which we will state but not prove.%
\begin{theorem}{}{}{x:theorem:thm_Cantor_1870}%
\alert{Cantor, (1870)}%
\par
\index{Cantor, Georg!uniqueness of Fourier series!second theorem}\index{Fourier Series!Cantor's second theorem on uniqueness} If the trigonometric series%
\begin{equation*}
\sum_{n=0}^\infty\left(c_n\cos n\pi  x\,+d_n\sin n\pi x\right) = 0\text{,}
\end{equation*}
for all \(x\in\RR\) then all of its coefficients vanish.%
\end{theorem}
He then extended this to the following:%
\begin{theorem}{}{}{x:theorem:thm_Cantor-1871-b}%
\alert{Cantor, (1871)}%
\par
\index{Cantor, Georg!uniqueness of Fourier series!third theorem on}\index{Fourier Series!Cantor's third theorem on uniqueness} If the trigonometric series%
\begin{equation*}
\sum_{n=0}^\infty\left(c_n\cos n\pi  x\,+d_n\sin n\pi x\right) = 0\text{,}
\end{equation*}
for all but finitely many \(x\in\RR\) then all of its coefficients vanish.%
\end{theorem}
\begin{figureptx}{Georg Cantor}{g:figure:idp302}{}%
\index{Cantor, Georg!portrait of}%
\begin{image}{0.325}{0.35}{0.325}%
\includegraphics[width=\linewidth]{images/Cantor.png}
\end{image}%
\tcblower
\end{figureptx}%
Observe that this is not a trivial generalization. Although the exceptional points are constrained to be finite in number, this number could still be extraordinarily large. That is, even if the series given above differed from zero on \(10^{10^{100000}}\) distinct points in the interval \((0, 10^{10^{-100000}})\) the coefficients still vanish. This remains true even if at each of these \(10^{10^{100000}}\) points the series converges to \(10^{10^{100000}}\). This is truly remarkable when you think of it this way.%
\par
At this point Cantor became more interested in these exceptional points than in the Fourier series problem that he'd started with. The next task he set for himself was to see just how general the set of exceptional points could be. Following Cantor's lead we make the following definitions.%
\begin{definition}{}{x:definition:def_accumulation-point}%
\index{limit!accumulation point}\index{accumulation point} Let \(S\subseteq \RR\) and let \(a\) be a real number. We say that \(a\) is a \terminology{limit point} (or an \terminology{accumulation point}) of \(S\) if there is a sequence \((a_n)\) with \(a_n\in S-\left\{a\right\}\) which converges to \(a\).%
\end{definition}
\begin{problem}{}{g:problem:idp303}%
\index{accumulation points}\index{accumulation points} Let \(S\subseteq\RR\) and let \(a\) be a real number. Prove that \(a\) is a limit point of \(S\) if and only if for every \(\eps>0\) the intersection%
\begin{equation*}
(a-\eps, a+\eps) \cap S-\left\{a\right\} \neq \emptyset.{}
\end{equation*}
%
\end{problem}
The following definition gets to the heart of the matter.%
\begin{definition}{}{x:definition:def_derived-set}%
\index{derived sets} Let \(S\subseteq\RR\). The set of all limit points of \(S\) is called the \terminology{derived set} of \(S\). The derived set is denoted \(S^{\prime}\).%
\end{definition}
Don't confuse the derived set of a set with the derivative of a function. They are completely different objects despite the similarity of both the language and notation. The only thing that they have in common is that they were somehow ``derived'' from something else.%
\begin{problem}{}{g:problem:idp304}%
\index{derived sets} Determine the derived set, \(S^\prime\), of each of the following sets.%
\begin{enumerate}[label=(\alph*)]
\item{}\(\displaystyle S=\left\{\frac11, \frac12, \frac13, \ldots\right\}\)%
\item{}\(\displaystyle S=\left\{0,\frac11, \frac12, \frac13, \ldots\right\}\)%
\item{}\(\displaystyle S=(0,1]\)%
\item{}\(\displaystyle S=\left[\left.0,1/2\right)\right.\cup\left.\left(1/2,1\right.\right]\)%
\item{}\(\displaystyle S=\QQ\)%
\item{}\(\displaystyle S=\RR-\QQ\)%
\item{}\(\displaystyle S=\ZZ\)%
\item{}Any finite set \(S\).%
\end{enumerate}
%
\end{problem}
\begin{problem}{}{g:problem:idp305}%
\index{derived sets} Let \(S\subseteq\RR\).%
\begin{enumerate}[label=(\alph*)]
\item{}Prove that \(\left(S^{\,\prime}\right)^{\,\prime}\subseteq S^{\,\prime}\).%
\item{}Give an example where these two sets are equal.%
\item{}Give an example where these two sets are not equal.%
\end{enumerate}
%
\end{problem}
The notion of the derived set forms the foundation of Cantor's exceptional set of values. Specifically, let \(S\) again be a set of real numbers and consider the following sequence of sets:%
\begin{equation*}
S^{\,\prime}\supseteq \left(S^\prime\right)^\prime\supseteq \left(\left(S^\prime\right)^\prime\right)^\prime\supseteq \cdots\text{.}
\end{equation*}
%
\par
Cantor \index{Cantor, Georg} showed that if, at some point, one of these derived sets is empty, then the uniqueness property still holds. Specifically, we have:%
\begin{theorem}{}{}{x:theorem:thm_Cantor-1871}%
\alert{Cantor, (1871)}%
\par
\index{Cantor, Georg!fourth theorem on the uniqueness of Fourier series} Let \(S\) be a subset of the real numbers with the property that one of its derived sets is empty.  Then if the trigonometric series \(\sum_{n=0}^\infty\left(c_n\cos n\pi
x\,+d_n\sin n\pi x\right)\) is zero for all \(x\in\RR-S\), then all of the coefficients of the series vanish.%
\end{theorem}
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Section 12.1 Infinite Sets}
\typeout{************************************************}
%
\begin{sectionptx}{Infinite Sets}{}{Infinite Sets}{}{}{x:section:BackToFourier-InfiniteSets}
The following theorem follows directly from our previous work with the NIP and will be very handy later. It basically says that a sequence of nested closed intervals will still have a non-empty intersection even if their lengths do not converge to zero as in the NIP.%
\begin{theorem}{}{}{x:theorem:thm_WeakNIP}%
\index{Nested Interval Property (NIP)!weak form}\index{weakened form of the NIP} Let \(\left(\left[a_n, b_n\right]\right)_{n=1}^\infty\) be a sequence of nested intervals such that \(\limit{n}{\infty}{\abs{b_n-a_n}}>0\). Then there is at least one \(c\in\RR\) such that \(c\in\left[a_n, b_n\right]\) for all \(n\in\NN\).%
\end{theorem}
\begin{proof}{}{g:proof:idp306}
By \hyperref[x:corollary:cor_IncBoundedConverge]{Corollary~{\xreffont\ref{x:corollary:cor_IncBoundedConverge}}} of \hyperref[x:chapter:IVTandEVT]{Chapter~{\xreffont\ref{x:chapter:IVTandEVT}}}, we know that a bounded increasing sequence such as \((a_n)\) converges, say to \(c\). Since \(a_n\leq a_m\leq b_n\) for \(m>n\) and \(\limit{m}{\infty}{a_m}=c\), then for any fixed \(n\), \(a_n\leq c\leq b_n\). This says \(c\in\left[a_n, b_n\right]\) for all \(n\in\NN\).%
\end{proof}
\begin{problem}{}{g:problem:idp307}%
\index{Nested Interval Property (NIP)!weak form}\index{weak NIP} Suppose \(\limit{n}{\infty}{\abs{b_n-a_n}}>0\). Show that there are at least two points, \(c\) and \(d\), such that \(c\in[a_n, b_n]\) and \(d\in[a_n, b_n]\) for all \(n\in\NN\).%
\end{problem}
Our next theorem says that in a certain, very technical sense there are more real numbers than there are counting numbers\textbackslash{}cite\textbraceleft{}franks10:\textunderscore{}cantor\textunderscore{}other\textunderscore{}proof\textunderscore{}rr\textunderscore{}uncoun\textbraceright{}. This probably does not seem terribly significant. After all, there are real numbers which are not counting numbers. What will make this so startling is that the same cannot be said about all sets which strictly contain the counting numbers. We will get into the details of this after the theorem is proved.%
\begin{theorem}{}{}{x:theorem:thm_NoSeriesContainsAllReals}%
\alert{Cantor, (1874)}%
\par
\index{Cantor, Georg!first proof that \(\RR\) is uncountable}\index{\(\RR\)!is uncountable!Cantor's first proof} Let \(S=\left(s_n\right)_{n=1}^\infty\) be a sequence of real numbers.  There is a real number \(c\), which is not in\footnotemark{} \(S\).%
\end{theorem}
\footnotetext[1]{To streamline things, we are abusing notation here as we are letting \(S\) denote both the sequence (which is ordered) and the underlying (unordered) set of entries in the sequence.\label{g:fn:idp308}}%
\begin{proof}{}{g:proof:idp309}
For the sake of obtaining a contradiction assume that the sequence \(S\) contains every real number; that is, \(S=\RR\). As usual we will build a sequence of nested intervals \(\left(\left[x_i, y_i\right]\right)_{i=1}^\infty\).%
\par
Let \(x_1\) be the smaller of the first two distinct elements of \(S\), let \(y_1\) be the larger and take \(\left[x_1,y_1\right]\) to be the first interval.%
\par
Next we assume that \(\left[x_{n-1}, y_{n-1}\right]\) has been constructed and build \(\left[x_n, y_n\right]\) as follows. Observe that there are infinitely many elements of \(S\) in \(\left(x_{n-1}, y_{n-1}\right)\) since \(S=\RR\). Let \(s_m\) and \(s_k\) be the \emph{first} two distinct elements of \(S\) such that%
\begin{equation*}
s_m, s_k \in \left(x_{n-1}, y_{n-1}\right) \text{.}
\end{equation*}
%
\par
Take \(x_n\) to be the smaller and \(y_n\) to be the larger of \(s_m\) and \(s_k\). Then \(\left[x_n, y_n\right]\) is the \(n\)th interval.%
\par
From the way we constructed them it is clear that%
\begin{equation*}
\left[x_1, y_1\right] \supseteq \left[x_2, y_2\right] \supseteq \left[x_3, y_3\right] \supseteq \ldots\text{.}
\end{equation*}
%
\par
Therefore by \hyperref[x:theorem:thm_WeakNIP]{Theorem~{\xreffont\ref{x:theorem:thm_WeakNIP}}} there is a real number, say \(c\), such that%
\begin{equation*}
c\in\left[x_n, y_n\right] \text{ for all }  n\in\NN\text{.}
\end{equation*}
%
\par
In fact, since \(x_1\lt x_2\lt x_3\ldots\lt y_3\lt y_2\lt y_1\) it is clear that%
\begin{equation}
x_n\lt c\lt y_n, \ \forall n\text{.}\label{x:men:eq_NIP-interval}
\end{equation}
%
\par
We will show that \(c\) is the number we seek. That the inequalities in \hyperref[x:men:eq_NIP-interval]{formula~({\xreffont\ref{x:men:eq_NIP-interval}})} are strict will play a crucial role.%
\par
To see that \(c\not\in S\) we suppose that \(c\in S\) and derive a contradiction.%
\par
So, suppose that \(c=s_p\) for some \(p\in\NN\). Then only \(\left\{s_1, s_2,\ldots,
s_{p-1}\right\}\) appear before \(s_p\) in the sequence \(S\). Since each \(x_n\) is taken from \(S\) it follows that only finitely many elements of the sequence \((x_n)\) appear before \(s_p=c\) in the sequence as well.%
\par
Let \(x_l\) be the \emph{last} element of \((x_n)\) which appears before \(c=s_p\) in the sequence and consider \(x_{l+1}\). The way it was constructed, \(x_{l+1}\) was one of the first two distinct terms in the sequence \(S\) strictly between \(x_l\) and \(y_l\), the other being \(y_{l+1}\). Since \(x_{l+1}\) does not appear before \(c=s_p\) in the sequence and \(x_l\lt c\lt y_l\), it follows that either \(c=x_{l+1}\) or \(c=y_{l+1}\). However, this gives us a contradiction as we know from \hyperref[x:men:eq_NIP-interval]{formula~({\xreffont\ref{x:men:eq_NIP-interval}})} that \(x_{l+1}\lt c\lt y_{l+1}\).%
\par
Thus \(c\) is not an element of \(S\).%
\end{proof}
So how does this theorem show that there are ``more'' real numbers than counting numbers? Before we address that question we need to be very careful about the meaning of the word 'more' when we're talking about infinite sets.%
\par
First let's consider two finite sets, say \(A=\left\{\alpha,\beta,\gamma,\delta\right\}\) and \(B=\left\{a,b,c,d,e\right\}\). How do we know that \(B\) is the bigger set? (It obviously is.) Clearly we can just count the number of elements in both \(A\) and \(B\). Since \(\abs{A}=4\) and \(\abs{B}=5\) and \(4\lt 5\) \(B\) is clearly bigger. But we're looking for a way to determine the relative size of two sets without counting them because we have no way of counting the number of elements of an infinite set. Indeed, it isn't even clear what the phrase ``the number of elements'' might \emph{mean} when applied to the elements of an infinite set.%
\par
When we count the number of elements in a finite set what we're really doing is matching up the elements of the set with a set of consecutive positive integers, starting at \(1\). Thus since%
\begin{align*}
1\amp \leftrightarrow\alpha\\
2\amp \leftrightarrow\beta\\
3\amp \leftrightarrow\gamma\\
4\amp \leftrightarrow\delta
\end{align*}
we see that \(\abs{A}=4\). Moreover, the order of the match-up is unimportant. Thus since%
\begin{align*}
2\amp \leftrightarrow e\\
3\amp \leftrightarrow a\\
5\amp \leftrightarrow b\\
4\amp \leftrightarrow d\\
1\amp \leftrightarrow c
\end{align*}
it is clear that the elements of \(B\) and the set \(\left\{1,2,3,4,5\right\}\) can be matched up as well. And it doesn't matter what order either set is in. They both have \(5\) elements.%
\par
Such a match-up is called a one-to-one correspondence. In general, if two sets can be put in one-to-one correspondence then they are the same ``size.'' Of course the word ``size'' has lots of connotations that will begin to get in the way when we talk about infinite sets, so instead we will say that the two sets have the same \emph{cardinality.} Speaking loosely, this just means that they are the same size.%
\par
More precisely, if a given set \(S\) can be put in one-to-one correspondence with a finite set of consecutive integers, say \(\left\{1,2,3,\ldots, N\right\}\), then we say that the cardinality of the set is \(N\). But this just means that both sets have the same cardinality. It is this notion of one-to-one correspondence, along with the next two definitions, which will allow us to compare the sizes (cardinalities) of infinite sets.%
\begin{definition}{}{x:definition:def_CountableSets}%
\index{cardinality!countable sets}\index{countable sets!defintion of} Any set which can be put into one-to-one correspondence with \(\NN=\left\{1,2,3,\ldots\right\}\) is called a \terminology{countably infinite} set. Any set which is either finite or countably infinite is said to be \terminology{countable}.%
\end{definition}
Since \(\NN\) is an infinite set, we have no symbol to designate its cardinality so we have to invent one. The symbol used by Cantor \index{Cantor, Georg} and adopted by mathematicians ever since is \(\aleph_0\).\footnote{\(\aleph{}\) is the first letter of the Hebrew alphabet and is pronounced "aleph." \(\aleph_0\) is pronounced "aleph null."\label{g:fn:idp310}} Thus the cardinality of any countably infinite set is \(\aleph_0\).%
\par
We have already given the following definition informally. We include it formally here for later reference.%
\begin{definition}{}{x:definition:def_EqualCardinality}%
\index{cardinality!equal cardinality} If two sets can be put into one-to-one correspondence then they are said to have the \alert{same cardinality}.%
\end{definition}
With these two definitions in place we can see that \hyperref[x:theorem:thm_NoSeriesContainsAllReals]{Theorem~{\xreffont\ref{x:theorem:thm_NoSeriesContainsAllReals}}} is nothing less than the statement that the real numbers are not countably infinite. Since it is certainly not finite, then we say that the set of real numbers is uncountable and therefore ``bigger'' than the natural numbers!%
\par
To see this let us suppose first that each real number appears in the sequence \((s_n)_{n=1}^\infty\) exactly once. In that case the indexing of our sequence is really just a one-to-one correspondence between the elements of the sequence and \(\NN:\)%
\begin{align*}
1\amp \leftrightarrow s_1\\
2\amp \leftrightarrow s_2\\
3\amp \leftrightarrow s_3\\
4\amp \leftrightarrow s_4\\
\amp \ \ \ \vdots
\end{align*}
%
\par
If some real numbers are repeated in our sequence then all of the real numbers are a subset of our sequence and will therefore also be countable (see \hyperref[x:problem:prob_countable_sets_unions_of]{Problem~{\xreffont\ref{x:problem:prob_countable_sets_unions_of}}}, part a).%
\par
In either case, every sequence is countable. But our theorem says that \emph{no} sequence in \(\RR\) includes all of \(\RR\). Therefore \(\RR\) is uncountable.%
\par
Most of the sets you have encountered so far in your life have been countable.%
\begin{problem}{}{g:problem:idp311}%
\index{countable sets!countable sets drill, 5 parts} Show that each of the following sets is countable.%
\begin{enumerate}[label=(\alph*)]
\item{}\(\displaystyle \left\{2,3,4,5,\ldots\right\}=\left\{n\right\}_{n=2}^\infty\)%
\item{}\(\displaystyle \left\{0,1,2,3,\ldots\right\}=\left\{n\right\}_{n=0}^\infty\)%
\item{}\(\displaystyle \left\{1,4,9,16,\ldots,n^2,\ldots\right\}=\left\{n^2\right\}_{n=1}^\infty\)%
\item{}The set of prime numbers%
\item{}\(\displaystyle \ZZ\)%
\end{enumerate}
%
\end{problem}
In fact, if we start with a countable set it is rather difficult to use it to build anything but another countable set.%
\begin{problem}{}{x:problem:prob_countable_sets_unions_of}%
\index{countable sets!unions and intersections of}\index{unions and intersections of countable sets} Let \(\left\{A_i\right\}\) be a collection of countable sets. Show that each of the following sets is also countable:%
\begin{enumerate}[label=(\alph*)]
\item{}Any subset of \(A_1\).%
\item{}\(\displaystyle A_1\cup A_2\)%
\item{}\(\displaystyle A_1\cup A_2 \cup A_3\)%
\item{}\(\displaystyle \displaystyle\bigcup_{i=1}^nA_i\)%
\item{}\(\displaystyle \displaystyle\bigcup_{i=1}^\infty A_i\)%
\end{enumerate}
%
\end{problem}
It seems that no matter what we do the only example of an uncountably infinite set is \(\RR\). But wait! Remember the rational numbers? They were similar to the real numbers in many ways. Perhaps they are uncountably infinite too?%
\par
Alas, no. The rational numbers turn out to be countable too.%
\begin{theorem}{}{}{x:theorem:thm_QisCountable}%
\index{\(\QQ\)!is countable} Show that \(\QQ\) is countable.%
\end{theorem}
\begin{proof}{Sketch of Proof.}{g:proof:idp312}
First explain how you know that all of the non-negative rational numbers are in this list:%
\begin{equation*}
\frac{0}{1},\frac{0}{2},\frac{1}{1},\frac{0}{3}, \frac{1}{2},\frac{2}{1},\frac{0}{4},\frac{1}{3}, \frac{2}{2}, \frac{3}{1}, \cdots\text{.}
\end{equation*}
%
\par
However there is clearly some duplication. To handle this, apply part (a) of \hyperref[x:problem:prob_countable_sets_unions_of]{Problem~{\xreffont\ref{x:problem:prob_countable_sets_unions_of}}}. Does this complete the proof or is there more to do?%
\end{proof}
\begin{problem}{}{g:problem:idp313}%
\index{\(\QQ\)!\(\QQ\) is countable} Prove \hyperref[x:theorem:thm_QisCountable]{Theorem~{\xreffont\ref{x:theorem:thm_QisCountable}}}.%
\end{problem}
The following corollary says that the cardinality of the real numbers is \emph{much} larger than the cardinality of the rational numbers, despite the fact that both are infinite.%
\par
That is, as a subset of the reals, the rationals can be contained in a sequence of intervals, the sum of whose lengths can be arbitrarily small. In a sense this says that a countably infinite set is so small (on the transfinite scale) that it is ``almost'' finite.%
\par
\index{measure zero} Usually we express this idea with the statement, ``\(\QQ\) is a set of \emph{measure zero} in \(\RR\)''. The term ``measure'' has a precise meaning which we will not pursue. The following corollary contains the essence of the idea.%
\begin{corollary}{}{}{x:corollary:cor_Q-MeasureZero}%
Let \(\eps>0\) be given. There is a collection of intervals in \(\RR\), \(I_n=[a_n,b_n]\) such that%
\begin{equation*}
\QQ \subset \bigcup_{n=1}^\infty I_n
\end{equation*}
and%
\begin{equation*}
\sum_{n=1}^\infty(b_n-a_n)\lt \eps\text{.}
\end{equation*}
%
\end{corollary}
\begin{problem}{}{g:problem:idp314}%
\index{\(\QQ\)!has measure zero in \(\RR\)}\index{measure zero!\(\QQ\) has measure zero in \(\RR\)} Prove \hyperref[x:corollary:cor_Q-MeasureZero]{Corollary~{\xreffont\ref{x:corollary:cor_Q-MeasureZero}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp315}{}\quad{}If we had only finitely many rationals to deal with this would be easy. Let \(\left\{r_1, r_2, \ldots,
r_k\right\}\) be these rational numbers and take \(a_n=r_n-\frac{\eps}{2k}\) and \(b_n=r_n+\frac{\eps}{2k}\). Then for all \(n=1,\ldots, k\) \(r_n\in[a_n,b_n]\) and%
\begin{equation*}
\sum_{n=1}^kb_n-a_n = \sum_{n=1}^k \frac{\eps}{k}=\eps\text{.}
\end{equation*}
%
\par
The difficulty is, how do we move from the finite to the infinite case?]%
\end{problem}
Notice how this idea hearkens back to the discussion of Leibniz's\index{Leibniz, Gottfried Wilhelm} approach to the Product Rule (\hyperref[x:men:eq_LeibnizProductRule]{equation~({\xreffont\ref{x:men:eq_LeibnizProductRule}})}). He simply tossed aside the expression \(\dx{ x}\dx{ y}\) because it was \emph{infinitely small} compared to either \(x\dx{ y}\) or \(y\dx{ x}\). Although this isn't quite the same thing we are discussing here it is similar and it is clear that Leibniz's insight and intuition were extremely acute. They were moving him in the right direction, at least.%
\par
All of our efforts to build an uncountable set from a countable one have come to nothing. In fact many sets that at first ``feel'' like they should be uncountable are in fact countable.\footnote{The failure is in the methods we've used so far. It is possible to build an uncountable set using just two symbols if we're clever enough, but this would take us too far away from our main topic.\label{g:fn:idp316}} This makes the uncountability of \(\RR\) all the more remarkable.%
\par
However if we \emph{start} with an uncountable set it is relatively easy to build others from it.%
\begin{problem}{}{g:problem:idp317}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Let \((a,b)\) and \((c,d)\) be two open intervals of real numbers. Show that these two sets have the same cardinality by constructing a one-to-one onto function between them.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp318}{}\quad{}A linear function should do the trick.%
\item[(b)]Show that any open interval of real numbers has the same cardinality as \(\RR\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp319}{}\quad{}Consider the interval \((-\pi/2,\pi/2)\).%
\item[(c)]Show that \((0,1]\) and \((0,1)\) have the same cardinality.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp320}{}\quad{}Note that \(\left\{1,1/2,1/3,\ldots\right\}\) and \(\left\{1/2, 1/3, \ldots\right\}\) have the same cardinality.%
\item[(d)]Show that \([0,1]\) and \((0,1)\) have the same cardinality.%
\end{enumerate}
\end{problem}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 12.2 Cantor's Theorem and Its Consequences}
\typeout{************************************************}
%
\begin{sectionptx}{Cantor's Theorem and Its Consequences}{}{Cantor's Theorem and Its Consequences}{}{}{x:section:BackToFourier-CantorsTheorem}
\begin{figureptx}{Richard Dedekind}{g:figure:idp321}{}%
\index{Dedekind, Richard!portrait of}%
\begin{image}{0.325}{0.35}{0.325}%
\includegraphics[width=\linewidth]{images/Dedekind.png}
\end{image}%
\tcblower
\end{figureptx}%
Once Cantor showed that there were two types of infinity (countable and uncountable), the following question was natural, ``Do all uncountable sets have the same cardinality?''%
\par
Just like not all ``non-dogs'' are cats, there is, offhand, no reason to believe that all uncountable sets should be the same size. However constructing uncountable sets of different sizes is not as easy as it sounds.%
\par
\index{Dedekind, Richard}\index{Cantor, Georg!unit interval and unit square have equal cardinalty} For example, what about the line segment represented by the interval \([0,1]\) and the square represented by the set \([0,1]\times[0,1]=\left\{(x,y)|0\leq x,y\leq 1\right\}\). Certainly the two dimensional square must be a larger infinite set than the one dimensional line segment.  Remarkably, Cantor showed that these two sets were the same cardinality.  In his 1877 correspondence of this result to his friend and fellow mathematician, Richard Dedekind, even Cantor remarked, ``I see it, but I don't believe it!''%
\par
The following gives the original idea of Cantor's proof. Cantor devised the following function \(f:[0,1]\times[0,1]\rightarrow [0,1]\). First, we represent the coordinates of any point \((x,y)\in [0,1]\times[0,1]\) by their decimal representations \(x=0.a_1 a_2 a_3\ldots\) and \(y=0.b_1 b_2 b_3\ldots\). Even terminating decimals can be written this way as we could write \(0.5=0.5000\ldots\). We can then define \(f(x,y)\) by%
\begin{equation*}
f((0.a_1 a_2 a_3\ldots ,0.b_1 b_2 b_3\ldots))=0.a_1 b_1 a_2 b_2 a_3 b_3\ldots\text{.}
\end{equation*}
%
\par
This relatively simple idea has some technical difficulties in it related to the following result.%
\begin{problem}{}{g:problem:idp322}%
\index{series!Geometric series!\((0.\bar{9})\) converges to \(1\)} Consider the sequence \((0.9,0.99,0.999,\ldots)\). Determine that this sequence converges and, in fact, it converges to \(1\). This suggests that \(0.999\ldots=1\).%
\end{problem}
Similarly, we have \(0.04999\ldots=0.05000\ldots\), etc. To make the decimal representation of a real number in \([0,1]\) unique, we must make a consistent choice of writing a terminating decimal as one that ends in an infinite string of zeros or an infinite string of nines [with the one exception \(0=0.000\ldots\) ]. No matter which choice we make, we could never make this function onto. For example, \(109/1100=0.09909090\ldots\) would have as its pre-image \((0.0999\ldots,0.9000\ldots)\) which would be a mix of the two conventions.%
\par
Cantor was able to overcome this technicality to demonstrate a one to one correspondence, but instead we will note that in either convention, the function is one-to-one, so this says that the set \([0,1]\times[0,1]\) is the same cardinality as some (uncountable) subset of \(\RR\).  The fact that this has the same cardinality as \(\RR\) is something we will come back to. But first we'll try construct an uncountable set which does not have the same cardinality as \(\RR\).  To address this issue, Cantor proved the following in 1891.%
\begin{theorem}{}{}{x:theorem:thm_CantorsTheorem}%
\alert{Cantor's Theorem}%
\par
\index{Cantor, Georg!Cantor's Theorem} Let \(S\) be any set.  Then there is no one-to-one correspondence between \(S\) and \(P(S)\), the set of all subsets of \(S\).%
\end{theorem}
Since \(S\) can be put into one-to-one correspondence with a subset of \(P(S)\) (\(a\rightarrow \left\{a\right\}\)), then this says that \(P(S)\) is at least as large as S. In the finite case \(\abs{P(S)}\) is strictly greater than \(\abs{S}\) as the following problem shows. It also demonstrates why \(P(S)\) is called the power set of \(S\).%
\begin{problem}{}{x:problem:prob_PowerSet}%
Prove: If \(\abs{S}=n\), then \(\abs{ P(S)}=2^n\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp323}{}\quad{}Let \(S={a_1,a_2,\ldots,a_n}\). Consider the following correspondence between the elements of \(P(S)\) and the set \(T\) of all \(n\)-tuples of yes (Y) or no (N):%
\begin{align*}
\{ \}  \amp \leftrightarrow \{N,N,N,\ldots,N\}\\
\{a_1\}\amp \leftrightarrow \{Y,N,N,\ldots ,N\}\\
\{a_2\}\amp \leftrightarrow \{N,Y,N,\ldots,N\}\\
\amp \vdots\\
S\amp \leftrightarrow \{Y,Y,Y,\ldots,Y\}
\end{align*}
%
\par
How many elements are in \(T?\)%
\end{problem}
\begin{problem}{}{g:problem:idp324}%
Prove \hyperref[x:theorem:thm_CantorsTheorem]{Cantor's Theorem~{\xreffont\ref{x:theorem:thm_CantorsTheorem}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp325}{}\quad{}Assume for contradiction, that there is a one-to-one correspondence \(f:S\rightarrow P(S)\). Consider \(A=\left\{x\in S|x\not\in f(x)\right\}\). Since \(f\) is onto, then there is \(a\in A\) such that \(A=f(a)\).  Is \(a\in A\) or is \(a\not\in
A?\)%
\end{problem}
Actually it turns out that \(\RR\) and \(P(\NN)\) have the same cardinality. This can be seen in a roundabout way using some of the above ideas from \hyperref[x:problem:prob_PowerSet]{Problem~{\xreffont\ref{x:problem:prob_PowerSet}}}. Specifically, let \(T\) be the set of all sequences of zeros or ones (you can use \(Y\)s or \(N\)s, if you prefer). Then it is straightforward to see that \(T\) and \(P(\NN)\) have the same cardinality.%
\par
If we consider \((0,1]\), which has the same cardinality as \(\RR\), then we can see that this has the same cardinality as \(T\) as well. Specifically, if we think of the numbers in binary, then every real number in \([0,1]\) can be written as \(\sum_{j=1}^\infty \frac{a_j}{2^j} =(a_1,a_2,\ldots)\) where \(a_j\in\left\{0,1\right\}\). We have to account for the fact that binary representations such as \(0.0111\ldots\) and \(0.1000\ldots\) represent the same real number (say that no representations will end in an infinite string of zeros), then we can see that \([0,1]\) has the same cardinality as \(T-U\), where \(U\) is the set of all sequences ending in an infinite string of zeros. It turns out that \(U\) itself is a countable set.%
\begin{problem}{}{g:problem:idp326}%
\index{countable sets!countable union of finite sets} Let \(U_n=\{(a_1,a_2,a_3,\ldots)\ |\ a_j\in \{0,1\} \text{ and } a_{n+1}=a_{n+2}=\cdots=0\}\). Show that for each \(n\), \(U_n\) is finite and use this to conclude that \(U\) is countably infinite.%
\end{problem}
The following two problems show that deleting a countable set from an uncountable set does not change its cardinality.%
\begin{problem}{}{g:problem:idp327}%
\index{sets!countably infinite subsets} Let \(S\) be an infinite set. Prove that \(S\) contains a countably infinite subset.%
\end{problem}
\begin{problem}{}{g:problem:idp328}%
Suppose \(X\) is an uncountable set and \(Y\subset X\) is countably infinite.  Prove that \(X\) and \(X-Y\) have the same cardinality.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp329}{}\quad{}Let \(Y=Y_0\). If \(X-Y_0\) is an infinite set, then by the previous problem it contains a countably infinite set \(Y_1\). Likewise if \(X-(Y_0\cup Y_1)\) is infinite it also contains an infinite set \(Y_2\). Again, if \(X-(Y_0\cup Y_1\cup Y_2)\) is an infinite set then it contains an infinite set \(Y_3\), etc. For \(n=1, 2, 3,\ldots \), let \(f_n:Y_{n-1}\rightarrow Y_n\) be a one-to-one correspondence and define \(f:X\rightarrow X-Y\) by%
\begin{equation*}
\begin{cases}f(x)=f_n(x), \amp \text{ if }  x\in Y_n, n=0,1,2,\ldots\\ f(x)=x, \amp \text{ if }  x\in X-(\cup_{n=0}^\infty Y_n ) \end{cases} \text{.}
\end{equation*}
%
\par
Show that \(f\) is one-to-one and onto.%
\end{problem}
The above problems say that \(R\), \(T-U\), \(T\), and \(P(N)\) all have the same cardinality.%
\par
As was indicated before, Cantor's \index{Cantor, Georg} work on infinite sets had a profound impact on mathematics in the beginning of the twentieth century. For example, in examining the proof of Cantor's Theorem, the eminent logician Bertrand Russell devised his famous paradox in 1901. Before this time, a set was naively thought of as just a collection of objects. Through the work of Cantor and others, sets were becoming a central object of study in mathematics as many mathematical concepts were being reformulated in terms of sets. The idea was that set theory was to be a unifying theme of mathematics. This paradox set the mathematical world on its ear.%
\par
\textbraceleft{}Russell's Paradox\textbraceright{} \index{Russell's Paradox} Consider the set of all sets which are not elements of themselves. We call this set \(D\) and ask, ``Is \(D\in D?\)'' Symbolically, this set is%
\begin{equation*}
D=\{S\ |\ S\not \in S\}\text{.}
\end{equation*}
%
\par
If \(D\in D\), then by definition, \(D\not\in D\). If \(D\not\in D\), then by definition, \(D\in D\).%
\par
If you look back at the proof of Cantor's Theorem, this was basically the idea that gave us the contradiction.  To have such a contradiction occurring at the most basic level of mathematics was scandalous.  It forced a number of mathematicians and logicians to carefully devise the axioms by which sets could be constructed. To be honest, most mathematicians still approach set theory from a naive point of view as the sets we typically deal with fall under the category of what we would call ``normal sets.'' In fact, such an approach is officially called Naive Set Theory (as opposed to Axiomatic Set Theory).  However, attempts to put set theory and logic on solid footing led to the modern study of symbolic logic and ultimately the design of computer (machine) logic.%
\par
Another place where Cantor's work had a profound influence in modern logic comes from something we alluded to before.  We showed before that the unit square \([0,1]\times [0,1]\) had the same cardinality as an uncountable subset of \(\RR\).  In fact, Cantor \index{Cantor, Georg} showed that the unit square had the same cardinality as \(\RR\) itself and was moved to advance the following in 1878.%
\begin{conjecture}{The Continuum Hypothesis.}{}{g:conjecture:idp330}%
\index{Continuum Hypothesis!original}%
Every uncountable subset of \(\RR\) has the same cardinality as \(\RR\).%
\end{conjecture}
Cantor was unable to prove or disprove this conjecture (along with every other mathematician). In fact, proving or disproving this conjecture, which was dubbed the Continuum Hypothesis, was one of Hilbert's famous 23 problems presented as a challenge to mathematicians at the International Congress of Mathematicians in 1900.%
\par
Since \(\RR\) has the same cardinality as \(P(\NN)\), then the Continuum Hypothesis was generalized to the:%
\begin{conjecture}{The Generalized Continuum Hypothesis.}{}{g:conjecture:idp331}%
\index{Continuum Hypothesis!generalized}%
Given an infinite set \(S\), there is no infinite set which has a cardinality strictly between that of \(S\) and its power set \(P(S)\).%
\end{conjecture}
Efforts to prove or disprove this were in vain and with good reason.  In 1940, the logician Kurt Gdel showed that the Continuum Hypothesis could not be disproved from the Zermelo-Fraenkel Axioms of set theory\footnote{One of the formal axiomatic approaches to set theory established by Ernst Zermelo in 1908 and revised by Abraham Fraenkel in 1921.\label{g:fn:idp332}}.  In 1963, Paul Cohen showed that the Continuum Hypothesis could not be proved using the Zermelo-Fraenkel Axioms.  In other words, the Zermelo-Fraenkel Axioms do not contain enough information to decide the truth of the hypothesis.%
\par
We are willing to bet that at this point your head might be swimming a bit with uncertainty. If so, then know that these are the same feelings that the mathematical community experienced in the mid twentieth century. In the past, mathematics was seen as a model of logical certainty. It is disconcerting to find that there are statements that are ``undecidable.'' In fact, Gdel proved in 1931 that a consistent finite axiom system that contained the axioms of arithmetic would always contain undecidable statements which could neither be proved true nor false with those axioms. Mathematical knowledge would always be incomplete.%
\begin{figureptx}{Kurt Gdel}{g:figure:idp333}{}%
\index{Gdel, Kurt!portrait of}%
\begin{image}{0.325}{0.35}{0.325}%
\includegraphics[width=\linewidth]{images/Godel.png}
\end{image}%
\tcblower
\end{figureptx}%
So by trying to put the foundations of calculus on solid ground, we have come to a point where we can never obtain mathematical certainty. Does this mean that we should throw up our hands and concede defeat? Should we be paralyzed with fear of trying anything? Certainly not! As we mentioned before, most mathematicians do well by taking a pragmatic approach: using their mathematics to solve problems that they encounter. In fact, it is typically the problems that motivate the mathematics. It is true that mathematicians take chances that don't always pan out, but they still take these chances, often with success. Even when the successes lead to more questions, as they typically do, tackling those questions usually leads to a deeper understanding. At the very least, our incomplete understanding means we will always have more questions to answer, more problems to solve.%
\par
What else could a mathematician ask for?%
\end{sectionptx}
\end{chapterptx}
 %
%
\typeout{************************************************}
\typeout{Chapter 13 Epilogues}
\typeout{************************************************}
%
\begin{chapterptx}{Epilogues}{}{Epilogues}{}{}{x:chapter:Epilogues}
%
%
\typeout{************************************************}
\typeout{Section 13.1 On the Nature of Numbers: A Dialogue (with Apologies to Galileo)}
\typeout{************************************************}
%
\begin{sectionptx}{On the Nature of Numbers: A Dialogue (with Apologies to Galileo)}{}{On the Nature of Numbers: A Dialogue (with Apologies to Galileo)}{}{}{x:section:Epilogues-NatureNumbers}
\begin{introduction}{}%
\alert{Interlocuters}:\emph{Salviati, Sagredo, and Simplicio; Three Friends of Galileo Galilei}%
\par
\alert{Setting}: Three friends meet in a garden for lunch in Renassaince Italy.  Prior to their meal they discuss the book \emph{How We Got From There to Here: A Story of Real Analysis.} How they obtained a copy is not clear.%
\par
\alert{Salviati}: My good sirs. I have read this very strange volume as I hope you have?%
\par
\alert{Sagredo}: I have and I also found it very strange.%
\par
\alert{Simplicio}: Very strange indeed; at once silly and mystifying.%
\par
\alert{Salviati}: Silly? How so?%
\par
\alert{Simplicio}: These authors begin their tome with the question, ``What is a number?'' This is an unusually silly question, don't you think? Numbers are numbers. Everyone knows what they are.%
\par
\alert{Sagredo}: I thought so as well until I reached the last chapter. But now I am not so certain. What about this quantity \(\aleph_0?\) If this counts the positive integers, isn't it a number? If not, then how can it count anything? If so, then what number is it? These questions plague me 'til I scarcely believe I know anything anymore.%
\par
\alert{Simplicio}: Of course \(\aleph_0\) is not a number! It is simply a new name for the infinite, and infinity is not a number.%
\par
\alert{Sagredo}: But isn't \(\aleph_0\) the cardinality of the set of natural numbers, \(\NN\), in just the same way that the cardinality of the set \(S=\left\{Salviati, Sagredo, Simplicio\right\}\) is \(3?\) If \(3\) is a number, then why isn't \(\aleph_0?\)%
\par
\alert{Simplicio}: Ah, my friend, like our authors you are simply playing with words. You count the elements in the set \(S=\left\{Salviati, Sagredo, Simplicio\right\};\) you see plainly that the \emph{number of elements} it contains is \(3\) and then you change your language. Rather than saying that the \emph{number of elements} in \(S\) is \(3\) you say that the \emph{cardinality} is \(3\). But clearly ``cardinality'' and ``number of elements'' mean the same thing.%
\par
Similarly you use the symbol \(\NN\) to denote the set of positive integers. With your new word and symbol you make the statement ``the \emph{cardinality} (number of elements) of \(\NN\) is \(\aleph_0\).'' This statement has the same grammatical form as the statement ``the \emph{number of elements} (cardinality) of \(S\) is three.'' Since three \emph{is} a number you conclude that \(\aleph_0\) is also a number.%
\par
But this is simply nonsense dressed up to sound sensible. If we unwind our notation and language, your statement is simply, ``The number of positive integers is infinite.'' This is obviously nonsense because infinity is \emph{not} a number.%
\par
Even if we take infinity as an undefined term and try to define it by your statement this is still nonsense since you are using the word ``number'' to define a new ``number'' called infinity. This definition is circular. Thus it is no definition at all. It is nonsense.%
\par
\alert{Salviati}: Your reasoning on this certainly seems sound.%
\par
\alert{Simplicio}: Thank you.%
\par
\alert{Salviati}:  However, there are a couple of small points I would like to examine more closely if you will indulge me?%
\par
\alert{Simplicio}: Of course. What troubles you?%
\par
\alert{Salviati}: You've said that we cannot use the word ``number'' to define numbers because this would be circular reasoning. I entirely agree, but I am not sure this is what our authors are doing.%
\par
Consider the set \(\left\{1, 2, 3\right\}\). Do you agree that it contains three elements?%
\par
\alert{Simplicio}: Obviously.%
\par
\alert{Sagredo}: Ah! I see your point! That there are three elements does \emph{not} depend on what those elements are. Any set with three elements has three elements regardless of the nature of the elements. Thus saying that the set \(\left\{1, 2, 3\right\}\) contains three elements does not define the word ``number'' in a circular manner because it is irrelevant that the \emph{number} 3 is one of the elements of the set. Thus to say that three is the cardinality of the set \(\left\{1, 2, 3\right\}\) has the same meaning as saying that there are three elements in the set \(\left\{Salviati, Sagredo, Simplicio\right\}\). In both cases the number ``\(3\)'' is the name that we give to the totality of the elements of each set.%
\par
\alert{Salviati}:  Precisely. In exactly the same way \(\aleph_0\) is the symbol we use to denote the totality of the set of positive integers.%
\par
Thus \(\aleph_0\) is a number in the same sense that '\(3\)' is a number, is it not?%
\par
\alert{Simplicio}:  I see that we can say in a meaningful way that three is the cardinality of any set with~.~.~.~well,~.~.~.~with three elements (it becomes very difficult to talk about these things) but this is simply a tautology! It is a way of saying that a set which has three elements has three elements!%
\par
This means only that we have counted them and we had to stop at three. In order to do this we must have numbers first. Which, of course, we do. As I said, everyone knows what numbers are.%
\par
\alert{Sagredo}:  I must confess, my friend, that I become more confused as we speak. I am no longer certain that I really know what a number is. Since you seem to have retained your certainty can you clear this up for me? Can you tell me what a number is?%
\par
\alert{Simplicio}:  Certainly. A number is what we have just been discussing. It is what you have when you stop counting. For example, three is the totality (to use your phrase) of the elements of the sets \(\left\{Salviati, Sagredo, Simplicio\right\}\) or \(\left\{1, 2, 3\right\}\) because when I count the elements in either set I have to stop at three. Nothing less, nothing more. Thus three is a number.%
\par
\alert{Salviati}:  But this definition only confuses me! Surely you will allow that fractions are numbers? What is counted when we end with, say \(4/5\) or \(1/5?\)%
\par
\alert{Simplicio}:  This is simplicity itself. \(4/5\) is the number we get when we have divided something into \(5\) equal pieces and we have counted four of these fifths. This is four-fifths. You see? Even the language we use naturally bends itself to our purpose.%
\par
\alert{Salviati}:  But what of one-fifth? In order to count one fifth we must first divide something into fifths. To do this we must know what one-fifth is, musn't we? We seem to be using the word ``number'' to define itself again. Have we not come full circle and gotten nowhere?%
\par
\alert{Simplicio}:  I confess this had not occurred to me before. But your objection is easily answered. To count one-fifth we simply divide our ``something'' into tenths. Then we count two of them. Since two-tenths is the same as one-fifth the problem is solved. Do you see?%
\par
\alert{Sagredo}:  I see your point but it will not suffice at all! It merely replaces the question, ``What is one-fifth?'' with, ``What is one-tenth?'' Nor will it do to say that one-tenth is merely two-twentieths. This simply shifts the question back another level.%
\par
Archimedes said, ``Give me a place to stand and a lever long enough and I will move the earth.'' But of course he never moved the earth because he had nowhere to stand. We seem to find ourselves in Archimedes' predicament: We have no place to stand.%
\par
\alert{Simplicio}:  I confess I don't see a way to answer this right now. However I'm sure an answer can be found if we only think hard enough. In the meantime I cannot accept that \(\aleph_0\) is a number. It is, as I said before, infinity and infinity is not a number! We may as well believe in fairies and leprechauns if we call infinity a number.%
\par
\alert{Sagredo}:  But again we've come full circle. We cannot say definitively that \(\aleph_n\) is or is not a number until we can state with confidence what a number is. And even if we could find solid ground on which to solve the problem of fractions, what of \(\sqrt{2}?\) Or \(\pi?\) Certainly these are numbers but I see no way to count to either of them.%
\par
\alert{Simplicio}:  Alas! I am beset by demons! I am bewitched! I no longer believe what I \emph{know} to be true!%
\par
\alert{Salviati}:  Perhaps things are not quite as bad as that. Let us consider further. You said earlier that we all know what numbers are, and I agree. But perhaps your statement needs to be more precisely formulated. Suppose we say instead that we all know what numbers \emph{need} to be? Or that we know what we \emph{want} numbers to be?%
\par
Even if we cannot say with certainly what numbers \emph{are} surely we can say what we want and need for them to be. Do you agree?%
\par
\alert{Sagredo}:  I do.%
\par
\alert{Simplicio}:  And so do I.%
\par
\alert{Salviati}:  Then let us invent numbers anew, as if we've never seen them before, always keeping in mind those properties we need for numbers to have. If we take this as a starting point then the question we need to address is, ``What do we need numbers to be?''%
\par
\alert{Sagredo}:  This is obvious! We need to be able to add them and we need to be able to multiply them together, and the result should also be a number.%
\par
\alert{Simplicio}:  And subtract and divide too, of course.%
\par
\alert{Sagredo}:  I am not so sure we actually need these. Could we not \emph{define} ``subtract two from three'' to be ``add negative two to three'' and thus dispense with subtraction and division?%
\par
\alert{Simplicio}:  I suppose we can but I see no advantage in doing so. Why not simply have subtraction and division as we've always known them?%
\par
\alert{Sagredo}:  The advantage is parsimony. Two arithmetic operations are easier to keep track of than four. I suggest we go forward with only addition and multiplication for now. If we find we need subtraction or division we can consider them later.%
\par
\alert{Simplicio}:  Agreed. And I now see another advantage. Obviously addition and multiplication must not depend on order. That is, if \(x\) and \(y\) are numbers then \(x+y\) must be equal to \(y+x\) and \(xy\) must be equal to \(yx\). This is not true for subtraction, for \(3-2\) does \emph{not} equal \(2-3\). But if we define subtraction as you suggest then this symmetry is preserved:%
\begin{equation*}
x+(-y) = (-y)+x\text{.}
\end{equation*}
%
\par
\alert{Sagredo}:  Excellent! Another property we will require of numbers occurs to me now. When adding or multiplying more than two numbers it should not matter where we begin. That is, if \(x\), \(y\) and \(z\) are numbers it should be true that%
\begin{equation*}
(x+y)+z = x+(y+z)
\end{equation*}
and%
\begin{equation*}
(x\cdot y)\cdot z = x\cdot(y\cdot z)\text{.}
\end{equation*}
%
\par
\alert{Simplicio}:  Yes! We have it! Any objects which combine in these precise ways can be called numbers.%
\par
\alert{Salviati}:  Certainly these properties are necessary, but I don't think they are yet sufficient to our purpose. For example, the number \(1\) is unique in that it is the only number which, when multiplying another number leaves it unchanged.%
\par
For example:  \(1\cdot3=3\). Or, in general, if \(x\) is a number then \(1\cdot x =x\).%
\par
\alert{Sagredo}:  Yes. Indeed. It occurs to me that the number zero plays a similar role for addition: \(0+x=x\).%
\par
\alert{Salviati}:  It does not seem to me that addition and multiplication, as we have defined them, force \(1\) or \(0\) into existence so I believe we will have to postulate their existence independently.%
\par
\alert{Sagredo}:  Is this everything then? Is this all we require of numbers?%
\par
\alert{Simplicio}:  I don't think we are quite done yet. How shall we get division?%
\par
\alert{Sagredo}:  In the same way that we defined subtraction to be the addition of a negative number, can we not define division to be multiplication by a reciprocal? For example, \(3\) divided by \(2\) can be considered \(3\) \emph{multiplied} by \(1/2\), can it not?%
\par
\alert{Salviati}:  I think it can. But observe that \emph{every} number will need to have a corresponding negative so that we can subtract any amount. And again nothing we've discussed so far forces these negative numbers into existence so we will have to postulate their existence separately.%
\par
\alert{Simplicio}:  And in the same way every number will need a reciprocal so that we can divide by any amount.%
\par
\alert{Sagredo}:  Every number that is, except zero.%
\par
\alert{Simplicio}:  Yes, this is true. Strange is it not, that of them all only this one number needs no reciprocal? Shall we also postulate that zero has no reciprocal?%
\par
\alert{Salviati}: I don't see why we should. Possibly \(\aleph_0\) is the reciprocal of zero. Or possibly not. But I see no need to concern ourselves with things we do not need.%
\par
\alert{Simplicio}: Is this everything then? Have we discovered all that we need for numbers to be?%
\par
\alert{Salviati}: I believe there is only one property missing. We have postulated addition and we have postulated multiplication and we have described the numbers zero and one which play similar roles for addition and multiplication respectively. But we have not described how addition and multiplication work together.%
\par
That is, we need a rule of distribution:  If \(x\), \(y\) and \(z\) are all numbers then%
\begin{equation*}
x\cdot(y+z) = x\cdot y+x\cdot z\text{.}
\end{equation*}
%
\par
With this in place I believe we have everything we need.%
\par
\alert{Simplicio}:  Indeed. We can also see from this that \(\aleph_0\) cannot be a number since, in the first place, it cannot be added to another number and in the second, even if it could be added to a number the result is surely not also a number.%
\par
\alert{Salviati}:  My dear Simplicio, I fear you have missed the point entirely! Our axioms do not declare what a number is, only how it behaves with respect to addition and multiplication with other numbers. Thus it is a mistake to presume that ``numbers'' are only those objects that we have always believed them to be. In fact, it now occurs to me that ``addition'' and ``multiplication'' also needn't be seen as the operations we have always believed them to be.%
\par
For example suppose we have three objects, \(\left\{a, b, c\right\}\) and suppose that we define ``addition'' and ``multiplication'' by the following tables:%
\begin{align*}
\begin{array}{c|ccc} 
+\amp a\amp b\amp c\\\hline 
a\amp a\amp b\amp c\\
b\amp b\amp c\amp a\\ 
c\amp c\amp a\amp b 
\end{array}  
\amp \amp \amp \amp \amp \amp 
\begin{array}{c|ccc} 
\cdot\amp a\amp b\amp c\\\hline 
a\amp a\amp a\amp a\\ 
b\amp a\amp b\amp c\\ 
c\amp a\amp c\amp b 
\end{array} 
\end{align*}
%
\par
I submit that our set along with these definitions satisfy all of our axioms and thus \(a\), \(b\) and \(c\) qualify to be called ``numbers.''%
\par
\alert{Simplicio}:  This cannot be! There is no zero, no one!%
\par
\alert{Sagredo}:  But there is. Do you not see that \(a\) plays the role of zero \textemdash{} if you add it to any number you get that number back. Similarly \(b\) plays the role of one.%
\par
\alert{Simplicio}: This is astonishing! If \(a, b\) and \(c\) can be numbers then I am less sure than ever that I know what numbers are! Why, if we replace \(a, b\), and \(c\) with \emph{Simplicio} \emph{Sagredo,} and \emph{Salviati} then \emph{we} become numbers ourselves!%
\par
\alert{Salviati}:  Perhaps we will have to be content with knowing how numbers behave rather than knowing what they are.%
\par
However I confess that I have a certain affection for the numbers I grew up with. Let us call those the ``real'' numbers. Any other set of numbers, such as our \(\left\{a,b,c\right\}\) above we will call a field of numbers, since they seem to provide us with new ground to explore. Or perhaps just a \emph{number field}?%
\par
As we have been discussing this I have been writing down our axioms.  They are these.  Numbers are any objects which satisfy all of the following properties:%
\par
\alert{AXIOMS OF NUMBERS}%
\par
%
\begin{description}
\item[{Axiom I: Definition of Operations}]They can be combined by two operations, denoted ``\(\cdot\)'' and ``+''.%
\item[{Axiom II: Closure}]If \(x\), \(y\) and \(z\) are numbers then \(x+y\) is also a number. \(x\cdot y\) is also a number.%
\item[{Axiom II: Commutativity}]\(x+y=y+x\) \(x\cdot y=y\cdot x\)%
\item[{Axiom III: Associativity}]\((x+y)+z=x+(y+z)\) \((x\cdot y)\cdot z = x\cdot(y\cdot z)\)%
\item[{Axiom IV: Additive Identity}]There is a number, denoted \(0\), such that for any number, \(x\),%
\begin{equation*}
x+0=x\text{.}
\end{equation*}
%
\item[{Axiom V: Multiplicative Identity}]There is a number, denoted \(1\), such that for any number, \(x\),%
\begin{equation*}
1\cdot x=x\text{.}
\end{equation*}
%
\item[{Axiom VI: Additive Inverses}]Given any number, \(x\), there is a number, denoted \(-x\), with the property that%
\begin{equation*}
x+(-x)=0\text{.}
\end{equation*}
%
\item[{Axiom VII: Multiplicative Inverse}]Given any number, \(x\ne0\), there is a number, denoted \(x^{-1}\), with the property that%
\begin{equation*}
x\cdot x^{-1} =1\text{.}
\end{equation*}
%
\item[{Axiom VIII: The Distributive Property}]If \(x\), \(y\) and \(z\) are numbers then%
\begin{equation*}
x\cdot(y+z) = x\cdot y+x\cdot z\text{.}
\end{equation*}
%
\end{description}
%
\par
\alert{Sagredo}: My friend, this is a thing of surpassing beauty! All seems clear to me now. Numbers are any group of objects which satisfy our axioms. That is, a number is anything that \emph{acts} like a number.%
\par
\alert{Salviati}:  Yes this seems to be true.%
\par
\alert{Simplicio}:  But wait! We have not settled the question: Is \(\aleph_0\) a number or not?%
\par
\alert{Salviati}:  If everything we have just done is valid then \(\aleph_0\) \emph{could be} a number. And so could \(\aleph_1\),\(\aleph_2 \ldots\) \emph{if} we can find a way to define addition and multiplication on the set \(\left\{\aleph_0, \aleph_1, \aleph_2 \ldots\right\}\) in a manner that agrees with our axioms.%
\par
\alert{Sagredo}:  An arithmetic of infinities! This is a very strange idea. Can such a thing be made sensible?%
\par
\alert{Simplicio}:  Not, I think, before lunch. Shall we retire to our meal?%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection 13.1.1 Additional Problems}
\typeout{************************************************}
%
\begin{subsectionptx}{Additional Problems}{}{Additional Problems}{}{}{g:subsection:idp334}
\begin{problem}{}{g:problem:idp335}%
Show that \(0\neq 1\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp336}{}\quad{}Show that if \(x\neq0\), then \(0\cdot x \neq x\).%
\end{problem}
\begin{problem}{}{g:problem:idp337}%
Consider the set of ordered pairs of integers: \(\left\{(x,y)|s, y \in \ZZ\right\}\), and define addition and multiplication as follows:%
\par
%
\begin{itemize}[label=\textbullet]
\item{}\lititle{Addition:.}\par%
\(\displaystyle (a,b)+(c,d) = (ad+bc, bd)\)%
\item{}\lititle{Multiplication:.}\par%
\((a,b)\cdot(c,d) = (ac, bd)\).%
\end{itemize}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]If we add the convention that%
\begin{equation*}
(ab, ad) = (b,d)
\end{equation*}
show that this set with these operations forms a number field.%
\item[(b)]Which number field is this?%
\end{enumerate}
\end{problem}
\begin{problem}{}{g:problem:idp338}%
Consider the set of ordered pairs of real numbers, \(\left\{(x,y)|x, y\in\RR\right\}\), and define addition and multiplication as follows:%
\par
%
\begin{itemize}[label=\textbullet]
\item{}\lititle{Addition:.}\par%
\(\displaystyle (a,b)+(c,d) = (a+c, b+d)\)%
\item{}\lititle{Multiplication:.}\par%
\((a,b)\cdot(c,d) = (ac- bd,ad+bc)\).%
\end{itemize}
%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Show that this set with these operations forms a number field.%
\item[(b)]Which number field is this?%
\end{enumerate}
\end{problem}
\end{subsectionptx}
\end{sectionptx}
%
%
\typeout{************************************************}
\typeout{Section 13.2 Building the Real Numbers}
\typeout{************************************************}
%
\begin{sectionptx}{Building the Real Numbers}{}{Building the Real Numbers}{}{}{x:section:Epilogues-BuildRealNumbers}
\begin{introduction}{}%
Contrary to the title of this section we will not be rigorously building the real numbers here. Instead our goal is to show why such a build is logically necessary, and to give a sense of some of the ways this has been accomplished in the past. This may seem odd given our uniform emphasis on mathematical rigor, especially in the third part of the text, but there are very good reasons for this.%
\par
One is simple practicality. The fact is that rigorously building the real numbers and then showing that they have the required properties is extraordinarily detailed work, even for mathematics. If we want to keep this text to a manageable size (we do), we simply don't have the room.%
\par
The second reason is that there is, as far as we know, very little for you to gain by it. When we are done we will have the real numbers. The same real numbers you have been using all of your life. They have the same properties, and quirks, they've always had. To be sure, they will not have lost any of their charm; They will be the same delightful mix of the mundane and the bizarre, and they are still well worth exploring and getting to know better. But nothing we do in the course of building them up logically from simpler ideas will help with that exploration.%
\par
A reasonable question then, is, ``Why bother?'' If the process is overwhelmingly, tediously detailed (it is) and gives us nothing new for our efforts, why do it at all?%
\par
Doing mathematics has been compared\footnote{By Andrew Wiles, the man who proved Fermat's Last Theorem.\label{g:fn:idp339}} to entering a dark room. At first you are lost. The layout of the room and furniture are unknown so you fumble about for a bit and slowly get a sense of your immediate environs, perhaps a vague notion of the organization of the room as a whole. Eventually, after much, often tedious exploration, you become quite comfortable in your room. But always there will be dark corners; hidden areas you have not yet explored. Such a dark area may hide anything; the latches to unopened doors you didn't know were there; a clamp whose presence explains why you couldn't move that little desk in the corner; even the light switch that would allow you to illuminate an area more clearly than you would have imagined possible.%
\par
But, and this is the point, there is no way to know what you will find there until you walk into that dark corner and begin exploring. Perhaps nothing. But perhaps something wonderful.%
\par
This is what happened in the late nineteenth century. The real numbers had been used since the Pythagoreans learned that \(\sqrt{2}\) was irrational. But really, most calculuations were (and still are) done with just the rational numbers. Moreover, since \(\QQ\) forms a ``set of measure zero,''\footnote{See \hyperref[x:corollary:cor_Q-MeasureZero]{Corollary~{\xreffont\ref{x:corollary:cor_Q-MeasureZero}}} of \hyperref[x:chapter:BackToFourier]{Chapter~{\xreffont\ref{x:chapter:BackToFourier}}}.\label{g:fn:idp340}} \index{measure zero} it is clear that most of the real numbers had gone completely unused. The set of real numbers was thus one of those ``dark corners'' of mathematics. It had to be explored.%
\par
``But even if that is true,'' you might ask, \textasciigrave{}\textasciigrave{}I have no interest in the logical foundations of the real numbers, especially if such knowledge won't tell me anything I don't already know. Why do I need to know all of the details of constructing \(\RR\) from \(\QQ?\)%
\par
The answer to this is very simple: You don't.%
\par
That's the other reason we're not covering all of the details of this material. We will explain enough to light up, dimly perhaps, this little corner of mathematics. Later, should you need (or want) to come back to this and explore further you will have a foundation to start with. Nothing more.%
\par
Until the nineteenth century the geometry of Euclid, as given in his book \emph{The Elements,} was universally regarded as the touchstone of mathematical perfection. This belief was so deeply embedded in Western culture that as recently as 1923, Edna St.~Vincent Millay opened one of the poems in her book \emph{The Harp Weaver and Other Poems} with the line ``Euclid alone has looked on beauty bare.''%
\par
Euclid begins his book by stating \(5\) simple axioms and proceeds, step by logical step, to build up his geometry. Although far from actual perfection, his methods are clean, precise and efficient \textemdash{} he arrives at the Pythagorean Theorem in only \(47\) steps (theorems) \textemdash{} and even today Euclid's \emph{Elements} still sets a very high standard of mathematical exposition and parsimony.%
\par
The goal of starting with what is clear and simple and proceeding logically, rigorously, to what is complex is still a guiding principle of all mathematics for a variety of reasons. In the late nineteenth century, this principle was brought to bear on the real numbers. That is, some properties of the real numbers that at first seem simple and intuitively clear turn out on closer examination, as we have seen, to be rather counter-intuitive. This alone is not really a problem. We can have counter-intuitive properties in our mathematics \textemdash{} indeed, this is a big part of what makes mathematics interesting \textemdash{} as long as we arrive at them logically, starting from simple assumptions the same way Euclid did.%
\par
Having arrived at a view of the real numbers which is comparable to that of our nineteenth century colleagues, it should now be clear that the real numbers and their properties must be built up from simpler concepts as suggested by our Italian friends in the previous section.%
\par
In addition to those properties we have discovered so far, both \(\QQ\) and \(\RR\) share another property which will be useful. We have used it throughout this text but have not heretofore made it explicit. They are both \emph{linearly ordered.} We will now make this property explicit.%
\begin{definition}{Linear Ordering.}{x:definition:def_NumberField}%
\index{number field!linearly ordered}%
A number field is said to be \emph{linearly ordered} if there is a relation, denoted ``\(\lt \)'', on the elements of the field which satisfies all of the following for all \(x, y\), and \(z\) in the field.%
\par
%
\begin{enumerate}
\item{}For all numbers \(x\) and \(y\) in the field, exactly one of the following holds:%
\par
%
\begin{enumerate}
\item{}\(\displaystyle x\lt y\)%
\item{}\(\displaystyle x=y\)%
\item{}\(\displaystyle y\lt x\)%
\end{enumerate}
%
\item{}If \(x\lt y\), then \(x+z\lt y+z\) for all \(z\) in the field.%
\item{}If \(x\lt y\), and \(0\lt z\), then \(x\cdot z \lt y\cdot z\).%
\item{}If \(x\lt y\) and \(y\lt z\) then \(x\lt z\).%
\end{enumerate}
%
\end{definition}
Any number field with such a relation is called a \terminology{linearly ordered number field} and as the following problem shows, not every number field is linearly ordered.%
\begin{problem}{}{g:problem:idp341}%
\begin{enumerate}[font=\bfseries,label=(\alph*),ref=\alph*]
\item[(a)]Prove that the following must hold in \emph{any} linearly ordered number field.%
\par
%
\begin{enumerate}
\item{}\(0\lt x\) if and only if \(-x\lt 0\).%
\item{}If \(x\lt y\) and \(z\lt 0\) then \(y\cdot z\lt x\cdot z\).%
\item{}For all \(x\neq 0\), \(0\lt x^2\).%
\item{}\(0\lt 1\).%
\end{enumerate}
%
\item[(b)]Show that the set of complex numbers (\(\CC\)) is not a linearly ordered field.%
\end{enumerate}
\end{problem}
In a thorough, rigorous presentation we would now assume the existence of the natural numbers \((\NN)\), and their properties and use these to define the integers, \((\ZZ)\). We would then use the integers to define the rational numbers, \((\QQ)\). We could then show that the rationals satisfy the field axioms worked out in the previous section, and that they are linearly ordered.%
\par
Then \textemdash{} at last \textemdash{} we would use \(\QQ\) to define the real numbers \((\RR)\), show that these also satisfy the field axioms and also have the other properties we expect: Continuity, the Nested Interval Property, the Least Upper Bound Property, the Bolzano-Weierstrass Theorem, the convergence of all Cauchy sequences, and linear ordering.%
\par
We would start with the natural numbers because they seem to be simple enough that we can simply assume their properties. As Leopold Kronecker (1823-1891) said: ``God made the natural numbers, all else is the work of man.''%
\par
Unfortunately this is rather a lot to fit into this epilogue so we will have to abbreviate the process rather severely.%
\par
We will assume the existence and properties of the rational numbers.  Building \(\QQ\) from the integers is not especially hard and it is easy to show that they satisfy the axioms worked out by Salviati, Sagredo and Simplicio in the previous section.  But the level of detail required for rigor quickly becomes onerous.%
\par
Even starting at this fairly advanced position in the chain of logic there is still a considerable level of detail needed to complete the process. Therefore our exposition will necessarily be incomplete.%
\par
Rather than display, in full rigor, how the real numbers can be built up from the rationals we will show, in fairly broad terms, three ways this has been done in the past. We will give references later in case you'd like to follow up and learn more.%
\end{introduction}%
%
%
\typeout{************************************************}
\typeout{Subsection 13.2.1 The Decimal Expansion}
\typeout{************************************************}
%
\begin{subsectionptx}{The Decimal Expansion}{}{The Decimal Expansion}{}{}{x:subsection:DecimalExpansion}
This is by far the most straightforward method we will examine. Since we begin with \(\QQ\), we already have some numbers whose decimal expansion is infinite. For example, \(\frac13= 0.333\ldots\). We also know that if \(x\in\QQ\) then expressing \(x\) as a decimal gives either a finite or a \emph{repeating infinite} decimal.%
\par
More simply, we can say that \(\QQ\) consists of the set of all decimal expressions which eventually repeat. (If it eventually repeats zeros then it is what we've called a finite decimal.)%
\par
We then define the real numbers to be the set of all infinite decimals, repeating or not.%
\par
It may feel as if all we have to do is define addition and multiplication in the obvious fashion and we are finished. This set with these definitions obviously satisfy all of the field axioms worked out by our Italian friends in the previous section. Moreover it seems clear that all of our equivalent completeness axioms are satisfied.%
\par
However, things are not quite as clear cut as they seem.%
\par
The primary difficulty in this approach is that the decimal representation of the real numbers is so familiar that everything we need to show \emph{seems} obvious. But stop and think for a moment. Is it really obvious how to define addition and multiplication of \emph{infinite} decimals? Consider the addition algorithm we were all taught in grade school. That algorithm requires that we line up two numbers at their decimal points:%
\begin{align*}
d_1d_2\amp .d_3d_4\\
+\   \delta_1\delta_2\amp .\delta_3\delta_4\text{.}
\end{align*}
%
\par
We then begin adding in the rightmost column and proceed to the left. \emph{But if our decimals are infinite we can't get started because there is no rightmost column!}%
\par
A similar problem occurs with multiplication.%
\par
So our first problem is to define addition and multiplication in \(\RR\) in a manner that re-captures addition and multiplication in \(\QQ\).%
\par
This is not a trivial task.%
\par
One way to proceed is to recognize that the decimal notation we've used all of our lives is really shorthand for the sum of an infinite series. That is, if \(x=0.d_1d_2d_3\ldots\) where \(0\leq d_i\leq 9\) for all \(i\in\NN\) then%
\begin{equation*}
x=\sum_{i=1}^\infty\frac{d_i}{10^i}\text{.}
\end{equation*}
%
\par
Addition is now apparently easy to define: If \(x=\sum_{i=1}^\infty\frac{d_i}{10^i}\) and \(y=\sum_{i=1}^\infty\frac{\delta_i}{10^i}\) then%
\begin{equation*}
x+y= \sum_{i=1}^\infty\frac{e_i}{10^i} \text{ where \(e_i =d_i+\delta_i\). }
\end{equation*}
%
\par
But there is a problem. Suppose for some \(j\in\NN\), \(e_j=d_i+\delta_i>10\). In that case our sum does not satisfy the condition \(0\leq e_i\leq 9\) so it is not even clear that the expression \(\sum_{i=1}^\infty\frac{e_i}{10^i}\) represents a real number. That is, we may not have the closure property of a number field. We will have to define some sort of ``carrying'' operation to handle this.%
\begin{problem}{}{g:problem:idp342}%
Define addition on infinite decimals in a manner that is closed.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp343}{}\quad{}Find an appropriate ``carry'' operation for our definition.%
\end{problem}
A similar difficulty arises when we try to define multiplication. Once we have a notion of carrying in place, we could define multiplication as just the multiplication of series. Specifically, we could define%
\begin{align*}
(0.a_1a_2a_3\ldots)\cdot(0.b_1b_2b_3\ldots) \amp =\left(\frac{a_1}{10}+\frac{a_2}{10^2}+\cdots\right) \cdot\left(\frac{b_1}{10}+\frac{b_2}{10^2}{10^3}+\cdots\right)\\
\amp =\frac{a_1b_1}{10^2}+\frac{a_1b_2+a_2b_1}{10^3}+\frac{a_1b_3+a_2b_2+a_3b_1}{10^4}+\cdots\text{.}
\end{align*}
%
\par
We could then convert this to a ``proper'' decimal using our carrying operation.%
\par
Again the devil is in the details to show that such algebraic operations satisfy everything we want them to. Even then, we need to worry about linearly ordering these numbers and our completeness axiom.%
\par
Another way of looking at this is to think of an infinite decimal representation as a (Cauchy) sequence of finite decimal approximations. Since we know how to add and multiply finite decimal representations, we can just add and multiply the individual terms in the sequences. Of course, there is no reason to restrict ourselves to only these specific types of Cauchy sequences, as we see in our next approach.%
\end{subsectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection 13.2.2 Cauchy Sequences}
\typeout{************************************************}
%
\begin{subsectionptx}{Cauchy Sequences}{}{Cauchy Sequences}{}{}{x:subsection:CauchySequences}
As we've seen, Georg Cantor \index{Cantor, Georg} began his career studying Fourier series and quickly moved on to more foundational matters in the theory of infinite sets.%
\par
But he did not lose his fascination with real analysis when he moved on. Like many mathematicians of his time, he realized the need to build \(\RR\) from \(\QQ\). He and his friend and mentor Richard Dedekind \index{Dedekind, Richard} (who's approach we will see in the next section) both found different ways to build \(\RR\) from \(\QQ\).%
\par
Cantor started with Cauchy sequences in \(\QQ\).%
\par
That is, we consider the set of all Cauchy sequences of rational numbers. We would like to \emph{define} each such sequence to be a real number. The goal should be clear. If \(\left(s_n\right)_{n=1}^\infty\) is a sequence in \(\QQ\) which converges to \(\sqrt{2}\) then we will call \(\left(s_n\right)\) the real number \(\sqrt{2}\).%
\par
This probably seems a bit startling at first. There are a \emph{lot} of numbers in \(\left(s_n\right)\) (countably infinitely many, to be precise) and we are proposing putting all of them into a big bag, tying it up in a ribbon, and calling the whole thing \(\sqrt{2}\). It seems a very odd thing to propose, but recall from the discusion in the previous section that we left the concept of ``number'' undefined. Thus if we can take \emph{any} set of objects and define addition and multiplication in such a way that the field axioms are satisfied, then those objects are legitimately numbers. To show that they are, in fact, the \emph{real} numbers we will also need the completeness property.%
\par
A bag full of rational numbers works as well as anything \emph{if} we can define addition and multiplication appropriately.%
\par
Our immediate problem though is not addition or multiplication but uniqueness. If we take one sequence \(\left(s_n\right)\) which converges to \(\sqrt{2}\) and define it to be \(\sqrt{2}\), what will we do with all of the other sequences that converge to \(\sqrt{2}?\)%
\par
Also, we have to be careful not to refer to any real numbers, like the square root of two for example, as we define the real numbers. This would be a circular \textemdash{} and thus useless \textemdash{} definition. Obviously though, we can refer to \emph{rational} numbers, since these are the tools we'll be using.%
\par
The solution is clear. We take \emph{all} sequences of rational numbers that converge to \(\sqrt{2}\), throw them into our bag and call \emph{that} \(\sqrt{2}\). Our bag is getting pretty full now.%
\par
But we need to do this without using \(\sqrt{2}\) because it is a real number. The following two definitions satisfy all of our needs.%
\begin{definition}{}{x:definition:def_EquivCauchySeq}%
\index{sequences!Cauchy sequences!equivalent} Let \(x=\left(s_n\right)_{k=1}^\infty\) and \(y=\left(\sigma_n\right)_{k=1}^\infty\) be Cauchy sequences in \(\QQ\). \(x\) and \(y\) are said to be equivalent if they satisfy the following property: For every \(\eps>0\), \(\eps\in\QQ\), there is a rational number \(N\) such that for all \(n>N\), \(n\in\NN\),%
\begin{equation*}
\abs{s_n-\sigma_n}\lt \eps\text{.}
\end{equation*}
%
\par
We will denote equivalence by writing, \(x\equiv y\).%
\end{definition}
\begin{problem}{}{x:problem:prob_EquivalentCauchySequences}%
\index{equivalent Cauchy sequences} Show that:%
\begin{enumerate}[label=(\alph*)]
\item{}\(\displaystyle x\equiv x\)%
\item{}\(\displaystyle x\equiv y \imp y\equiv x\)%
\item{}\(x\equiv y\) and \(y\equiv z \imp x\equiv z\)%
\end{enumerate}
%
\end{problem}
\begin{definition}{}{x:definition:def_RealsViaCauchy}%
\index{sequences!Cauchy sequences!real numbers as Cauchy sequences} Every set of all equivalent Cauchy sequences defines a real number.%
\end{definition}
A very nice feature of Cantor's method is that it is very clear how addition and multiplication should be defined.%
\begin{definition}{}{x:definition:def_AddTimesViaCauchy}%
\index{sequences!Cauchy sequences!addition and multiplication of} If%
\begin{equation*}
x= \left\{\left.\left(s_n\right)_{k=1}^\infty \right| \left(s_n\right)_{k=1}^\infty \text{ is Cauchy in \(\QQ\) } \right\}
\end{equation*}
and%
\begin{equation*}
y= \left\{\left.\left(\sigma_n\right)_{k=1}^\infty \right| \left(\sigma_n\right)_{k=1}^\infty \text{ is Cauchy in \(\QQ\) } \right\}
\end{equation*}
then we define the following:%
\begin{itemize}[label=\textbullet]
\item{}\lititle{Addition:.}\par%
\(\displaystyle x+y = \left\{\left.\left(t_n\right)_{k=1}^\infty \right| t_k = s_k+\sigma_k, \forall (s_n) \in x, \text{ and } (\sigma_n)\in y\right\}\)%
\item{}\lititle{Multiplication:.}\par%
\(\displaystyle x\cdot y = \left\{\left.\left(t_n\right)_{k=1}^\infty \right| t_k = s_k\sigma_k, \forall (s_n) \in x, \text{ and } (\sigma_n)\in y\right\}\)%
\end{itemize}
%
\end{definition}
The notation used in \hyperref[x:definition:def_RealsViaCauchy]{Definition~{\xreffont\ref{x:definition:def_RealsViaCauchy}}} can be difficult to read at first, but basically it says that addition and multiplication are done component-wise. However since \(x\) and \(y\) consist of all equivalent sequences we have to take \emph{every} possible choice of \((s_n)\in x\) and \((\sigma_n)\in y\), form the sum (product) \((s_n+\sigma_n)_{n=1}^\infty\) (\((s_n\sigma_n)_{n=1}^\infty\)) and then show that all such sums (products) are equivalent. Otherwise addition (multiplication) is not well-defined: It would depend on which sequence we choose to represent \(x\) and \(y\).%
\begin{problem}{}{x:problem:prob_CauchyAdditionWellDefined}%
\index{sequences!Cauchy sequences!addition of is well defined}\index{\(\RR\)!addition of Cauchy sequences} Let \(x\) and \(y\) be real numbers in \(\QQ\) (that is, let them be sets of equivalent Cauchy sequences). If \((s_n)\) and \((t_n)\) are in \(x\) and \((\sigma_n)\) and \((\tau_n)\) are in \(y\) then%
\begin{equation*}
(s_n+t_n)_{n=1}^\infty \equiv (\sigma_n+\tau_n)_{n=1}^\infty\text{.}
\end{equation*}
%
\end{problem}
\begin{theorem}{}{}{x:theorem:thm_ZeroInCauchySequences}%
\index{sequences!Cauchy sequences!zero as a Cauchy sequence} Let \(0^*\) be the set of Cauchy sequences in \(\QQ\) which are all equivalent to the sequence \((0, 0, 0, \ldots)\). Then%
\begin{equation*}
0^*+x=x\text{.}
\end{equation*}
%
\end{theorem}
\begin{proof}{}{g:proof:idp344}
From \hyperref[x:problem:prob_CauchyAdditionWellDefined]{Problem~{\xreffont\ref{x:problem:prob_CauchyAdditionWellDefined}}} it is clear that in forming \(0^*+x\) we can choose any sequence in \(0^*\) to represent \(0^*\) and any sequence in \(x\) to represent \(x\). (This is because any other choice will yield a sequence equivalent to \(0^*+x\).)%
\par
Thus we choose \((0, 0, 0, \ldots)\) to represent \(0^*\) and any element of \(x\), say \((x_1, x_2, \ldots)\), to represent \(x\). Then%
\begin{align*}
(0, 0, 0, \ldots) + (x_1, x_2, x_3, \ldots) \amp = (x_1, x_2, x_3, \ldots)\\
\amp = x\text{.}
\end{align*}
%
\par
Since any other sequences taken from \(0^*\) and \(x\) respectively, will yield a sum equivalent to \(x\) (see \hyperref[x:problem:prob_EquivalentCauchySequences]{Problem~{\xreffont\ref{x:problem:prob_EquivalentCauchySequences}}}) we conclude that%
\begin{equation*}
0^*+x=x\text{.}\qedhere
\end{equation*}
%
\end{proof}
\begin{problem}{}{g:problem:idp345}%
\index{\(\RR\)!as Cauchy sequences!identify the multiplicative identity}\index{\(\RR\)!the number \(1\) as a Cauchy sequence} Identify the set of equivalent Cauchy sequences, \(1^*\), such that%
\begin{equation*}
1^*\cdot x=x\text{.}
\end{equation*}
%
\end{problem}
\begin{problem}{}{g:problem:idp346}%
\index{\(\RR\)!defined by Cauchy sequences}\index{number field!for Cauchy sequences}\index{sequences!Cauchy sequences!field axioms for} Let \(x, y\), and \(z\) be real numbers (equivalent sets of Cauchy sequences). Show that with addition and multiplication defined as above we have:%
\begin{enumerate}[label=(\alph*)]
\item{}\lititle{a).}\par%
\(\displaystyle x+y=y+x\)%
\item{}\lititle{b).}\par%
\(\displaystyle (x+y)+z=x+(y+z)\)%
\item{}\lititle{c).}\par%
\(\displaystyle x\cdot y=y\cdot x\)%
\item{}\lititle{d).}\par%
\(\displaystyle (x\cdot y)\cdot z=x\cdot (y\cdot z)\)%
\item{}\lititle{e).}\par%
\(\displaystyle x\cdot(y+z)=x\cdot y+x\cdot z\)%
\end{enumerate}
%
\end{problem}
Once the existence of additive and multiplicative inverses is established\footnote{We will not address this issue here, but you should give some thought to how this might be accomplished.\label{g:fn:idp347}} the collection of all sets of equivalent Cauchy sequences, with addition and mulitiplication defined as above satisfy all of the field axioms. It is clear that they form a number field and thus deserve to be called numbers.%
\par
However this does not necessarily show that they form \(\RR\). We also need to show that they are complete in the sense of \hyperref[x:chapter:IVTandEVT]{Chapter~{\xreffont\ref{x:chapter:IVTandEVT}}}. It is perhaps not too surprising that when we build the real numbers using equivalent Cauchy sequences the most natural completeness property we can show is that if a sequence of real numbers is Cauchy then it converges.%
\par
However we are not in a position to show that Cauchy sequences in \(\RR\) converge. To do this we would first need to show that these sets of equivalence classes of Cauchy sequences (real numbers) are linearly ordered.%
\par
Unfortunately showing the linear ordering, while not especially hard, is time consuming. So we will again invoke the prerogatives of the teacher and brush all of the difficulties aside with the assertion that it is straightforward to show that the real numbers as we have constructed them in this section are linearly ordered and are complete. If you would like to see this construction in full rigor we recommend the book, \emph{The Number System} by H.~A.~Thurston~\hyperlink{x:biblio:thurston56__number_system}{[{\xreffont 16}]}.\footnote{Thurston first builds \(\RR\) as we've indicated in this section. Then as a final remark he shows that the real numbers must be exactly the infinite decimals we saw in the previous section.\label{g:fn:idp348}}%
\end{subsectionptx}
%
%
\typeout{************************************************}
\typeout{Subsection 13.2.3 Dedekind Cuts}
\typeout{************************************************}
%
\begin{subsectionptx}{Dedekind Cuts}{}{Dedekind Cuts}{}{}{x:subsection:DedekindCuts}
An advantage of building the reals via Cauchy sequences in the previous section is that once we've identified equivalent sequences with real numbers it is \emph{very} clear how addition and multiplication should be defined.%
\par
On the other hand, before we can even start to understand that construction, we need a fairly strong sense of what it means for a sequence to converge and enough experience with sequences to be comfortable with the notion of a Cauchy sequence. Thus a good deal of high level mathematics must be mastered before we can even begin.%
\par
The method of ``Dedekind cuts'' first developed by Richard Dedekind \index{Dedekind, Richard!Dedekind cuts} (though he just called them ``cuts'') in his 1872 book, \emph{Continuity and the Irrational Numbers} shares the advantage of the Cauchy sequence method in that, once the candidates for the real numbers have been identified, it is very clear\footnote{``Clear'' does not mean ``easy to do'' as we will see.\label{g:fn:idp349}} how addition and multiplication should be defined. It is also straightforward to show that most of the field axioms are satisfied.%
\par
In addition, Dedekind's method also has the advantage that very little mathematical knowledge is required to get started. This is intentional. In the preface to the first edition of his book, Dedekind \index{Dedekind, Richard} states:%
\begin{quote}%
This memoir can be understood by anyone possessing what is usually called common sense; no technical philosophic, or mathematical, knowledge is in the least degree required. (quoted in~\hyperlink{x:biblio:hawking05__god_creat_integ}{[{\xreffont 5}]})%
\end{quote}
While he may have overstated his case a bit, it is clear that his intention was to argue from very simple first principles just as Euclid did.%
\par
His starting point was the observation we made in \hyperref[x:chapter:NumbersRealRational]{Chapter~{\xreffont\ref{x:chapter:NumbersRealRational}}}: The rational number line is full of holes. More precisely we can ``cut'' the rational line in two distinct ways:%
\begin{enumerate}
\item{}We can pick a rational number, \(r\). This choice divides all other rational numbers into two classes: Those greater than \(r\) and those less than \(r\).%
\item{}We can pick one of the holes in the rational number line. In this case \emph{all} of the rational fall into two classes: Those greater than the hole and those less.%
\end{enumerate}
%
\par
But to speak of rational numbers as less than or greater than something that is \emph{not} there is utter nonsense. We'll need a better (that is, a rigorous) definition.%
\par
As before we will develop an overall sense of this construction rather than a fully detailed presentation, as the latter would be far too long to include.%
\par
Our presentation will closely follow that of Edmund Landau's in his classic 1951 text \emph{Foundations of Analysis}~\hyperlink{x:biblio:landau66__found_analy}{[{\xreffont 7}]}. We do this so that if you choose to pursue this construction in more detail you will be able to follow Landau's presentation more easily.%
\begin{definition}{}{x:definition:def_dedekind-cuts}%
\alert{Dedekind Cut}%
\par
\index{Dedekind, Richard!Dedekind cuts!definition of}\index{Dedekind, Richard!Dedekind cuts} A set of \emph{positive}\footnotemark{} rational numbers is called a \emph{cut} if%
\par
%
\begin{description}
\item[{Property I}]It contains a positive rational number but does not contain all positive rational numbers.%
\item[{Property II}]Every positive rational number in the set is less than every positive rational number not in the set.%
\item[{Property III}]There is no element of the set which is greater than every other element of the set.%
\end{description}
%
\end{definition}
\footnotetext[6]{Take special notice that we are not using the negative rational numbers or zero to build our cuts.  The reason for this will become clear shortly.\label{g:fn:idp350}}%
Given their intended audiences, Dedekind \index{Dedekind, Richard} and Landau shied away from using too much notation. However, we will include the following for those who are more comfortable with the symbolism as it may help provide more perspective. Specifically the properties defining a Dedekind cut \(\alpha\) can be written as follows.%
\par
%
\begin{description}
\item[{Property I}]\(\alpha\ne\emptyset\) and \(\QQ^+-\alpha\ne\emptyset\).%
\item[{Property II}]If \(x\in\alpha\) and \(y\in\QQ^+-\alpha\), then \(x\lt y\). (Alternatively, if \(x\in\alpha\) and \(y\lt x\), then \(y\in\alpha\).)%
\item[{Property III}]If \(x\in\alpha\), then \(\exists\ z\in\alpha\) such that \(x\lt z\).%
\end{description}
%
\par
Properties I-III really say that Dedekind cuts are bounded open intervals of rational numbers starting at \(0\). For example, \((0,3)\cap\QQ^+\) is a Dedekind cut (which will eventually be the real number \(3\)). Likewise, \(\left\{x|x^2\lt 2\right\}\cap\QQ^+\) is a Dedekind cut (which will eventually be the real number \(\sqrt{2}\)). Notice that care must be taken not to actually refer to irrational numbers in the properties as the purpose is to construct them from rational numbers, but it might help to ground you to anticipate what will happen.%
\par
Take particular notice of the following three facts:%
\begin{enumerate}
\item{}Very little mathematical knowledge is required to understand this definition. We need to know what a set is, we need to know what a rational number is, and we need to know that given two positive rational numbers either they are equal or one is greater.%
\item{}The language Landau uses is \emph{very} precise. This is necessary in order to avoid such nonsense as trying to compare something with nothing like we did a couple of paragraphs up.%
\item{}We are only using the \emph{positive} rational numbers for our construction. The reason for this will become clear shortly. As a practical matter for now, this means that the cuts we have just defined will (eventually) correspond to the \emph{positive} real numbers.%
\end{enumerate}
%
\begin{definition}{}{x:definition:def_dedekind-cuts-ordering}%
\index{Dedekind, Richard!Dedekind cuts!ordering of} Let \(\alpha\) and \(\beta\) be cuts. Then we say that \(\alpha\) is less than \(\beta\), and write%
\begin{equation*}
\alpha\lt \beta
\end{equation*}
if there is a rational number in \(\beta\) which is not in \(\alpha\).%
\end{definition}
Note that, in light of what we said prior to \hyperref[x:definition:def_dedekind-cuts-ordering]{Definition~{\xreffont\ref{x:definition:def_dedekind-cuts-ordering}}} (which is taken directly from Landau), we notice the following.%
\begin{theorem}{}{}{x:theorem:thm_OrderingCuts}%
\index{Dedekind, Richard!Dedekind cuts!ordering of} Let \(\alpha\) and \(\beta\) be cuts. Then \(\alpha\lt \beta\) if and only if \(\alpha\subset\beta\).%
\end{theorem}
\begin{problem}{}{g:problem:idp351}%
\index{Dedekind, Richard!Dedekind cuts!order properties}\index{\(\RR\)!ordering Dedekind cuts} Prove \hyperref[x:theorem:thm_OrderingCuts]{Theorem~{\xreffont\ref{x:theorem:thm_OrderingCuts}}} and use this to conclude that if \(\alpha\) and \(\beta\) are cuts then exactly one of the following is true:%
\begin{enumerate}
\item{}\(\alpha=\beta\).%
\item{}\(\alpha\lt \beta\).%
\item{}\(\beta\lt \alpha\).%
\end{enumerate}
%
\end{problem}
We will need first to define addition and multiplication for our cuts and eventually these will need to be extended of \(\RR\) (once the non-positive reals have also been constructed). It will be necessary to show that the extended definitions satisfy the field axioms. As you can see there is a lot to do.%
\par
As we did with Cauchy sequences and with infinite decimals, we will stop well short of the full construction. If you are interested in \index{Dedekind, Richard} exploring the details of Dedekind's construction, Landau's book~\hyperlink{x:biblio:landau66__found_analy}{[{\xreffont 7}]} is very thorough and was written with the explicit intention that it would be accessible to students. In his ``Preface for the Teacher'' he says%
\begin{quote}%
I hope that I have written this book, after a preparation stretching over decades, in such a way that a normal student can read it in two days.%
\end{quote}
This may be stretching things. Give yourself at least a week and make sure you have nothing else to do that week.%
\par
Addition and multiplication are defined in the obvious way.%
\begin{definition}{}{x:definition:def_dedekind-cuts-addition}%
\alert{Addition on cuts}%
\par
\index{Dedekind, Richard!Dedekind cuts!addition of} Let \(\alpha\) and \(\beta\) be cuts. We will denote the set \(\left\{x+y|x\in\alpha,
y\in\beta\right\}\) by \(\alpha+\beta\).%
\end{definition}
\begin{definition}{}{x:definition:def_dedekind-cuts-multiplication}%
\alert{Multiplication on cuts}%
\par
\index{Dedekind, Richard!Dedekind cuts!multiplication of positive cuts} Let \(\alpha\) and \(\beta\) be cuts. We will denote the set \(\left\{xy|x\in\alpha,
y\in\beta\right\}\) by \(\alpha\beta \text{ or } \alpha\cdot\beta\).%
\end{definition}
If we are to have a hope that these objects will serve as our real numbers we must have closure with respect to addition and multiplication. We will show closure with respect to addition.%
\begin{theorem}{}{}{x:theorem:thm_dedekind-cuts-closure}%
\alert{Closure with Respect to Addition}%
\par
\index{Dedekind, Richard!Dedekind cuts!closure of} If \(\alpha\) and \(\beta\) are cuts then \(\alpha+\beta\) is a cut.%
\end{theorem}
\begin{proof}{}{g:proof:idp352}
We need to show that the set \(\alpha+\beta\) satisfies all three of the properties of a cut.%
\par
%
\begin{description}
\item[{Property I}]Let \(x\) be \emph{any} rational number in \(\alpha\) and let \(x_1\) be a rational number not in \(\alpha\).  Then by Property II \(x\lt x_1\).%
\par
Let \(y\) be \emph{any} rational number in \(\beta\) and let \(y_1\) be a rational number not in \(\beta\).  Then by Property II \(y\lt
y_1\).%
\par
Thus since \(x+y\) represents a generic element of \(\alpha+\beta\) and \(x+y\lt x_1+y_1\), it follows that \(x_1+y_1\not\in\alpha+\beta\).%
\item[{Property II}]We will show that the contrapositive of \alert{Property II} is true: If \(x\in\alpha+\beta\) and \(y\lt x\) then \(y\in\alpha+\beta\).%
\par
First, let \(x\in\alpha+\beta\). Then there are \(x_\alpha\in\alpha\) and \(x_\beta\in\beta\) such that \(y\lt x=x_\alpha+x_\beta\). Therefore \(\frac{y}{x_\alpha+x_\beta}\lt  1\), so that%
\begin{align*}
x_\alpha\left(\frac{y}{x_\alpha+x_\beta}\right)\amp \lt  x_\alpha\\
\intertext{and}
x_\beta\left(\frac{y}{x_\alpha+x_\beta}\right)\amp \lt  x_\beta\text{.}
\end{align*}
%
\par
Therefore \(x_\alpha\left(\frac{y}{x_\alpha+x_\beta}\right)\in\alpha\) and \(x_\beta\left(\frac{y}{x_\alpha+x_\beta}\right)\in\beta\). Therefore%
\begin{equation*}
y=x_\alpha\left(\frac{y}{x_\alpha+x_\beta}\right)+x_\beta\left(\frac{y}{x_\alpha+x_\beta}\right)\in\alpha+\beta\text{.}
\end{equation*}
%
\item[{Property III}]Let \(z\in\alpha+\beta\). We need to find \(w>z\), \(w\in\alpha+\beta\). Observe that for some \(x\in\alpha\) and \(y\in\beta\)%
\begin{equation*}
z=x+y\text{.}
\end{equation*}
%
\par
Since \(\alpha\) is a cut, there is a rational number \(x_1\in\alpha\) such that \(x_1>x\). Take \(w=x_1+y\in\alpha+\beta\). Then%
\begin{equation*}
w=x_1+y>x+y=z\text{.}
\end{equation*}
%
\end{description}
%
\end{proof}
\begin{problem}{}{g:problem:idp353}%
\index{Dedekind, Richard!Dedekind cuts!multiplication of} Show that if \(\alpha\) and \(\beta\) are cuts then \(\alpha\cdot\beta\) is also a cut.%
\end{problem}
At this point we have built our cuts and we have defined addition and multiplication for cuts. However, as observed earlier the cuts we have will (very soon) correspond only to the positive real numbers. This may appear to be a problem but it really isn't because the non-positive real numbers can be defined in terms of the positives, that is, in terms of our cuts. We quote from Landau~\hyperlink{x:biblio:landau66__found_analy}{[{\xreffont 7}]}:%
\begin{quote}%
These cuts will henceforth be called the ``positive numbers;''  .  .  .%
\par
We create a new number \(0\) (to be read ``zero''), distinct from the positive numbers.%
\par
We also create numbers which are distinct from the positive numbers as well as distinct from zero, and which we will call negative numbers, in such a way that to each \(\xi\) (I.e. to each positive number) we assign a negative number denoted by \(-\xi\) (\(-\) to be read ``minus''). In this, \(-\xi\) and \(-\nu\) will be considered as the same number (as equal) if and only if \(\xi\) and \(\nu\) are the same number.%
\par
The totality consisting of all positive numbers, of \(0\), and of all negative numbers, will be called the real numbers.%
\end{quote}
Of course it is not nearly enough to simply postulate the existence of the non-negative real numbers.%
\par
All we have so far is a set of objects we're calling the real numbers. For some of them (the positive reals\footnote{That is, the cuts.\label{g:fn:idp354}}) we have defined addition and multiplication. These definitions will eventually turn out to correspond to the addition and multiplication we are familiar with.%
\par
However we do not have either operation for our entire set of proposed real numbers. Before we do this we need first to define the \emph{absolute value} of a real number. This is a concept you are very familiar with and you have probably seen the following definition: Let \(\alpha\in\RR\). Then%
\begin{equation*}
\abs{\alpha} = \begin{cases}\alpha\amp  \text{ if \(\alpha\ge0,\) } \\ -\alpha\amp  \text{ if \(\alpha\lt 0.\) } \end{cases}
\end{equation*}
%
\par
Unfortunately we cannot use this definition because we do not yet have a linear ordering on \(\RR\) so the statement \(\alpha\ge0\) is meaningless. Indeed, it will be our definition of absolute value that orders the real numbers. We must be careful.%
\par
Notice that by definition a negative real number is denoted with the dash ('-') in front. That is \(\chi\) is positive while \(-\chi\) is negative. Thus if \(A\) is \emph{any} real number then one of the following is true:%
\begin{enumerate}
\item{}\(A=\chi\) for some \(\chi\in\RR\) (\(A\) is positive)%
\item{}\(A=-\chi\) for some \(\chi\in\RR\) (\(A\) is negative)%
\item{}\(A=0\).%
\end{enumerate}
%
\par
We define absolute value as follows:%
\begin{definition}{}{x:definition:def_absolute-value}%
\index{Dedekind, Richard!Dedekind cuts!absolute value} Let \(A\in\RR\) as above. Then%
\begin{equation*}
\abs{A}= \begin{cases}\chi \amp  \text{ if \(A=\chi{}\)} \\ 0 \amp  \text{ if \(A=0\) } \\ \chi \amp  \text{ if \(A=-\chi{}.\)} {} \end{cases}
\end{equation*}
%
\end{definition}
With this definition in place it is possible to show that \(\RR\) is linearly ordered. We will not do this explicitly. Instead we will simply assume that the symbols ``\(\lt\)'' ``\(>\),'' and ``\(=\)'' have been defined and have all of the properties we have learned to expect from them.%
\par
We now extend our definitions of addition and multiplication from the positive real numbers (cuts) to all of them. Curiously, multiplication is the simpler of the two.%
\begin{definition}{}{x:definition:def_dedekind-multiplication}%
\alert{Multiplication}%
\par
\index{Dedekind, Richard!Dedekind cuts!multiplication of} Let \(\alpha, \beta\in\RR\). Then%
\begin{equation*}
\alpha\cdot\beta = \begin{cases}-\abs{\alpha}\abs{\beta}\amp  \text{ if \(\alpha>0,\beta\lt 0\) or \(\alpha\lt 0, \beta>0,\) }  \\ \ \ \,\abs{\alpha}\abs{\beta}\amp  \text{ if \(\alpha\lt 0,\beta\lt 0,\) }  \\ \ \ \ \ \ \, 0\amp  \text{ if \(\alpha=0\) or \(\beta=0.\) }  {} \end{cases}
\end{equation*}
%
\end{definition}
Notice that the case where \(\alpha\) and \(\beta\) are both positive was already handled by \hyperref[x:definition:def_dedekind-cuts-multiplication]{Definition~{\xreffont\ref{x:definition:def_dedekind-cuts-multiplication}}} because in that case they are both cuts.%
\par
Next we define addition.%
\begin{definition}{}{x:definition:def_dedekind-addition}%
\alert{Addition}%
\par
\index{Dedekind, Richard!Dedekind cuts!addition of} Let \(\alpha, \beta\in\RR\). Then%
\begin{equation*}
\alpha+\beta= \begin{cases}-(\abs{\alpha}+\abs{\beta}) \amp  \text{ if \(\alpha\lt 0, \beta\lt 0 \) } \\ \ \ \ \abs{\alpha}-\abs{\beta}\amp  \text{ if \(\alpha>0, \beta\lt 0, \abs{\alpha}>\abs{\beta} \)} \\ \ \ \ \ \ \ \ \,0             \amp  \text{ if \(\alpha>0, \beta\lt 0, \abs{\alpha}=\abs{\beta} \)} \\ -(\abs{\alpha}-\abs{\beta})   \amp  \text{ if \(\alpha>0, \beta\lt 0, \abs{\alpha}\lt \abs{\beta} \)} \\ \ \ \ \ \ \beta+\alpha        \amp  \text{ if \(\alpha\lt 0, \beta>0 \) } \\ \ \ \ \ \ \ \ \,\beta{}       \amp  \text{ if \(\alpha=0 \) } \\ \ \ \ \ \ \ \ \,\alpha        \amp  \text{ if \(\beta=0 \) } \end{cases} \text{.}
\end{equation*}
%
\end{definition}
But wait! In the second and fourth cases of our definition we've actually defined addition in terms of subtraction.\footnote{Notice also that the fifth case refers to the addition as defined in the second case.\label{g:fn:idp355}} But we haven't defined subtraction yet! Oops!%
\par
This is handled with the definition below, but it illuminates very clearly the care that must be taken in these constructions. The real numbers are \emph{so} familiar to us that it is extraordinarily easy to make unjustified assumptions.%
\par
Since the subtractions in the second and fourth cases above are done with positive numbers we only need to give meaning to the subtraction of cuts.%
\begin{definition}{}{x:definition:def_CutSubtraction}%
\index{Dedekind, Richard!Dedekind cuts!subtraction of} If \(\alpha\), \(\beta\) and \(\delta\) are cuts then the expression%
\begin{equation*}
\alpha-\beta=\delta
\end{equation*}
is defined to mean%
\begin{equation*}
\alpha=\delta+\beta\text{.}
\end{equation*}
%
\end{definition}
Of course, there is the detail of showing that there is such a cut \(\delta\). (We warned you of the tediousness of all this.) Landau goes through the details of showing that such a cut exists. We will present an alternative by defining the cut \(\alpha-\beta\) directly (assuming \(\beta\lt \alpha\)). To motivate this definition, consider something we are familiar with: \(3-2=1\). In terms of cuts, we want to say that the open interval from \(0\) to \(3\) ``minus'' the open interval from \(0\) to \(2\) should give us the open interval from \(0\) to \(1\). Taking elements from \((0,3)\) and subtracting elements from \((0,2)\) won't do it as we would have differences such as \(2.9-.9=2\) which is not in the cut \((0,1)\). A moment's thought tells us that what we need to do is take all the elements from \((0,3)\) and subtract all the elements from \((2,\infty)\), restricting ourselves only to those which are positive rational numbers. This prompts the following definition.%
\begin{definition}{}{x:definition:def_SubtractionOfCutsAsSets}%
\index{Dedekind, Richard!Dedekind cuts!as sets} Let \(\alpha\) and \(\beta\) be cuts with \(\beta\lt \alpha\). Define \(\alpha-\beta\) as follows:%
\begin{equation*}
\alpha-\beta =\left\{x-y|x\in\alpha \text{ and } y\not\in\beta\right\}\cap\QQ^+\text{.}
\end{equation*}
%
\end{definition}
To show that, in fact, \(\beta+(\alpha-\beta)=\alpha\), the following technical lemma will be helpful.%
\begin{lemma}{}{}{x:lemma:lem_Technical}%
Let \(\beta\) be a cut, \(y\) and \(z\) be positive rational numbers not in \(\beta\) with \(y\lt z\), and let \(\eps>0\) be any rational number. Then there exist positive rational numbers \(r\) and \(s\) with \(r\in\beta\), and \(s\not\in\beta\), such that \(s\lt z\), and \(s-r\lt \eps\).%
\end{lemma}
\begin{problem}{}{g:problem:idp356}%
Prove \hyperref[x:lemma:lem_Technical]{Lemma~{\xreffont\ref{x:lemma:lem_Technical}}}.%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp357}{}\quad{}Since \(\beta\) is a cut there exists \(r_1\in\beta\). Let \(s_1=y\not\in\beta\).  We know that \(r_1\lt s_1\lt
z\).  Consider the midpoint \(\frac{s_1+r_1}{2}\).  If this is in \(\beta\) then relabel it as \(r_2\) and relabel \(s_1\) as \(s_2\).  If it is not in \(\beta\) then relabel it as \(s_2\) and relabel \(r_1\) as \(r_2\), etc.%
\end{problem}
\begin{problem}{}{g:problem:idp358}%
Let \(\alpha\) and \(\beta\) be cuts with \(\beta\lt
\alpha\).  Prove that \(\beta+(\alpha-\beta)=\alpha\).%
\par\smallskip%
\noindent\textbf{\blocktitlefont Hint}.\hypertarget{g:hint:idp359}{}\quad{}It is pretty straightforward to show that \(\beta+(\alpha-\beta)\subseteq\alpha\).  To show that \(\alpha\subseteq\beta+(\alpha-\beta)\), we let \(x\in\alpha\).  Since \(\beta\lt \alpha\), we have \(y\in\alpha\) with \(y\not\in\beta\). We can assume without loss of generality that \(x\lt
y\). (Why?) Choose \(z\in\alpha\) with \(y\lt
z\).  By the \hyperref[x:lemma:lem_Technical]{Lemma~{\xreffont\ref{x:lemma:lem_Technical}}}, there exists positive rational numbers \(r\) and \(s\) with \(r\in\beta\), \(s\in\beta\), \(s\lt z\), and \(s-r\lt z-x\).  Show that \(x\lt r+(z-s)\).%
\end{problem}
We will end by saying that no matter how you construct the real number system, there is really only one.  More precisely we have the following theorem which we state without proof.\footnote{In fact, \emph{not} proving this result seems to be standard in real analysis references.  Most often it is simply stated as we've done here.\label{g:fn:idp360}}%
\begin{theorem}{}{}{x:theorem:thm_R-is-R}%
\index{\(\RR\)!any complete, linearly ordered field is isomorphic to}\index{any complete, linearly ordered field is isomorphic to \(\RR\).} Any complete, linearly ordered field is isomorphic to \(\RR\).%
\end{theorem}
Two linearly ordered number fields are said to be isomorphic if there is a one-to-one, onto mapping between them (such a mapping is called a bijection) which preserves addition, multiplication, and order. More precisely, if \({\cal F}_1\) and \({\cal F}_2\) are both linearly ordered fields, \(x,
y \in{\cal F}_1\) and \(\phi:{\cal F}_1\rightarrow{\cal F}_2\) is the mapping then%
\begin{enumerate}
\item{}\(\displaystyle \phi(x+y)=\phi(x)+\phi(y)\)%
\item{}\(\displaystyle \phi(x\cdot y) = \phi(x)\cdot\phi(y)\)%
\item{}\(x\lt y \imp \phi(x)\lt \phi(y)\).%
\end{enumerate}
%
\par
Remember that we warned you that these constructions were fraught with technical details that are not necessarily illuminating. Nonetheless, at this point, you have everything you need to show that the set of all real numbers as defined above is linearly ordered and satisfies the Least Upper Bound property.%
\par
But we will stop here in order, to paraphrase Descartes, to leave for you the joy of further discovery.%
\end{subsectionptx}
\end{sectionptx}
\end{chapterptx}
\end{partptx}
%
\backmatter
%
%
%% A lineskip in table of contents as transition to appendices, backmatter
\addtocontents{toc}{\vspace{\normalbaselineskip}}
%
%
%
\typeout{************************************************}
\typeout{References  Bibliography}
\typeout{************************************************}
%
\begin{references-chapter-numberless}{Bibliography}{}{Bibliography}{}{}{g:references:idp361}
%% If this is a top-level references
%%   you can replace with "thebibliography" environment
\begin{referencelist}
\bibitem[1]{x:biblio:bradley09__cauch_cours}\hypertarget{x:biblio:bradley09__cauch_cours}{}RobertE. Bradley and C. Edward Sandifer. \emph{\textit{Cauchy's Cours d'analyse}: An Annotated Translation}. Sources and Studies in the History of Mathematics and Physical Sciences. Springer, 2009.
\bibitem[2]{x:biblio:dunham90__journ_throug_genius}\hypertarget{x:biblio:dunham90__journ_throug_genius}{}William Dunham. \emph{Journey Through Genius}. Penguin Books, 1990.
\bibitem[3]{x:biblio:franks10__cantor_other_proof_rr_uncoun}\hypertarget{x:biblio:franks10__cantor_other_proof_rr_uncoun}{}John Franks. Cantor's other proofs that \(\RR\) is uncountable. \emph{Mathematics Magazine}, 83(4):283-{}-{}289, October 2010.
\bibitem[4]{x:biblio:grabiner81__origin_cauch_rigor_calculy}\hypertarget{x:biblio:grabiner81__origin_cauch_rigor_calculy}{}Judith Grabiner. \emph{The Origins of Cauchy's Rigorous Calculus}. MIT Press, Cambridge MA, 1981.
\bibitem[5]{x:biblio:hawking05__god_creat_integ}\hypertarget{x:biblio:hawking05__god_creat_integ}{}Stephen Hawking, editor. \emph{God Created the Integers: The Mathematical Breakthroughs that Changed History}. Running Press, Philadelphia, London, 2005.
\bibitem[6]{x:biblio:jahnke03__histor_analy}\hypertarget{x:biblio:jahnke03__histor_analy}{}H. Jahnke, editor. \emph{A History of Analysis}. AMS Publications, Providence RI, 2003.
\bibitem[7]{x:biblio:landau66__found_analy}\hypertarget{x:biblio:landau66__found_analy}{}Edmund Landau. \emph{Foundations of Analysis}. Chelsea Publishing Company, New York, NY, 1966. Translated by F. Steinhardt, Columbia University.
\bibitem[8]{x:biblio:levenson09__newton_count}\hypertarget{x:biblio:levenson09__newton_count}{}Thomas Levenson. \emph{Newton and the Counterfeiter}. Houghton Mifflin Harcourt, 2009.
\bibitem[9]{x:biblio:netz07__archim_codex}\hypertarget{x:biblio:netz07__archim_codex}{}Reviel Netz and William Noel. \emph{The Archimedes Codex}. Da Capo Press, 2007.
\bibitem[10]{x:biblio:newton45__sir_isaac_two_treat_quadr}\hypertarget{x:biblio:newton45__sir_isaac_two_treat_quadr}{}Isaac Newton. \emph{Sir Isaac Newton's Two Treatises of the Quadrature of Curves and Analysis by Equation of an Infinite Number of Terms, Explained}. Society for the Encouragement of Learning, 1745. Translated from Latin by John Stewart, A. M. Professor of Mathematicks in the Marishal College and University of Aberdeen.
\bibitem[11]{x:biblio:Bernoulli_bio_mactutor}\hypertarget{x:biblio:Bernoulli_bio_mactutor}{}J. J. O'Connor and E. F. Robertson. \emph{The Brachistochrone Problem}. \emph{http:\slash{}\slash{}www-gap.dcs.st-and.ac.uk", MacTutor History of Mathematics archive}
\bibitem[12]{x:biblio:robinson74__non_stand_analy}\hypertarget{x:biblio:robinson74__non_stand_analy}{}Abraham Robinson. \emph{Non-standard analysis}. North-Holland Pub. Co., 1974.
\bibitem[13]{x:biblio:russo96__forgot_revol}\hypertarget{x:biblio:russo96__forgot_revol}{}Lucio Russo and Silvio Levy. \emph{The Forgotten Revolution: How Science Was Born in 300 BC and Why It Had to Be Reborn}. Springer, 1996.
\bibitem[14]{x:biblio:sandifer07__early_mathem_leonar_euler}\hypertarget{x:biblio:sandifer07__early_mathem_leonar_euler}{}C. Edward Sandifer. \emph{The Early Mathematics of Leonard Euler}. Spectrum, 2007. ISBN 10: 0883855593, ISBN 13: 978-0883855591.
\bibitem[15]{x:biblio:struik69__sourc_book_mathem}\hypertarget{x:biblio:struik69__sourc_book_mathem}{}Dirk Struik, editor. \emph{Source Book in Mathematics, 1200-1800}. Harvard University Press, Cambridge, MA, 1969.
\bibitem[16]{x:biblio:thurston56__number_system}\hypertarget{x:biblio:thurston56__number_system}{}H. A. Thurston. \emph{The Number System}. Blackie and Son Limited, London, Glassgow, 1956.
\end{referencelist}
\end{references-chapter-numberless}
%
%% The index is here, setup is all in preamble
%% Index locators are cross-references, so same font here
{\xreffont\printindex}
%
\end{document}